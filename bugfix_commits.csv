Hash,Message,Parent Hashes,Is Merge Commit?,Modified Files
dc226fdb7aae074ce16aa68d088dfde1be654892,add two test files,['8c4e2a30071fdb6b816eb9abdbdad7e9dd0135e2'],False,"['test_config.py', 'test_run.py']"
98f5131bb6bf252294a63cf1ed38227773112ee5,add test model Megatron_GPT2,['3bc21cd042f4b64f59b7aaf8bcfdf327d28f3589'],False,"['__init__.py', 'ds_config_func_bs4.json', 'ds_config_func_bs8.json', 'ds_config_func_bs8_no_zero.json', 'ds_config_func_scheduler.json', 'ds_config_perf_bs16.json', 'ds_config_perf_bs32.json', 'ds_config_perf_bs8.json', 'ds_gpt2_test.sh', 'run_checkpoint_test.py', 'run_func_test.py', 'run_perf_baseline.py', 'run_perf_test.py', 'test_common.py', 'run_sanity_check.py']"
e9b097f1ad5fc6894bd0c56386344da4fffa7d47,examples -> DeepSpeedExamples,['681001fb6d44f7907d80ceac3ccf07a7f3a6bd36'],False,['ds_gpt2_test.sh']
caaae9924de348201277bb39930e52d67f82124c,Model tests executable fix,['e9b097f1ad5fc6894bd0c56386344da4fffa7d47'],False,['ds_gpt2_test.sh']
b61a22171d126f496058331e51e8e0a1ea7f1954,"Distributed testing (#6)

* Adds distributed_test decorator and some unit tests.

* Setting NCCL backend.

* Parametrizes test.

* rank -> local_rank

* Temporarily disable CUDA initialization.",['97f36c114291223a949f2f7b06f16f4780688d62'],False,"['common.py', 'test_dist.py']"
52c5a936ed834d4abd396dc69da630512a569b7b,"add allreduce test (#7)

* add allreduce test

* comment out set rank to cuda for now

* switched back to gloo",['b61a22171d126f496058331e51e8e0a1ea7f1954'],False,"['common.py', 'test_dist.py']"
d68462038f1d5e91355e92b3f2c2d5c40590f39e,add version check test (#9),['52c5a936ed834d4abd396dc69da630512a569b7b'],False,['test_config.py']
438aa017730881e48f3957727463556b40314448,"Enables NCCL backend in @distributed_test (#13)

* Enables NCCL backend in @distributed_test

* Adds pytest-forked to avoid CUDA re-initialization issue.

* paste typo

* transcription typo",['188f7d4e0b17aac52f6e8d1ede0cd1c36e6fefb6'],False,"['README.md', 'azure-pipelines.yml', 'requirements.txt', 'common.py', 'test_dist.py']"
af81f6f5400e38c5d7a6590bbf6631b9b00faa68,"Handle missing optional configuration fields correctly (#24)

Co-authored-by: Shaden Smith <ShadenTSmith@gmail.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['54940278eae9ad4f802ca6269f2d29ea61083c40'],False,"['deepspeed_config.py', 'test_ds_config.py']"
8326aff2799d5b0e3309a8b67b747276b072a6cf,"Improve doc string for add_XXX_arguments (#32)

Unit tests for add_XXX_arguments",['af81f6f5400e38c5d7a6590bbf6631b9b00faa68'],False,"['__init__.py', 'test_ds_arguments.py']"
073da7295fa22c216063fbe9bbbf957b160f4680,"Ported Megatron tutorial. (#30)

* Ported DeepSpeed overview.

* Renamed subsection

* Formatting table of contents

* initial import of Megatron tutorial

* Grammatical edits, formatting, and paths.

* formatting and data download instructions

* formatting tutorial

* formatting tutorial

* formatting tutorial

* formatting tutorial

* formatting tutorial

* formatting tutorial

* new perf chart

* removing TODO

* adding pointer to tutorial

* edits

* azure to low bandwidth

Co-authored-by: Samyam Rajbhandari <samyamr@microsoft.com>",['766f030e722b28fcca66f077f9d89bcf502c3de8'],False,"['README.md', 'DeepSpeed-vs-Megatron.png', 'megatron-gpt2-perf-test.png', 'MegatronGPT2Tutorial.md']"
5a0abc6598ce64bae39bd2f4b6b08ae80c8c093d,"Samyamr/batchconfig (#33)

* simplifying the batch config, using a single assert to test for validity and allowing for specifying only the micro batch size

* Simplifying Batch Config, Adding ability to specify batch using just micro_batch, and adding a bunch of unit tests

* ran formatting

* Typo fixes and added the config file

* reformatting

* path fixes

* removing print statements",['073da7295fa22c216063fbe9bbbf957b160f4680'],False,"['deepspeed_config.py', 'ds_gpt2_test.sh', 'ds_batch_config.json', 'test_config.py']"
ddefa23bd04dda125e4ddb71b4ce304dc4013d67,"Yuxiong doc edits (#38)

* Added features.md and Getting Started guide.

* trimming newlines

* Adding hostfile discussion

* include/exclude emphasis

* Edits overview

* minor edits on the overview

* minor edits

* Fixing broken GPT2 links

* Clarify our relationship with Megatron-LM

* Update README.md

* Update README.md

* minor edits

* fix formatting

Co-authored-by: Shaden Smith <ShadenTSmith@gmail.com>
Co-authored-by: yuxionghe <yuxhe@microsoft.com>",['5a0abc6598ce64bae39bd2f4b6b08ae80c8c093d'],False,"['README.md', 'features.md', 'MegatronGPT2Tutorial.md']"
50ae149f824a887834bdfc7a252b35a68e624132,Moving to major/minor/patch versioning. (#51),['e4c55390aecef3a7e9b7d94cd18d46ac6f2c9b36'],False,"['__init__.py', 'install.sh', 'setup.py', 'test_config.py']"
cf8b9c185f8210ce528be44908cdc86b9947ae33,Typo fix (#57),['8575200079041cf42685b1cae11e0885a5b53507'],False,['README.md']
f78ccc06c947d61134885059f23194e4eb3299a0,"Fix broken link in README.md for the 1Cycle doc. (#60)

* Fix broken link for the 1Cycle doc.

* Removed the 1Cycle link from README.md.",['5a2e4b7d09fa646c401f5fd8e01cb5e2afec2835'],False,['README.md']
4f7d016d3ecbd9c030c7ac5ea395b05b1a625a30,remove the undefined variable in ckpt testing (#67),['f78ccc06c947d61134885059f23194e4eb3299a0'],False,['run_checkpoint_test.py']
4f0680c144e05822396ebf30dc9149db869b52f8,Fix broken links in CIFAR (#69),['4f7d016d3ecbd9c030c7ac5ea395b05b1a625a30'],False,['CIFAR-10.md']
bf2689a9dd4492ff7173763e93cd58115de333bb,"Fix bug in install script, bump TF version (#71)

* bump tf version in dockerfile

* Update install.sh",['4f0680c144e05822396ebf30dc9149db869b52f8'],False,"['Dockerfile', 'install.sh']"
fd53b56a735b8b8eca5cea4872134ca37c702b4f,fix typo (#72),['bf2689a9dd4492ff7173763e93cd58115de333bb'],False,['README.md']
37ff62ccb8637a3b263a163d79fe961946415ade,"Porting BingBertSquad test (#70)

* Porting BingBertSquad test

* Updating default paths.

* Enable model tests.

* Updating DeepSpeedExamples submodule

* Adding BingBertSquad's log uploads.

* Messed up the submodule again :-)",['e6c37c043d9284acc0e549940f201d5f1603b55e'],False,"['DeepSpeedExamples', 'azure-pipelines.yml', 'BingBertSquad_run_func_test.py', 'BingBertSquad_test_common.py', '__init__.py', 'deepspeed_bsz24_fp16_config.json', 'deepspeed_bsz24_fp32_config.json', 'run_BingBertSquad.sh', 'run_BingBertSquad_sanity.sh', 'run_tests.sh', 'run_sanity_check.py']"
807480a04bdc371525776e42c31f7c72d9132f0a,"Fix issue with empty grads for non-fused optimizers (#83)

bug fixes for adamw/lamb and corresponding tests",['37ff62ccb8637a3b263a163d79fe961946415ade'],False,"['fp16_unfused_optimizer.py', 'test_fp16.py']"
bca23057de5b88a659a16b48dc2a15cf5a97232a,"Add requirements for cifar10 (#79)

* add install requirements command line

* add pillow library to fix version

* modify to uppercase

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['807480a04bdc371525776e42c31f7c72d9132f0a'],False,"['CIFAR-10.md', 'requirements.txt']"
001abe2362d9edba062070fb05df40925f54cb3e,"Refactor simple model test, fix pythonpath issue (#96)

Also a fix for #94",['f2d7513561eb72f6b9c5188b5a227ecb7b05a2ee'],False,"['__init__.py', 'deepspeed_light.py', 'deepspeed_run.py', 'install.sh', 'simple_model.py', 'test_config.py', 'test_fp16.py']"
cd0d6f3c039b928475172fc9e33c762ec7e03e65,"Fixes to support subscriptions with other VMs (#105)

* Update scripts to handle cases where you have other VMs in your sub

* Support subs with other VMs and fix for PDSH permission error

* Minor fix to support subs with other VMs",['5897091eb94fdd7a6885dfa5eb58c13083136b03'],False,"['azure_ssh.sh', 'setup_docker.sh', 'setup_vms.sh']"
6efee45c4d6aec415e7155459c19147d2fe3ffdd,"Option to keep VM without incuring charges with shutdown command (#106)

* Update scripts to handle cases where you have other VMs in your sub

* Support subs with other VMs and fix for PDSH permission error

* Minor fix to support subs with other VMs

* Added shutdown with or without delete VM option

In Azure deallocate is like machine shutdown (and prevents billing). You can restart deallocated VM. To fully drop the VM delete is used. This command with ""-d"" option will fully delete the VM. Without any argument it justs deallocates / shutd down the VM.",['cd0d6f3c039b928475172fc9e33c762ec7e03e65'],False,['shutdown_vms.sh']
5aa58b3878789332856af9107bef5768dd3e1c9c,"Init distributed torch only if needed (#108)

* add auto-detect to torch dist init

* update tests to infer distributed init status

* prevent crash if dist_init_required is True but already initiliazed

* only init if safe to do so (forgot to add this file in prev commit)",['6efee45c4d6aec415e7155459c19147d2fe3ffdd'],False,"['__init__.py', 'deepspeed_light.py', 'test_config.py', 'test_fp16.py']"
ccec2463ee491313fc89a5c5c1ab6431432dfa47,add some csr addition unit tests (#110),['5aa58b3878789332856af9107bef5768dd3e1c9c'],False,['test_csr.py']
40885599abec1258b486815a8b8289e9b20d9fea,Fix typo (#123),['7dbeba308ee54d4737743abf8bf6fe47cb6b5229'],False,['features.md']
1c0b326e772e20082772fdcd10e3f95ed481d555,"Make lr schedulers support fp16 optimizers (#124)

* add tests cases for onecycle policy with fp16/zero

* Make lr schedulers support fp16 optimizers

* Fix formatting

* More specific naming

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['27d8385148ac86acfb9c85643e7a995e797b1d5c'],False,"['deepspeed_lr_schedules.py', 'test_fp16.py']"
936117b589e848318c44deb3ce128659afde6580,"Enhancement: Ability to load checkpoint without loading the optimizer… (#128)

* Enhancement: Ability to load checkpoint without loading the optimizer states. Unittest testing saving and loading checkpoint with fused, unfused and zero optimizer. The unitest takes about 165s",['1c0b326e772e20082772fdcd10e3f95ed481d555'],False,"['deepspeed_light.py', 'deepspeed_zero_optimizer.py', 'fp16_optimizer.py', 'fp16_unfused_optimizer.py', 'run_checkpoint_test.py', 'test_checkpointing.py']"
5042dc0085537db79aaf4e175e86d5f5bf5ae4d8,drafting Jekyll webpage (#143),['d6bc44bfad4b92acde4193321d1cc9601da55030'],False,"['.gitignore', '404.html', 'CNAME', 'Gemfile', 'Gemfile.lock', '_config.yml', 'navigation.yml', 'tutorials-landing.md', '2020-02-13-release.md', '2020-02-13-turing-nlg.md', '2020-03-17-reduce-scatter.md', '2020-03-17-zero-stage2.md', 'cifar-10.md', 'getting-started.md', 'DeepSpeed-vs-Megatron.png', 'azure.md', 'index.html', 'Makefile', 'build-api-docs.sh', 'requirements.local.txt', 'requirements.readthedocs.txt', 'conf.py', 'deepspeed.pt.rst', 'deepspeed.rst', 'index.rst', 'modules.rst', 'config_json.md', 'features.md', '1cycle_lr.png', 'loss_and_lr.png', 'lr_schedule.png', 'megatron-gpt2-perf-test.png', 'model_convergence.png', 'index.md', '1Cycle.md', 'MegatronGPT2Tutorial.md', 'lrrt.md']"
b84a1fa410934353db1a68055f224be353c99989,Web edits (#147),['4d735946b8f256bc80ba13e3530f85c91d041ff4'],False,"['README.md', 'README.md', '_config.yml', 'navigation.yml', 'config_json.md', 'features.md', '1Cycle.md', 'azure.md', 'getting-started.md', 'lrrt.md', 'megatron.md', 'main.scss', '1cycle_lr.png', 'loss_and_lr.png', 'lr_schedule.png', 'megatron-gpt2-perf-test.png', 'model_convergence.png', 'contributing.md', 'index.md']"
29855c2716a99fbb79e0b73dca9c18b650bd8107,"Fix permalinks (#149)

* fix docs permalink

* fix docs permalink",['b84a1fa410934353db1a68055f224be353c99989'],False,"['README.md', '_config.yml', 'config_json.md']"
a76572dc7cc7030b6208cf1dc3e2f6e501d460c8,Adding static loss scaling for ZeRO. (#166),['012d91df67a9ddd66df847c7608481af027cace9'],False,"['.gitignore', 'deepspeed_zero_optimizer.py', 'test_fp16.py']"
20557f70954f06cbbf8d9fdb1910b63c6af4eb77,Fix ThroughputTimer with hybrid parallelism. (#171),['a76572dc7cc7030b6208cf1dc3e2f6e501d460c8'],False,['deepspeed_light.py']
43f27332c2b36e32d61ad12f8caa716c7966d781,"Add ""zero_allow_untested_optimizer"" option in conf file (#173)

* added zero_allow_untested_optimizer flag helpers

* add zero_allow_untested_optimizer config constants

* zero_allow_untested_optimizer logic with assertion

* Added unit test and CustomOptimizer helper class",['20557f70954f06cbbf8d9fdb1910b63c6af4eb77'],False,"['deepspeed_config.py', 'deepspeed_constants.py', 'deepspeed_light.py', 'simple_model.py', 'test_fp16.py']"
53c73fe3e216bb6d2232b613bbf7bf0479246f6e,"Support multi-output models (#170)

* Push to remote

* Correctly handle multi output models by doing loss scaling in backward()
Unit tests for multi output models

* Fix formatting issues

* Formatting issues fix

* Fix formatting

* Update DeepSpeedExamples submodule
Enable Megatron model tests",['43f27332c2b36e32d61ad12f8caa716c7966d781'],False,"['DeepSpeedExamples', 'deepspeed_light.py', 'run_sanity_check.py', 'multi_output_model.py', 'test_multi_output_model.py']"
90017d3a31beee0ef5421ac08edcd0fa441eea11,AllReduce bucket fix. (#186),['4cbfcc75f7d0b710ab76b0c81f9ea9fa4ffba063'],False,['deepspeed_light.py']
bf4797c2c5694828911dc39d39576b7c4188845c,"Fix  perf bug (#194)

Co-authored-by: Shaden Smith <Shaden.Smith@microsoft.com>",['b7f5cb78c60d6a5792a2018976d0533d9d65cfb2'],False,['deepspeed_zero_optimizer.py']
c014a55b2d759428ec57e8b39976f36b20ef2401,Fixes missing newline in code example (#201),['dd166ee6b619a04e6a5a83b91a8cb279339d9d41'],False,['bert-pretraining.md']
512a0d4de186552764fcb49a6f7b6b362b7496c1,Fix index out of range error when parameter count is not multiple of ranks (#202),['c014a55b2d759428ec57e8b39976f36b20ef2401'],False,"['deepspeed_zero_optimizer.py', 'test_fp16.py']"
6cb332f1b1ea786b5d83ebf9c66cb1f85d1245f6,"CSR+FP32 fix (#206)

1) CSR parameter names should end with .weight. 
2) When using basic optimizer directly, DeepSpeed should handle zero_grad. Letting the basic optimizer do the zero_grad resulted in residual gradients in the embedding layer due to unknown reasons.",['a0cd61e8577307529cd1e0cb92741c13ababde2b'],False,['deepspeed_light.py']
a90a32d79d4209d280540bf61a418b0e2e1288a2,Bug fix for sparse grads (#208),['3ce531c9798613e1265e7c64482a28f7b1a17c4d'],False,['deepspeed_light.py']
7dc209c661b05bc7b08fd554236237d2b4bdbea1,"add basic post-install test (#209)

* add basic post-install test",['a90a32d79d4209d280540bf61a418b0e2e1288a2'],False,"['basic_install_test.py', 'install.sh']"
b2c87edfb6b56e68e022cfb607c1073895d34318,Fix global_steps checkpoint loading. (#139),['4f42bbb0e8d88e9b992b9c84ea7852b7925149ba'],False,"['README.md', 'deepspeed_light.py', 'test_checkpointing.py']"
0be026e38b27fe6fbd8b57a9ce51027dea470f59,"Support dynamic loss scale args in fp16 optimizers (#212)

* Support dynamic loss scale args in fp16 optimizers

* Update names",['b2c87edfb6b56e68e022cfb607c1073895d34318'],False,"['deepspeed_config.py', 'fp16_optimizer.py', 'fp16_unfused_optimizer.py', 'loss_scaler.py', 'test_dynamic_loss_scale.py']"
c61e23b4b108df2af0dda7939ee59d4ae9090415,"adding BingSqaud e2e test (#214)

* adding BingSqaud e2e test

* updating the draft test; bring final step under try section

* finalizinf test for base deepspeed and deepspeed with ZeRO

* applying the comment (thanks Jeff); fixed formatting",['ccaa9901c9a0e118eb105e8affbafb141b2ba1f3'],False,['test_e2e_squad.py']
f2ac7eafd54c49acb8981650637dedd939e96c14,"ZeRO-2 (#217)

Updates for ZeRO stage 2 + ZeRO stage 1 w. RS

Co-authored-by: Tunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Samyam Rajbhandari <samyamr@microsoft.com>
Co-authored-by: Shaden Smith <ShadenTSmith@gmail.com>
Co-authored-by: Elton Zheng <eltonz@microsoft.com>
Co-authored-by: Shaden Smith <Shaden.Smith@microsoft.com>
Co-authored-by: yuxionghe <yuxhe@microsoft.com>
Co-authored-by: Arash Ashari <arashari@microsoft.com>",['c61e23b4b108df2af0dda7939ee59d4ae9090415'],False,"['.gitignore', 'DeepSpeedExamples', 'README.md', 'azure-pipelines.yml', '__init__.py', 'deepspeed_checkpointing.py', 'deepspeed_checkpointing_config.py', 'deepspeed_config.py', 'deepspeed_config_utils.py', 'deepspeed_constants.py', 'deepspeed_light.py', 'deepspeed_timer.py', 'deepspeed_utils.py', 'deepspeed_zero_config.py', 'deepspeed_zero_optimizer.py', 'zero_optimizer_stage1.py', 'zero_utils.py', '_config.yml', 'navigation.yml', 'news-home.html', 'config-json.md', 'features.md', '2020-03-17-zero-stage2.md', '2020-05-19-bert-record.md', '2020-05-19-zero-stage2.md', 'getting-started.md', 'megatron.md', 'DeepSpeed-vs-Megatron.png', 'deepspeed-speedup.png', 'deepspeed-throughput-seq512.png', 'zero-full.png', 'requirements.local.txt', 'requirements.readthedocs.txt', 'activation-checkpointing.rst', 'checkpointing.rst', 'conf.py', 'index.rst', 'initialize.rst', 'model-checkpointing.rst', 'training.rst', 'index.md', 'BingBertSquad_run_func_test.py', 'deepspeed_bsz24_fp16_config.json', 'deepspeed_bsz24_fp16_zero2_config.json', 'deepspeed_bsz24_fp32_config.json', 'run_BingBertSquad_sanity.sh', 'ds_config_func_bs4_zero1.json', 'ds_config_func_bs4_zero2.json', 'ds_config_func_bs8_no_zero.json', 'ds_config_func_bs8_zero1.json', 'ds_config_func_bs8_zero2.json', 'ds_config_func_scheduler.json', 'ds_config_perf_bs16.json', 'ds_config_perf_bs32.json', 'ds_config_perf_bs8.json', 'ds_gpt2_test.sh', 'run_checkpoint_test.py', 'run_func_test.py', 'run_sanity_check.py', 'test_model.py', 'common.py', 'test_checkpointing.py', 'test_ds_config.py', 'test_fp16.py', 'test_multi_output_model.py']"
53ac7947028ab7167ec3ff56e3eaf98b63f973a4,reduce size of megatron tests (#223),['8a18e73e18cb6bb846b5d5a11e9a7ff91caedda8'],False,"['ds_config_func_bs4_zero2.json', 'ds_config_func_bs8_zero2.json', 'ds_gpt2_test.sh', 'run_checkpoint_test.py', 'run_func_test.py', 'run_sanity_check.py']"
e62afbc8c88a3f2b0c72755502169ddd4cdd107d,Tense fix (#225),['00183ed58d2344255622bffdee473836e0f80e16'],False,['bert-pretraining.md']
b466e84e9fb9172a7677ed4ba1c70865d2a8f222,fix redundant (#228),['e62afbc8c88a3f2b0c72755502169ddd4cdd107d'],False,['features.md']
abe2204ddd1d5f60cd3303ead4df22deb5303a5b,"Support fp32 grad clipping and fix max_grad_norm confusion (#232)

* updates to support fp32 grad clipping and disable max_grad_norm",['6a9d57f6cc7a95952ea7b396c75630c0265931e2'],False,"['deepspeed_config.py', 'deepspeed_constants.py', 'deepspeed_light.py', 'ds_config_func_bs4_zero1.json', 'ds_config_func_bs4_zero2.json', 'ds_config_func_bs8_no_zero.json', 'ds_config_func_bs8_zero1.json', 'ds_config_func_bs8_zero2.json', 'ds_config_func_scheduler.json', 'ds_config_perf_bs16.json', 'ds_config_perf_bs32.json', 'ds_config_perf_bs8.json', 'ds_batch_config.json', 'test_checkpointing.py', 'test_fp16.py']"
d24d3de901fa8b110923f1ee9c2ccf0efd56d31f,"Samyamr/cpu memory bloat fix zero (#233)

* Fix for CPU memory Bloating Issue caused by pyorch backward graph creation in allgather. Fixed by calling detach on tensors before calling all_gather

* Fix for CPU memory Bloating Issue caused by pyorch backward graph creation in allgather. Fixed by calling detach on tensors before calling all_gather

* Fix for CPU memory Bloating Issue caused by pyorch backward graph creation in allgather. Fixed by calling detach on tensors before calling all_gather",['abe2204ddd1d5f60cd3303ead4df22deb5303a5b'],False,"['deepspeed_zero_optimizer.py', 'zero_optimizer_stage1.py']"
b652395e5e0584a32123b49adcd8a758fb7e9aab,"fix: typo (#238)

* fix: typo in code docs

* more pythonic code",['6fe0edb80494577d6c21e5b5014faeab1a0d2e47'],False,"['deepspeed_checkpointing.py', 'deepspeed_config_utils.py']"
734d8991c845664c5138a9f69704fd058a51b3c1,"Transformer kernel release (#242)

* Transformer kernels release

Co-authored-by: Shaden Smith <ShadenTSmith@gmail.com>
Co-authored-by: Elton Zheng <eltonz@microsoft.com>
Co-authored-by: Reza Yazdani <reyazda@microsoft.com>
Co-authored-by: RezaYazdaniAminabadi <44502768+RezaYazdaniAminabadi@users.noreply.github.com>
Co-authored-by: Tunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Shaden Smith <ShadenTSmith@gmail.com>
Co-authored-by: Shaden Smith <Shaden.Smith@microsoft.com>
Co-authored-by: Samyam Rajbhandari <samyamr@microsoft.com>
Co-authored-by: Shaden Smith <ShadenTSmith@gmail.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>
Co-authored-by: Samyam Rajbhandari <samyamr@microsoft.com>
Co-authored-by: Shaden Smith <ShadenTSmith@gmail.com>
Co-authored-by: Elton Zheng <eltonz@microsoft.com>
Co-authored-by: Reza Yazdani <reyazda@microsoft.com>
Co-authored-by: RezaYazdaniAminabadi <44502768+RezaYazdaniAminabadi@users.noreply.github.com>
Co-authored-by: Tunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Shaden Smith <Shaden.Smith@microsoft.com>
Co-authored-by: Samyam Rajbhandari <samyamr@microsoft.com>",['b652395e5e0584a32123b49adcd8a758fb7e9aab'],False,"['.clang-format', '.pre-commit-config.yaml', 'DeepSpeedExamples', 'basic_install_test.py', 'fused_lamb_cuda.cpp', 'fused_lamb_cuda_kernel.cu', 'StopWatch.h', 'Timer.h', 'context.h', 'cublas_wrappers.h', 'custom_cuda_layers.h', 'dropout.h', 'ds_transformer_cuda.h', 'feed_forward.h', 'gelu.h', 'gemm_test.h', 'general_kernels.h', 'normalize_layer.h', 'softmax.h', 'strided_batch_gemm.h', 'type_shim.h', 'fused_lamb_cuda.cpp', 'fused_lamb_cuda_kernel.cu', 'cublas_wrappers.cu', 'dropout_kernels.cu', 'ds_transformer_cuda.cpp', 'gelu_kernels.cu', 'general_kernels.cu', 'normalize_kernels.cu', 'softmax_kernels.cu', 'transform_kernels.cu', 'type_shim.h', '__init__.py', 'deepspeed_cuda.py', 'deepspeed_fused_lamb.py', 'deepspeed_run.py', '2020-05-19-bert-record.md', '2020-05-28-fastest-bert-training.md', 'bert-finetuning.md', 'bert-pretraining.md', 'transformer_kernel.md', 'end-to-end-bert-training.PNG', 'layernorm_animation.gif', 'layernorm_deepspeed.gif', 'layernorm_ds.png', 'layernorm_pytorch.gif', 'layernorm_torch.png', 'qkv_fusion.png', 'single-gpu-throughput.png', 'softmax_animation.gif', 'softmax_deepspeed.gif', 'softmax_ds.png', 'softmax_pytorch.gif', 'softmax_torch.png', 'transformer_kernel_perf.png', 'transformer_kernel_perf_seq128.PNG', 'transformer_kernel_perf_seq512.PNG', 'transformer_preln_arch.png', 'checkpointing.rst', 'conf.py', 'index.rst', 'kernel.rst', 'requirements.txt', 'setup.py', 'modeling.py', 'modelingpreln.py', 'test_cuda_backward.py', 'test_cuda_forward.py']"
c7d0b0ca83c39ee300d4b3a6501b4229d57286df,Image links (#243),['6622de165dcfffb5a1993c2313f3e7e053122b4a'],False,"['2020-05-19-bert-record.md', '2020-05-28-fastest-bert-training.md', 'megatron.md']"
0c77f878b67069baf3c5a066b32778c10a38db84,center images (#244),['c7d0b0ca83c39ee300d4b3a6501b4229d57286df'],False,['2020-05-28-fastest-bert-training.md']
bbd8cd7d703a1333849712f33ed2acd8e75be040,update tests,['e04e401613bf13a235453d1f9ab06ef8bf4cf7fe'],False,['test_cuda_backward.py']
e1ad8803eb802bf9cc74ddb54b411d720f5ea6b4,"Add log util (#230)

* Add log util

* replace all occurrences of print and logging

* address format

* disable propagate to avoid duplicate log",['2a1c5db1b01792b1d3ca54f23998a717240ba390'],False,"['__init__.py', 'deepspeed_checkpointing.py', 'deepspeed_config.py', 'deepspeed_launch.py', 'deepspeed_light.py', 'deepspeed_lr_schedules.py', 'deepspeed_run.py', 'deepspeed_timer.py', 'deepspeed_utils.py', 'deepspeed_zero_config.py', 'deepspeed_zero_optimizer.py', 'fp16_optimizer.py', 'fp16_unfused_optimizer.py', 'log_utils.py', 'zero_optimizer_stage1.py', 'zero_utils.py']"
2312f04b0ceb07d81a16d0fc81f327a04beea57d,"Support migration to FP16 optimizer  (#249)

* Debugging

* Fix step() bug; Make step timing optional

* Remove unnecessary changes

* Format fixes

* Replace list with scalar variable

* Remove redundant code

* Fix typo",['f73d717cf95946fa3c21b7d06ac65b654896bb4e'],False,"['deepspeed_light.py', 'fp16_optimizer.py']"
fc1de4ffd28b5975700cf87efb36b699d15c5a0a,fix transformer kernel preln config (#257),['2312f04b0ceb07d81a16d0fc81f327a04beea57d'],False,['deepspeed_cuda.py']
6e87251cd3f36019487583ef1d76fa5fd4dff3ee,"add the fine-tuning results (#260)

* add the fine-tuning results

* updating tutorial and blog-post

* updated the tutorials and links",['96c4daabc162c3c05fe602152ee2ab2d780c0e23'],False,"['2020-05-28-fastest-bert-training.md', 'bert-finetuning.md']"
224494bdea41f9f044b8107ac25f2d2725e32f9f,"Update deepspeed_utils.py (#270)

* Removing handle_overflow debugging code in deepspeed_utils.py

* Removing handle_overflow debugging code in deepspeed_zero_optimizer.py

Removing unnecessary overflow handle code. Not sure why it was there in the first place.",['02c392f0ea00fbf3852d5ac2df11fa9a60ac8a97'],False,"['deepspeed_utils.py', 'deepspeed_zero_optimizer.py']"
88c319aad68fae3193f7b951807a968dc1df0e3d,"Handle parameter groups smaller than DP (#273)

* Load non-DeepSpeed checkpoints into ZeRO optimizer

* Handle parameters smaller than DP

* Formatting fixes",['664fa30cecf9071ef499860808d9fe7eb1c613ca'],False,"['deepspeed_light.py', 'deepspeed_zero_optimizer.py', 'fp16_optimizer.py', 'zero_optimizer_stage1.py', 'test_fp16.py']"
6379292c62285ddcdfcfc14a35048cf4fe1b0d85,"Improving deepspeed.ai website (#269)

* syntax/typo fix

* add README for documentation

* fix links

* update navigation

* typo fix

* docs readme fix",['88c319aad68fae3193f7b951807a968dc1df0e3d'],False,"['README.md', '_config.yml', 'navigation.yml', 'features.md', 'azure.md', 'bert-pretraining.md', 'cifar-10.md', 'contributing.md']"
4a3234e0ab3a6b0039741d7ea2fe0c1b82584763,"ZeRO-2: Handle gradients of empty partitions (#275)

* Load non-DeepSpeed checkpoints into ZeRO optimizer

* Handle parameters smaller than DP

* Formatting fixes

* Handle empty partitions

* Fix perf bug

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['97787881ee2af488f3f92c6889430a96a466a86c'],False,"['deepspeed_zero_optimizer.py', 'test_fp16.py']"
f545312416e8bdd6c0a7d19e644fd49846a54b6a,"Support amp deepspeed backend (#286)

* add amp support for deepspeed (non-ZeRO)
* tests for amp mode",['4a3234e0ab3a6b0039741d7ea2fe0c1b82584763'],False,"['deepspeed_config.py', 'deepspeed_constants.py', 'deepspeed_light.py', 'test_fp16.py']"
7ccc9daf9ec909b71c7261f8e77876774b1633af,"Support loading and saving ZeRO checkpoints with changing DP degree (#240)

* Support saving and loading ZeRO checkpoints on different data
parallelism degree.

* Fix formatting

* Support checkpoint with varying GPU count in ZeRO stage 1

* Fix formatting

* Formatting fixes

* Update model tests

* Remove pprint

* Minor fix

* Fix formatting

* Update model tests

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['366d88164d6f7bb998dcc789924acecc30aa48e2'],False,"['deepspeed_light.py', 'deepspeed_zero_config.py', 'deepspeed_zero_optimizer.py', 'zero_optimizer_stage1.py', 'run_checkpoint_test.py']"
607814feb9b44977b83812dc38ee952c8ae61f84,Fix bug in fp32 optimizer state loading (#289),['7ccc9daf9ec909b71c7261f8e77876774b1633af'],False,"['deepspeed_light.py', 'simple_model.py', 'test_checkpointing.py']"
376818ef9df2d9d8932beb7adde10f89b3653570,"Empty grad fix (#291)

* empty grad fix
* add unit tests for empty grad",['607814feb9b44977b83812dc38ee952c8ae61f84'],False,"['deepspeed_light.py', 'simple_model.py', 'test_fp16.py']"
1f972427fc6c9b22da97ca8b89c194bfb3988d6f,only global rank 0 can log tensorboard data; avoid multi gpu/node race for the log directory (#296),['376818ef9df2d9d8932beb7adde10f89b3653570'],False,['deepspeed_light.py']
3cc96e17f1e313148da7d0d349392c584c286ed8,"Avoid deadlock for unsynchronized non-zero checkpointing (#297)

* Avoid deadlock for unsynchronized non-zero checkpointing

* Fix formatting issues

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['871f7e63056be679cbe5784ab4cc1da347648940'],False,['deepspeed_light.py']
e50b8838e0aad4edc18cde5b0b89c7774943f6f7,"Fix nv_peer_mem version (#304)

* fix nv_peer_mem version in dockerfile

* fix security issue, remove pillow dependency (this is only needed for cifar example which has its own requirements.txt)",['97c5427372fa86a3c31c7d9f1662c1e806674da1'],False,"['Dockerfile', 'requirements.txt']"
c35e9441d54d76b2eecf1b252621507a0dc952b8,"Removing () from assertion. (#307)

The parenthesis alter the evaluation of the assert() and it will always evaluate to True.",['9d07d756aa935ceb650e0e67a90298e57b29627d'],False,['zero_optimizer_stage1.py']
29c5fe26112f417c25955baab8e9dc7d7f33c5e4,"Add webinar link (#309)

Add webinar on-demand links and update readme",['c35e9441d54d76b2eecf1b252621507a0dc952b8'],False,"['README.md', '2020-05-19-press-release.md', '2020-05-19-zero-stage2.md', '2020-05-28-fastest-bert-training.md', '2020-07-24-deepspeed-webinar.md', '2020-08-07-webinar-on-demand.md']"
cd68e6e55a2155ffaea681b18e012aafd686dce4,"Fix+tests for get_lr from lr_scheduler before training starts (#310)

* add fix and tests for get_lr from lr_scheduler before training starts",['903a41a59d6d5ae8817fbdbfd5014efc0f7356f8'],False,"['deepspeed_lr_schedules.py', 'test_lr_schedulers.py']"
892ece67696c5153db2bc871ecb8be7339b33c73,bumping DSE commit for pillow security fix (#312),['cd68e6e55a2155ffaea681b18e012aafd686dce4'],False,['DeepSpeedExamples']
7240abf3550e20eeb361434ef342605640bc3847,"Samyamr/grad acc stage2 (#338)

* Adding gradient accumulation support for ZeRO Stage 2. Changing all Megatron-LM tests to also test gradient accumulation

* Gradient Accumulation support for Stage 2. Model tests added to test the feature

* formatting

* Update deepspeed_light.py

removing comment

* Update ds_config_func_bs8_zero1.json

reverting this file back. Its not needed for this PR

* defining baseline prefix

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['458c0d92f54bc4895d743d7ce1be80d8f738c48d'],False,"['deepspeed_light.py', 'deepspeed_zero_optimizer.py', 'ds_config_func_bs8_zero0_gas10.json', 'ds_config_func_bs8_zero2_gas10.json', 'run_func_test.py', 'test_common.py']"
f4726b75c45d0535301a89d10a2b2bf6b337e020,"Update run_func_test.py

Renaming config files to gas3",['6122a74ede9a878f514912a421468521e7de4e1f'],False,['run_func_test.py']
e5bbc2e559aeae7736e581a7495909e473c2e37b,"Sparse attn + ops/runtime refactor + v0.3.0 (#343)

* Sparse attn + ops/runtime refactor + v0.3.0

Co-authored-by: Arash Ashari <arashari@microsoft.com>

Co-authored-by: Arash Ashari <arashari@microsoft.com>",['838f53b7613d65fd8260bb76331ce5b8b0911848'],False,"['.clang-format', '.gitignore', 'Dockerfile', 'azure-pipelines.yml', 'basic_install_test.py', 'ds', 'utils.cpp', '__init__.py', '__init__.py', 'launch.py', 'runner.py', '__init__.py', '__init__.py', 'fused_lamb.py', '__init__.py', 'bert_sparse_self_attention.py', 'matmul.py', 'softmax.py', 'sparse_attention_utils.py', 'sparse_self_attention.py', 'sparsity_config.py', '__init__.py', 'matmul.tr', 'softmax_bwd.tr', 'softmax_fwd.tr', '__init__.py', 'transformer.py', '__init__.py', '__init__.py', 'checkpointing.py', 'config.py', 'config.py', 'config_utils.py', 'constants.py', 'csr_tensor.py', 'dataloader.py', 'engine.py', '__init__.py', 'fused_optimizer.py', 'loss_scaler.py', 'unfused_optimizer.py', 'lr_schedules.py', 'utils.py', '__init__.py', 'config.py', 'stage1.py', 'stage2.py', 'utils.py', '__init__.py', 'logging.py', 'timer.py', 'config-json.md', '2020-09-09-sparse-attention.md', 'sparse_attention.md', 'transformer_kernel.md', 'sa_backward_pass.png', 'sa_bert_base_time_result.png', 'sa_bert_large_time_result.png', 'sa_fixed_sparsity_structure.png', 'sa_forward_pass.png', 'sa_gpt2_time_result.png', 'sa_long_document_comprehension_result.png', 'sa_maximum_sequence_runnable_on_bert.png', 'sa_variable_sparsity_structure.png', 'variable_sparsity_pattern.png', 'install.sh', 'requirements-dev.txt', 'requirements-sparse-attn.txt', 'requirements.txt', 'setup.py', 'skip_sparse_attention.py', 'test_checkpointing.py', 'test_config.py', 'test_csr.py', 'test_ds_config.py', 'test_run.py']"
1661e83032d32bb61451ee2f4aaf31e68c901bef,update DSE and rename SA tests,['5518aae5f9a317df9e41b1c1e14d59234c0b2d98'],False,"['DeepSpeedExamples', 'test_sparse_attention.py']"
1ebcd6c50af7bc5c41507c60961f9031c729ec8c,Update test_sparse_attention.py,['1661e83032d32bb61451ee2f4aaf31e68c901bef'],False,['test_sparse_attention.py']
a64b0abbcc8fb0be18b6572982e95641a6871e01,fixed a typo; this was fixed before but seems like it has been lost in the refactor (#364),['ac12833ea790932eb2c23747375279ebc9aecfdf'],False,"['config.py', 'constants.py']"
9dadf38dd637d283d0869837cd5324f4086cbe1c,"Update Sparse Attention Tutorial (#357)

* adding BingSqaud e2e test

* updating the draft test; bring final step under try section

* finalizinf test for base deepspeed and deepspeed with ZeRO

* applying the comment (thanks Jeff); fixed formatting

* update Sparse Attention Tutorial

* fixed few issues and applied comments for better organization and readability

* updated sparse attention tutorial with making how to use section incremental; applying more comments

Co-authored-by: arashashari <arashashari@ArashMSLaptop.redmond.corp.microsoft.com>",['9e83ef21ea842a56ed6ba4c7b6952eaa6efd5cca'],False,"['navigation.yml', '2020-09-09-sparse-attention.md', 'sparse-attention.md']"
01726ce2b8ec1adbffae7974b5bfe600962c2043,"Add 1-bit Adam support to DeepSpeed (#380)

* 1-bit adam (#353)

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>
Co-authored-by: Your Name <you@example.com>
Co-authored-by: tanghl1994 <htang14@ur.rochester.edu>
Co-authored-by: Hank <tanghl1994@gmail.com>
Co-authored-by: root <root@node2x12b.cs.rochester.edu>
Co-authored-by: Ammar Ahmad Awan <awan.ammar@microsoft.com>",['234bba0cdc7517647e58764a2d5b99375b0d0579'],False,"['azure-pipelines.yml', 'basic_install_test.py', 'constants.py', 'launch.py', 'multinode_runner.py', 'runner.py', 'config.py', 'custom_collectives.py', 'engine.py', 'fused_optimizer.py', 'onebit_adam.py', 'utils.py', 'getting-started.md', 'onebit-adam.md', 'install.sh', 'requirements-1bit-adam.txt', 'setup.py', 'test_com_reduce_cuda.py', 'test_com_reduce_host.py', 'test_server_error.py']"
161e8e60c6df5a74b3b607bc97bc889fa68a63bb,"fixing a link issue with SA tutorial (#387)

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['01726ce2b8ec1adbffae7974b5bfe600962c2043'],False,['2020-09-09-sparse-attention.md']
41db1c2f033b82485a3ef7bd0a986d1a1b579519,"ZeRO-Offload release (#391)

* ZeRO-Offload (squash) (#381)

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Reza Yazdani <reyazda@microsoft.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>
Co-authored-by: Jie <37380896+jren73@users.noreply.github.com>
Co-authored-by: Arash Ashari <arashari@microsoft.com>
Co-authored-by: Reza Yazdani <reyazda@microsoft.com>
Co-authored-by: Samyam Rajbhandari <samyamr@microsoft.com>
Co-authored-by: Shaden Smith <Shaden.Smith@microsoft.com>
Co-authored-by: arashashari <arashashari@ArashMSLaptop.redmond.corp.microsoft.com>
Co-authored-by: RezaYazdaniAminabadi <44502768+RezaYazdaniAminabadi@users.noreply.github.com>
Co-authored-by: Reza Yazdani <reyazda@microsoft.com>
Co-authored-by: Samyam Rajbhandari <samyamr@microsoft.com>
Co-authored-by: Shaden Smith <Shaden.Smith@microsoft.com>",['79093d74aa1632e5eac937480774b6fd29084228'],False,"['cpu_adam.cpp', 'custom_cuda_kernel.cu', 'cpu_adam.h', 'custom_cuda_layers.h', '__init__.py', '__init__.py', '__init__.py', 'cpu_adam.py', 'deepspeed_zero_utils.py', 'config.py', 'constants.py', 'engine.py', 'config.py', 'constants.py', 'stage1.py', 'stage2.py', 'utils.py', 'features.md', 'index.md', 'install.sh', 'requirements.txt', 'setup.py', 'ds_config_func_bs4_zero1.json', 'ds_config_func_bs4_zero2.json', 'ds_config_func_bs4_zero2_offload.json', 'ds_config_func_bs8_no_zero.json', 'ds_config_func_bs8_zero0_gas3.json', 'ds_config_func_bs8_zero1.json', 'ds_config_func_bs8_zero2.json', 'ds_config_func_bs8_zero2_gas3.json', 'ds_config_func_bs8_zero2_offload.json', 'ds_config_func_scheduler.json', 'ds_config_perf_bs16.json', 'ds_config_perf_bs32.json', 'ds_config_perf_bs8.json', 'ds_gpt2_test.sh', 'run_checkpoint_test.py', 'run_func_test.py', 'adam_test.py', 'adam_test1.py', 'test_adam_acuracy.py', 'test_checkpointing.py', 'test_dynamic_loss_scale.py', 'test_fp16.py']"
65c2f974d8308b3bd13036623fb1474caa535f82,"Pipeline parallel training engine. (#392)

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['41db1c2f033b82485a3ef7bd0a986d1a1b579519'],False,"['__init__.py', '__init__.py', 'checkpointing.py', 'config.py', 'dataloader.py', 'engine.py', 'fused_optimizer.py', 'unfused_optimizer.py', '__init__.py', 'engine.py', 'module.py', 'p2p.py', 'schedule.py', 'topology.py', 'utils.py', 'stage1.py', '__init__.py', 'timer.py', 'navigation.yml', 'pipeline.md', '3d-parallelism.png', 'pipe-schedule.png', 'conf.py', 'index.rst', 'pipeline-extending.rst', 'pipeline.rst', 'install.sh', 'common.py', 'simple_model.py', 'test_checkpointing.py', 'test_data.py', 'test_partition.py', 'test_pipe.py', 'test_pipe_module.py', 'test_pipe_schedule.py', 'test_runtime_utils.py', 'test_topology.py']"
dca0b7841e4449e9a03d0f603df634b35d2e015a,"Fix datatype issue with sparse attention softmax (#363)

Fixes a dataype issue with softmax where the number of blocks being sent to the Triton kernel source was a torch.Tensor but should have been a python integer. On some environments (e.g., conda) this resulted in triton not knowing how to serialize the input (and crashing in our tests). Once switching to the correct datatype that triton expects this seems to have solved the issue.

Co-authored-by: Shaden Smith <Shaden.Smith@microsoft.com>",['093f09ff27ba3350b6e014a524075240612d5187'],False,['softmax.py']
b1d4bd734bffc99c0684edbec201fe347381c30b,fix for 16GB v100 nodes (#393),['2dea61f285ba245171a93672a90d7652b2608a30'],False,"['stage2.py', 'test_fp16.py']"
e549be607c0f85fc3eb91b3ce977f1d063d65f3c,"supporting different intermediate sizes other than 4 * hidden_dim (#389)

* supporting different intermediate sizes other than 4*hidden_dim

* run precommit

* uncommnet the unit tests

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['c82756cd1571acde62916554446c5a6b324b5f20'],False,"['ds_transformer_cuda.cpp', 'transformer.py', 'test_cuda_backward.py', 'test_cuda_forward.py']"
4ac9bf60a740a5a8433ee256c48527c0fba63beb,"Revert ""supporting different intermediate sizes other than 4 * hidden_dim (#389)"" (#404)

This reverts commit e549be607c0f85fc3eb91b3ce977f1d063d65f3c.",['e549be607c0f85fc3eb91b3ce977f1d063d65f3c'],False,"['ds_transformer_cuda.cpp', 'transformer.py', 'test_cuda_backward.py', 'test_cuda_forward.py']"
91b4a93db01318cf7ad39b33ee3b2d2715abac54,"pytest skips for tests requiring certain ops (#411)

* add pytest skips around tests that require certain ops to be installed",['473ff985fb8770fbbd9073554a666aac683d57d2'],False,"['test_adam_acuracy.py', 'test_checkpointing.py', 'test_cuda_backward.py', 'test_cuda_forward.py', 'test_dynamic_loss_scale.py', 'test_fp16.py', 'test_sparse_attention.py']"
55ed105771d08fbffc0cb6d8cd56a2e61206ad1d,fix bug related to stitching reduced grads across communication partitions (#318),['91b4a93db01318cf7ad39b33ee3b2d2715abac54'],False,['stage1.py']
a9e8325f15d1537eba37fa6725e6cc12e60ee976,"add cpu-adam, reformat, add colors (#413)",['55ed105771d08fbffc0cb6d8cd56a2e61206ad1d'],False,['basic_install_test.py']
0e942df008a2540c729d985bc1abdce8a98c7168,"Add Linear warmup+decay lr schedule (#414)

Update lr schedule unit tests",['a9e8325f15d1537eba37fa6725e6cc12e60ee976'],False,"['lr_schedules.py', 'test_lr_schedulers.py']"
7d91be97652b464d389c7aa39a3a657a88f2d181,"Minor doc fixes (#417)

* Update installation instructions

* Format fix

* ZeRO tutorial

* Format fixes

* ZeRO-Offload

* ZeRO and ZeRO-Offload tutorials

* Update navigation page

* Format fixes

* Add yuxhe feedback

* Fix blog post link

* Fix OneBit-Adam link
Tweak scheduler example

* Fix date link

Co-authored-by: Shaden Smith <Shaden.Smith@microsoft.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['0e942df008a2540c729d985bc1abdce8a98c7168'],False,"['config-json.md', '2020-09-09-onebit-adam-news.md', 'onebit-adam.md']"
f5cce75e7061a6736f3021fd5a5a0c744704ed4f,"Overflow fix (#416)

* Switches fused_optimizer overflow calculation",['7d91be97652b464d389c7aa39a3a657a88f2d181'],False,['fused_optimizer.py']
4fef478f57fb27053f3d6f2ceab25ae635ec34a1,"Fix a typo in comments (#415)

* Update stage2.py

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['f5cce75e7061a6736f3021fd5a5a0c744704ed4f'],False,['stage2.py']
c66f3881568c6ff7e77c6160dae20b5fea9fe4dd,Fix few typos in the docs (#418),['5812e84544fe5adc9c82593d15edf8d377ab902c'],False,['megatron.md']
01b6e27e7833a2ac3b415b03674ef11116078e9c,"Activation checkpointing bugfix and unit tests (#420)

* Activation checkpointing bugfix and unit tests.

* Activation checkpointing bugfix and unit tests.

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['5bc7d4e1e6364f6d39d3320d9d9d09d9dba53e3f'],False,"['checkpointing.py', 'test_activation_checkpointing.py']"
a74a604a9e9dee5167b0ac94717674ae2902c494,"Revert ""Activation checkpointing bugfix and unit tests (#420)"" (#422)

This reverts commit 01b6e27e7833a2ac3b415b03674ef11116078e9c.

Co-authored-by: Shaden Smith <ShadenTSmith@gmail.com>",['01b6e27e7833a2ac3b415b03674ef11116078e9c'],False,"['checkpointing.py', 'test_activation_checkpointing.py']"
a825f99688b12d5b881fb2b578ac1d3fc956d138,Fix activation checkpoint unit tests for GPU systems (#421),['a74a604a9e9dee5167b0ac94717674ae2902c494'],False,"['checkpointing.py', 'test_activation_checkpointing.py']"
a148bd33d6cabdc363754e622a9aa24493ef7512,Add configurable intermediate size to transformer kernels (#423),['a825f99688b12d5b881fb2b578ac1d3fc956d138'],False,"['context.h', 'ds_transformer_cuda.cpp', 'transformer.py', 'test_cuda_backward.py', 'test_cuda_forward.py']"
f0f2a7026877d3fd2f6f5565a67cdffc89750cf0,"support dynamic sequence length in transformer kernels (#424)

Co-authored-by: Conglong Li <conglong.li@gmail.com>",['71f7df3945c467c851888b7ab274796ea57bb67a'],False,"['context.h', 'custom_cuda_layers.h', 'dropout.h', 'ds_transformer_cuda.h', 'gelu.h', 'normalize_layer.h', 'softmax.h', 'strided_batch_gemm.h', 'cublas_wrappers.cu', 'ds_transformer_cuda.cpp', 'gelu_kernels.cu', 'general_kernels.cu', 'normalize_kernels.cu', 'softmax_kernels.cu', 'transformer.py', 'test_cuda_backward.py', 'test_cuda_forward.py']"
5d40f006bbcf6aeb5f7e8bff1f33a00422aefdd1,"Fix urls in tutorial (#436)

* url fix

* revert absolute path but keep some actual fix

* add real readme",['f0f2a7026877d3fd2f6f5565a67cdffc89750cf0'],False,"['README.md', 'README.md', 'sparse-attention.md', 'zero-offload.md', 'zero.md']"
6d176c45fa7942ed236e02d193d26732f5816969,link fix part two :-) (#441),['0ca82156e1799672a9522e539c65f379bdb91a2a'],False,['pipeline.md']
5412a3341fa5d0211629ee87899015f98a62e0cc,unit test rename (#442),['6d176c45fa7942ed236e02d193d26732f5816969'],False,['test_cpu_adam.py']
6f28ea30958f615191c4ca3ef5b3ada13b5cc4e7,fix typos (#446),['5412a3341fa5d0211629ee87899015f98a62e0cc'],False,"['p2p.py', 'stage2.py']"
7b8be2a7d9a56847c30cd9db30a43b6dcfd975fa,"Disable default installation of CPU Adam (#450)

* Disable default installation of CPU Adam

* Handle cpufeature import/use errors separately",['6f28ea30958f615191c4ca3ef5b3ada13b5cc4e7'],False,"['stage2.py', 'setup.py', 'test_cpu_adam.py']"
95575579b3304f3cdd6bc1f9715535d424768b87,"Use parentesis around min and max to enable Windows build (#449)

* Towards Windows build

* formatting

Co-authored-by: Bruno Cabral <bruno@potelo.com.br>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['7b8be2a7d9a56847c30cd9db30a43b6dcfd975fa'],False,"['context.h', 'gemm_test.h', 'ds_transformer_cuda.cpp']"
11cf47efe1729a68206a743a2e0f740f9d148234,temporarily disable lr unit tests,['67176383c4195d3c13a91e78b6f8701d9d579dec'],False,['test_lr_schedulers.py']
679fc13512a74c039ac8881d557185c9860ac18a,turning off different tests (temp),['11cf47efe1729a68206a743a2e0f740f9d148234'],False,"['test_lr_schedulers.py', 'test_pipe.py']"
2efea6944616c7be9e35874adc37dbaf150ea05e,"gan tutorial (#462)

* gan tutorial

* formatting fix

* adding pointer to repo; adding navigation link

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['679fc13512a74c039ac8881d557185c9860ac18a'],False,"['navigation.yml', 'gan.md']"
c39a76fed71f87fd0b026cf6dfd1aee8283319ed,"Fix printing momentum for non-deepspeed optimizer (#464)

* Fix printing momentum for non-deepspeed optimizer

Fix printing momentum for non-deepspeed optimizer

* fix momentum access for Adam

fix momentum access for Adam",['2efea6944616c7be9e35874adc37dbaf150ea05e'],False,['engine.py']
23fc48f320bae0eeb46058d6b949c79b1b6569dd,"Add DeepSpeed_Adam optimizer (#468)

* Update installation instructions

* Format fix

* ZeRO tutorial

* Format fixes

* ZeRO-Offload

* ZeRO and ZeRO-Offload tutorials

* Update navigation page

* Format fixes

* Add yuxhe feedback

* Fix blog post link

* Fix OneBit-Adam link
Tweak scheduler example

* Fix date link

* Add DeepSpeed_Adam

Co-authored-by: Shaden Smith <Shaden.Smith@microsoft.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['c39a76fed71f87fd0b026cf6dfd1aee8283319ed'],False,['config-json.md']
f5aa2547d88f145c4b5223d5bb8f79db69ee5288,"Add CPUAdam optimizer for zero-offload in deepspeed engine (#484)

* add adamW to CPU-ADAM implementation

* supporting cpu-adam optimizer for zero-offload on deepspeed side

* bump DSE to match cpu-adam updates

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['d720fdb6857f4b71d922ca1e8efbe5271b5fb7c2'],False,"['DeepSpeedExamples', 'cpu_adam.cpp', 'cpu_adam.h', '__init__.py', 'cpu_adam.py', 'config.py', 'engine.py', 'stage2.py', 'config-json.md', 'optimizers.rst', 'test_cpu_adam.py']"
7d4d742bf03f8e1707130391e0b39bd6d93a702a,"Fixing CPU-Adam convergence issue (#503)

* fixing cpu-adam

* fixing copy with optimizer for data and model parallelism

* fixing cpu-adam

* fix cpu-adam

* fix cpu-adam",['4c37d705202f626682a5128a2fc4f4bcfb997fdc'],False,"['cpu_adam.cpp', 'cpu_adam.h', 'cpu_adam.py', 'stage2.py']"
e351090c6cbf3f92f7f875ce4233230d936f2baa,"PLD documentation (#514)

* PLD documentation

* Formatting fixes",['7d4d742bf03f8e1707130391e0b39bd6d93a702a'],False,"['navigation.yml', '2020-10-28-progressive-layer-dropping-news.md', 'progressive_layer_dropping.md', 'index.md']"
41fb24b3f06bd3d7e8b5fa6c71958cc6234910aa,"Fix PLD news url (#515)

* PLD documentation

* Formatting fixes

* Fix url bug",['e351090c6cbf3f92f7f875ce4233230d936f2baa'],False,['index.md']
be1147c08acde34d7a13b73f8d33e55c3f719de4,"PLD release (#513)

* Progressive layer dropping docs (#499)

* test

* Adding tutorial and news page for pld

* updating the tutorial and posts of PLD

* update the finetune tutorial

* Update PLD tutorial (#512)

* Update installation instructions

* Format fix

* ZeRO tutorial

* Format fixes

* ZeRO-Offload

* ZeRO and ZeRO-Offload tutorials

* Update navigation page

* Format fixes

* Add yuxhe feedback

* Fix blog post link

* Fix OneBit-Adam link
Tweak scheduler example

* Fix date link

* Add DeepSpeed_Adam

* Add PLD tutorial to navigation

Co-authored-by: Shaden Smith <Shaden.Smith@microsoft.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>

* updating the pld docs

* DeepSpeed implementation of PLD (#508)

* DeepSpeed implementation of PLD

* Format fixes

* Formatting fixes

* Fix broken url

* Address PR feedback

* Bump DSE

Co-authored-by: Minjia Zhang <33713995+minjiaz@users.noreply.github.com>
Co-authored-by: Shaden Smith <Shaden.Smith@microsoft.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>
Co-authored-by: Minjia Zhang <minjiaz@microsoft.com>",['e082d4752e3db2acf4348a4522daba077bb58aec'],False,"['DeepSpeedExamples', 'README.md', 'config.py', 'constants.py', 'engine.py', 'progressive_layer_drop.py', '2020-10-28-progressive-layer-dropping-news.md', 'progressive_layer_dropping.md', 'index.md', 'simple_model.py', 'test_pld.py']"
eea1c285a8271b94e43070d058869fe9ef4576a8,fix bug on non-DLTS infra when no output path set (#523),['be1147c08acde34d7a13b73f8d33e55c3f719de4'],False,['engine.py']
31f46feee2d491d58a13404e354440551de9d5bf,"DeepSpeed JIT op + PyPI support (#496)

Co-authored-by: Shaden Smith <Shaden.Smith@microsoft.com>
Co-authored-by: Reza Yazdani <reyazda@microsoft.com>",['0ad4fd880b90cae452695d9b95de41feb1cfbba0'],False,"['.gitignore', '.gitmodules', 'MANIFEST.in', 'README.md', 'azure-pipelines.yml', 'basic_install_test.py', 'ds_report', 'compat.h', 'custom_cuda_kernel.cu', 'fused_adam_frontend.cpp', 'multi_tensor_adam.cu', 'multi_tensor_apply.cuh', 'flatten_unflatten.cpp', '__init__.py', 'env_report.py', 'git_version_info.py', '__init__.py', '__init__.py', 'cpu_adam.py', 'fused_adam.py', 'multi_tensor_apply.py', 'csrc', '__init__.py', 'fused_lamb.py', 'op_builder', 'matmul.py', 'softmax.py', '__init__.py', 'transformer.py', 'engine.py', 'stage2.py', 'utils.py', '_config.yml', 'advanced-install.md', 'getting-started.md', 'index.md', 'install.sh', '__init__.py', 'builder.py', 'cpu_adam.py', 'fused_adam.py', 'fused_lamb.py', 'sparse_attn.py', 'stochastic_transformer.py', 'transformer.py', 'utils.py', 'requirements-sparse_attn.txt', 'requirements.txt', 'setup.py', 'modelingpreln.py', 'test_checkpointing.py', 'test_cpu_adam.py', 'test_cuda_backward.py', 'test_cuda_forward.py', 'test_dynamic_loss_scale.py', 'test_fp16.py', 'test_sparse_attention.py', 'apex', 'version.txt']"
ca9ab1201ff56815652b7b85496d262c2d89d6c1,"ds_report bug fix on cpu and guard torch import in setup.py (#524)

* on cpu box error gracefully if cuda home doesn't exist

* gaurd against torch import issue

* fix sytax error

* fix import",['31f46feee2d491d58a13404e354440551de9d5bf'],False,"['env_report.py', 'setup.py']"
7752dc5ea1331d10fdbe4962648a71f2c7e6329f,"Fix layout bug in ZeRO Stage 1 checkpoint logic (#531)

* Fix layout bug in ZeRO Stage 1 checkpoint logic
Add elastic checkpoint option for ZeRO stage 1, default to True

* Format fixes",['9941ce75225868ef9222a0360683a563d05d87ad'],False,"['engine.py', 'config.py', 'constants.py', 'stage1.py', 'test_checkpointing.py']"
08c96a1bc6d9c0345b31889bba66d73f20d0b0b5,"ZeRO-1 tune max-elems + bug fix (#532)

* zero-1 memory fix

* auto-tune max elems per comm to reduce padding/comm intervals

* clean-up and added previously missing reduction options

* fix testing backing to work with torch1.7",['fdd81c305ce417bbdf81e1985a73de697ad64e3b'],False,"['engine.py', 'utils.py', 'stage1.py', 'common.py']"
dce054dbba525ec2fd59e7a5d2e06a190d133498,"backwards compatability w. v020 ckpts, fix issue with zero-1 ckpts (#543)",['9de21b72b5e8adb6c1fe4ae96cbddaa929178cc1'],False,"['__init__.py', 'stage1.py']"
d81cb26d92fc8126af7375a171bc1f755feef54a,"Fix setup.py for cpu-only environment installation (#538)

* Add guard to not using `torch.version.cuda` above no-CUDA environment.
* Fix several typos on setup.py.

Signed-off-by: Seunghwan Hong <seunghwan@scatterlab.co.kr>

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['dce054dbba525ec2fd59e7a5d2e06a190d133498'],False,['setup.py']
0178e6cc227e0f6d60cb5f3675edd1cda9a0396e,"Fix unbalanced gradients bug in ZeRO-2 gradient accumulation (#545)

* Use zero-tensors for missing gradients to avoid size mismatch

* Unit test for unbalanced gradients in ZeRO

* Formatting fixes",['6b28bc5db58fa95628b9cf69e350dcacc2f33478'],False,"['stage2.py', 'simple_model.py', 'test_zero.py']"
6021b70288f53965443f62caeae9a0b4443f1b74,Support non-tensor state in checkpoint (#548),['0178e6cc227e0f6d60cb5f3675edd1cda9a0396e'],False,"['stage1.py', 'simple_model.py', 'test_checkpointing.py']"
00c3a254a9a5915720863b8e281852be64519879,"Bug fix for norm calculation in absence of model parallel group (#551)

In the absence of a model parallel group, model_parallel_allreduce should not do any reduction. This commit fixes the bug which was doing a model parallel allreduce across world group when model parallel group is None",['bcd56f9772a71c50069804fbb2f5739cc6f8c776'],False,['stage2.py']
eec44af1e301cf5c53521a42347f897078c6ae72,Turn back on PP tests (#558),['0e831e23b6d2a200bf805f6169ce49c5ebd93b4d'],False,['test_pipe.py']
73c3262df63e85c2b2f0d48bf9217c58508e44f3,bump to 0.3.6 and fix manifest to include reqs (#561),['600971365315b3a50580d0f96d25db28d915486b'],False,"['MANIFEST.in', 'version.txt']"
17f36f1b2ecdcfcd42d83c6f5526aff7ac6fb0c4,"[doc] typo fix and clarification (#563)

This PR:
* fixes a misspelled method name
* also `( () )` doesn't read too well, until one reads the code and understands that it's not a formatting bug. I proposed to simply say that it's a callable object.",['c51fa65de847ba44f0a1bcfc9957cb4e5fae3ab6'],False,['getting-started.md']
c78c29f9384ec8a3addb049504a4167508fe1e4d,"supporting different hidden dimensions (#559)

* supporting different hidden dimensions

* add support for larger hidden dimensions (greater than 8K)

* remove empty line

* add loop unrolling factor for dropout kernels

* update different kernels based on the reviews

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['17f36f1b2ecdcfcd42d83c6f5526aff7ac6fb0c4'],False,"['custom_cuda_layers.h', 'cublas_wrappers.cu', 'dropout_kernels.cu', 'general_kernels.cu', 'normalize_kernels.cu', 'transform_kernels.cu', 'test_cuda_backward.py', 'test_cuda_forward.py']"
9f52a36fad37bcb26e19dfc767d4405310f52399,"tracking optimizer step in cpu-adam when loading checkpoint (#564)

* tracking optimizer step in cpu-adam when loading checkpoint

* add warning/error message for updating optimizer step count

* resolve build issue

* supporting state update from the python side

* track step from python in all cases

* remove comma",['c78c29f9384ec8a3addb049504a4167508fe1e4d'],False,"['cpu_adam.cpp', 'cpu_adam.h', 'cpu_adam.py']"
845921b3b646a640ef5529207e70e06335c43a52,Add 'latest' checkpoint save/load support (#569),['7a75f8b36f596c87f538a3642cb6eeb16acf2898'],False,"['engine.py', 'test_checkpointing.py']"
1e44d48d5377a5f839c53ccefcd6951625479f51,"Fix potential random layout inconsistency issues in sparse attention modules (#534)

* 1) Register layout as buffer of module so that we can save/load checkpoint; 2) Add a broadcast of layout at the beginning to ensure different processes will have consistent layout during distributed training.

* Add docstring for max_seq_length argument in SparseSelfAttention

Co-authored-by: Zhun Liu <zhunliu@microsoft.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['ff58fa7e5a4f637a21d11daad0192683fe50ed15'],False,['sparse_self_attention.py']
d901a6d2f5c4539ff30553759cfbc7f197e8ec00,"Pin triton to 0.2.3 for now, 0.3.0 is broken",['2f6269787a10ac336f80c5ca4342823184cecd6a'],False,['requirements-sparse_attn.txt']
8a184b6b1ddfe4bf41467c06b693817af80aa530,"[build] fix computer capability arch flags, add PTX, handle PTX (#591)

* fix arch flags, add PTX

* bug fix

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['0518252d6431133f94e697df9ac46d1efb1a6a2f'],False,['builder.py']
6380ee35116dfc7c2a037d48fbd790d8dbabfc1d,"Fixes for RTD build errors (#606)

Co-authored-by: Shaden Smith <Shaden.Smith@microsoft.com>",['007466e576b65a5efd2c8195c4f1349b1e225c1b'],False,"['git_version_info.py', 'softmax.py', 'conf.py', 'requirements-readthedocs.txt']"
fd2f970bdfd3eada289b4e19a3adcf2c352a4d8f,"Transformer-kernel - supporting any arbitrary sequence-length (#587)

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['6380ee35116dfc7c2a037d48fbd790d8dbabfc1d'],False,"['DeepSpeedExamples', 'ds_transformer_cuda.cpp', 'softmax_kernels.cu', 'transformer.py', 'test_cuda_backward.py', 'test_cuda_forward.py']"
81aeea361da3936b875a678b9cb44596800510b5,"Elastic training support (#602)

Co-authored-by: Samyam Rajbhandari <samyamr@microsoft.com>",['7435b2f10af773b0204e77c3549b2b7df9a7a65b'],False,"['main.yml', 'ds_elastic', '__init__.py', 'config.py', 'constants.py', 'elasticity.py', 'config.py', 'config_utils.py', 'engine.py', 'engine.py', 'builder.py', 'requirements.txt', 'setup.py', 'test_checkpointing.py', 'test_elastic.py', 'version.txt']"
24e07399fc3ee95417d7a582c3e8952e1ac7d567,update SA comp check to fix torch-cpu issue (#631),['81aeea361da3936b875a678b9cb44596800510b5'],False,['sparse_attn.py']
e6ac7311363da55cf22abfe413679dd34b5ad756,Support initialization with dict configuration (#632),['24e07399fc3ee95417d7a582c3e8952e1ac7d567'],False,"['engine.py', 'simple_model.py', 'test_fp16.py']"
a9a83a6fcfcf654d75017453fbb3a476000180ce,"Allow DeepSpeed models to be initialized with optimizer=None (#469)

Allow DeepSpeed models to be initialized with optimizer=None

Co-authored-by: Shaden Smith <Shaden.Smith@microsoft.com>",['e6ac7311363da55cf22abfe413679dd34b5ad756'],False,"['engine.py', 'test_config.py']"
d38ad6a17164b9bf07477ceb17ca5c7f09657720,change dist to torch.distributed to fix bug in assert. (#638),['a9a83a6fcfcf654d75017453fbb3a476000180ce'],False,['distributed.py']
5ab12795958dff0b56d3faf7421abe9cad7bcba1,Fix docstring format (#640),['46d2e2872b64ebccb8bf4eb5c8a3a55f9adaaa6c'],False,['engine.py']
44bd538b110ce0e8fc69626854631c3aee0dc094,"Module replacement support (#586)

Co-authored-by: Reza Yazdani <reyazda@microsoft.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['5ab12795958dff0b56d3faf7421abe9cad7bcba1'],False,"['__init__.py', 'inject.py', 'replace_module.py', '__init__.py', 'module_inject.py', 'transformer.py', 'utils.py', 'bert-pretraining.md', 'transformer_kernel.md', 'test_cuda_backward.py', 'test_cuda_forward.py']"
bc046dc40b85a9c0d3dc35ad949aceb6d8b91efd,add additional validation checks in elastic config (#646),['828d75bad7cea9c62ea60a2a53d0f486ac4cc342'],False,"['config.py', 'constants.py', 'test_elastic.py']"
da5563a9c1b7f89ed31da849940ec8b039660477,"LR scheduler unit tests (#429)

* Add Linear warmup+decay lr schedule
Update lr schedule unit tests

* LR scheduler unit tests for LR Range Test and 1Cycle

* Disable yapf to preserve parameterizaton

* Disable test_pipe.py for CI debugging

* Disable test_lr_scheduler for CI debugging

* Disable test_lr_scheduler for CI debugging

* Enable all unit tests for CI debugging

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['c14b839d9898f4c84e372e896e3ce8fa2e169a79'],False,"['lr_schedules.py', 'test_lr_schedulers.py', 'test_pipe.py']"
adcfd2694da3daa88efcf00bb58175908415b458,"Handle actvitation checkpointing args that are None or non-tensors (#660)

Special thanks to @g-karthik for tracking this issue down.",['da5563a9c1b7f89ed31da849940ec8b039660477'],False,"['checkpointing.py', 'test_activation_checkpointing.py']"
e2fbe4d23850150bd2b8b3e59ed08f97ff5c4452,"squash latest flops profiling changes (#1) (#664)

Co-authored-by: Cheng Li <pistasable@gmail.com>

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['adcfd2694da3daa88efcf00bb58175908415b458'],False,"['__init__.py', 'config.py', 'constants.py', 'README.md', '__init__.py', 'profiler.py', 'config.py', 'engine.py', 'test_flops_profiler.py']"
f032e56f8af4d40bdbb74d15fa9c5cd63f8b7a40,Validate consistent ckpt tags across ranks (#667),['981bc7d4934c70a6f00a33c5a7946352c3ee76cb'],False,"['__init__.py', 'config.py', 'constants.py', 'engine.py', 'test_checkpointing.py']"
865104be85902ca398038045ad9cf94ec7d48745,Support optimizer AdamW type (#670),['f032e56f8af4d40bdbb74d15fa9c5cd63f8b7a40'],False,"['config.py', 'engine.py', 'utils.py', 'test_fp16.py']"
7b07e123067bbc49cadd77270e8d898c86f71aaa,"doc fix (#651)

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['e729a3fd995ec1421bf2f9ad0ad8075fca008735'],False,['config-json.md']
e59ba12d4df2e61e85abbd2c82a6097ac3722d7a,make test_pipe more stable (#683),['7b0bee0b135589eceebbc7c105cd9826de6d5097'],False,['test_pipe.py']
34c83a5a64da67ff186eec4fdd7ab3c1badf0486,"Fix ZeRO 2 + Pipelining (#677)

* Fix ZeRO 2 + Pipelining",['e59ba12d4df2e61e85abbd2c82a6097ac3722d7a'],False,['engine.py']
5221832e1e303fba07355853425dae76f6bb86a9,"Fix wrong idx bug in invertible LayerNormBackward1 (#692)

* fix wrong idx bug in invertible LayerNormBackward1

this index bug cause wrong scale grad

* fix unexpected deletion

* fix idx for LayerNormBackward1_fused_add

* move pos defination in LayerNormBackward1 kernels

* fix format error

Co-authored-by: Reza Yazdani <reyazda@microsoft.com>",['852c524a1cfb111f77adab1e1ed55ebc298edc8d'],False,['normalize_kernels.cu']
91b1b7f33ec76b326ad7d992e8578926c267fb5c,[transformer-kernel] turn off unit test printing (#701),['cd29f8b8d5db9b75db1aed1f7b09dee98917b7f2'],False,"['torch16.yml', 'test_cuda_backward.py']"
2e2dd861f309f64bafbb4e3d0126e953f2ed4d18,"Dist testing backend fixes, etc. (#708)",['91b1b7f33ec76b326ad7d992e8578926c267fb5c'],False,"['engine.py', 'distributed.py', 'common.py', 'simple_model.py', 'test_checkpointing.py', 'test_cuda_backward.py', 'test_dynamic_loss_scale.py', 'test_fp16.py']"
b08aa6f3cd5f5cdf805edf3dcd394e8cb54f1882,"Improve starred expressions (#696)

* Improve starred expressions

`deepspeed/profiling/flops_profiler/profiler.py` uses starred expressions
that are no longer valid with [PEP 617][1]. The new Python parser is in 3.9,
and this change allows DeepSpeed to run with the newest Python version. I have
not checked all locations that has this issue. However, this change allows me
to run simple examples.

[1]: https://www.python.org/dev/peps/pep-0617/

* Match style for ""Improve starred expressions"", although readability suffers

The style guide might need to be updated for this new use case of expressions.
Python [Issue 40631][1] includes more discussion on the change.

[1]: https://bugs.python.org/issue40631

Co-authored-by: Cheng Li <pistasable@gmail.com>",['4f1d827c52f61e9379b787f20ca533e1e71a8275'],False,['profiler.py']
c5b3f40e8481748f9658a19c2df1f17c5b579919,Fixed typo in Readme. (#737),['b08aa6f3cd5f5cdf805edf3dcd394e8cb54f1882'],False,['README.md']
6ee3b296da69cfc516f8c8db2109c3c53cbe7e93,"Clickable screenshots (#746)

* Fix docstring

* Make screenshots clickable for easier viewing",['6beca3cee41db044a11985f4a563b9f7911a114d'],False,['zero.md']
e2dfe0d17be21ce08e759dab4712bc37debd44c4,"Add flops profiler tutorial (#682)

* work on flops profiler tutorial

* update flops profiler tutorial

* add flops profiler tutorial and fix names

* work on flops profiler tutorial

* update flops profiler tutorial

* add flops profiler tutorial and fix names

* fix tailing ws

* fix names

* remove multistep profiling and update docs

* fix cases where functionals and submodules coexist in a parent module, update readme

* fix typo

* always invoke post hook function

* fix module flops sum and update tests

* update tutorial",['6ee3b296da69cfc516f8c8db2109c3c53cbe7e93'],False,"['out2', 'config.py', 'constants.py', 'README.md', 'profiler.py', 'engine.py', '_config.yml', 'navigation.yml', 'config-json.md', 'features.md', 'flops-profiler.md', 'test_flops_profiler.py']"
1b8ca8ece592faf0cdf15f3531fa0bc4929ded36,fix spelling mistake (#749),['59eed176295048a10b4f0ea4d24d07286fc63d64'],False,['onebit_adam.py']
248f6383a1426390a33981793a22fe3dec78b855,"1-bit Adam documentation fix (#747)

* 1-bit adam doc fix

* 1-bit adam doc fix

* 1-bit adam doc fix

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['1b8ca8ece592faf0cdf15f3531fa0bc4929ded36'],False,"['README.md', 'onebit-adam.md', 'index.md']"
78e776a9ace5dc8007be43b536ad0f732688169d,"[install] fixes/improvements/docs (#752)

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['6fb16100ba356d7f31b9f483245576128b1bba0d'],False,"['advanced-install.md', 'install.sh']"
ec8b1cb0a0a5752bba029da4bdc91616c0f5bec7,"Activation checkpointing for non-tensor arguments and return values (#741)

* Activation checkpoint support for non tensor input/output

* Format fixes

* Address PR comments; Add ordering edge case tests",['7bf1b837a478fc773dbd5fc539bba113b8ac550c'],False,"['checkpointing.py', 'utils.py', 'test_activation_checkpointing.py']"
c28a71f95f1994bf6e2dc7e9e19b2e60a8cb260d,"Minor doc tweaks (#761)

* Fix docstring

* Make screenshots clickable for easier viewing

* Navigation menu in alphabetical order; More clicable screenshots

* Rename 1Cycle doc

* Tweak naming",['7cab55c79178fce9a154abe4e7b5b7abd82648a1'],False,"['_config.yml', 'navigation.yml', 'one-cycle.md', 'zero-offload.md']"
8067efa41cd3a6fa67d1a7495ccceb27682bc7e5,Fix NameError: name 'dist' is not defined (#763),['c28a71f95f1994bf6e2dc7e9e19b2e60a8cb260d'],False,['distributed.py']
1fcc5f7a78c6af0c7e1e4cbc243b31110482591d,Fix transformer kernel CUDA illegal memory access error (#765),['68e138b6dfc0a5eec4e3b50db1a2604d919cb56f'],False,['ds_transformer_cuda.cpp']
ee1ffe2e88595407209ff2a27494d6c107fac234,"CPU-Adam fix for scalar mode (#735)

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['1fcc5f7a78c6af0c7e1e4cbc243b31110482591d'],False,"['cpu_adam.cpp', 'context.h', 'cpu_adam.h']"
e60e92eb0a06673748c4cb63fbcf713ddd12fc22,"[doc] fix incorrect param name (#773)

Invalid param name

Thanks.",['29fa4b2be24c281571d74bb6724d5ba90eba0183'],False,['config-json.md']
e2dfcadf3b30437a6232f6c689e695aa44931a4c,"Fix the bias-add and add the layer-norm-eps parameter (#791)

* fix the bias-add precision and indexing and also adding the layer-norm-eps as a configurable parameter for transformer

* add ACC_HALF config

* use defined to check if ACC_Half is defined",['48065c06d716376c9beb842701435e554b27ad6e'],False,"['ds_transformer_cuda.h', 'normalize_layer.h', 'dropout_kernels.cu', 'ds_transformer_cuda.cpp', 'transform_kernels.cu', 'replace_module.py', 'transformer.py']"
490e6f7ceeb21b5b2dd72c755786822c352d754b,"fixing the compiling issue for the AMD architecture (#796)

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['62396b7121c8d1bca0fd868cabe5d8f4640080ad'],False,['cpu_adam.py']
db987cf1b5971187257a49a7d2d54ca2653052d5,fixed typo (#802),['7eb083c22112b67ee8ea43036479bb7680136349'],False,['topology.py']
937c5ceec1906e1a8f8e21d2dd165f5280050b26,"issue with the implementation of column_sum_reduce (#804)

hi, i take a look at the code of column_sum_reduce, i have 2 questions:
   1. the goal of column_sum_reduce is to get the column sum of inp matrix with shape[rows, width] and the result shape should be [width],right ? It seems that the judgment condition of pos is not suitable
   2. the implementation of cuda kernel based on the asumption that, the thread with same threadIdx.y will group into a thread_block_tile, the blockDim is (32,32), i read the nvidia document https://on-demand.gputechconf.com/gtc/2017/presentation/s7622-Kyrylo-perelygin-robust-and-scalable-cuda.pdf, THREAD BLOCK TILE is a subset of threads of a thread block, divided into tiles in row-major order. doesn't it mean thread with the same threadIdx.x will group into a thread_block_tile ?
thanks !!!!

Co-authored-by: Reza Yazdani <44502768+RezaYazdaniAminabadi@users.noreply.github.com>",['db987cf1b5971187257a49a7d2d54ca2653052d5'],False,['general_kernels.cu']
8295d7a89ecd833d07bbeebf742631343c4b156b,"Fixing gelu_checkpointing memory issue (#812)

* fixing buffers in transformer kernel when gelu-checkpoint is enabled

* fixing the test issue for other memory optimization flags

* fixing a bug for when attn_dropout_checkpoint is enabled",['937c5ceec1906e1a8f8e21d2dd165f5280050b26'],False,['ds_transformer_cuda.cpp']
599258f97992e5a47db9408e4e3622805ce1adb5,"ZeRO 3 Offload (#834)

* Squash stage3 v1 (#146)

Co-authored-by: Samyam <samyamr@microsoft.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>
Co-authored-by: Samyam Rajbhandari <samyamr@microsoft.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Shaden Smith <Shaden.Smith@microsoft.com>
Co-authored-by: Shaden Smith <ShadenTSmith@gmail.com>
Co-authored-by: eltonzheng <eltonz@microsoft.com>

* Fix correctness bug (#147)

* formatting fix (#150)

* stage3 bugfix (API) update and simplified FP16 Z3 tests (#151)

* fp16 Z3 API update and bugfix

* revert debug change

* ZeRO-3 detach and race condition bugfixes (#149)

* trying out ZeRO-3 race condition fix

* CUDA sync instead of stream

* reduction stream sync

* remove commented code

* Fix optimizer state_dict KeyError (#148)

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>

* fix for smaller SGS sizes, ensures each grad is backed by unique tensors (#152)

* Simplifying the logic for getting averaged gradients (#153)

* skip for now

* Z3 Docs redux (#154)

* removing some TODOs and commented code (#155)

* New Z3 defaults (#156)

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>

* formatting

* megatron external params

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Shaden Smith <Shaden.Smith@microsoft.com>
Co-authored-by: Shaden Smith <ShadenTSmith@gmail.com>
Co-authored-by: eltonzheng <eltonz@microsoft.com>",['ba33e86e31b0415b7ae66bb89d57f032faa8b599'],False,"['main.yml', '__init__.py', 'runner.py', 'cpu_adam.py', 'checkpointing.py', 'config.py', 'engine.py', 'utils.py', '__init__.py', 'config.py', 'constants.py', 'contiguous_memory_allocator.py', 'linear.py', 'partition_parameters.py', 'stage3.py', 'test.py', 'utils.py', 'Dockerfile', 'config-json.md', 'zero.md', 'cpu-adam.rst', 'index.rst', 'zero3.rst', 'install.sh', '__init__.py', 'builder.py', 'cpu_adam.py', 'fused_adam.py', 'fused_lamb.py', 'sparse_attn.py', 'stochastic_transformer.py', 'transformer.py', 'utils.py', 'requirements.txt', 'setup.py', 'stage3_test.py', 'test.py', 'test_checkpointing.py', 'test_cuda_backward.py', 'test_fp16.py', 'test_zero_context.py']"
75ffdaf716ae6ebd3341d49b2ca18b94565dd201,Fix zero3 tutorial link,['d7de9165db2b9be7389d4dde931bd4595138b044'],False,['2021-03-08-zero3-offload.md']
af548971f3f90759908c16c254f2e32510522b9e,Fix for RTD,['9c5eee3d5c67ddce53ecc06616e4eec3c86c53f9'],False,['requirements-readthedocs.txt']
2e6692c8addfabe1e3eaa874aae7f05d880030c3,Fix regression in runner (#843),['49496364c05a52c2511937419294087f77e46610'],False,['runner.py']
dd03cff29f0bf5416afa03bc9be323be903d51aa,set adamw_mode default true (follows FusedAdam and < 0.3.11 logic) (#844),['564eb4bd932aaf3cad1ef2f8fe6e21de24002492'],False,"['cpu_adam.py', 'config.py', 'engine.py', 'test_adamw.py']"
29853c3eed8bea58bd04f48edb77ee3be913054d,"less scary overflow notice (#833)

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['dd03cff29f0bf5416afa03bc9be323be903d51aa'],False,"['fused_optimizer.py', 'loss_scaler.py', 'unfused_optimizer.py', 'stage1.py', 'stage2.py']"
e0f36ed5a1ebc7b1ed044bf459f3576f30ff949a,"Add optimizers and schedules to RTD and updated the corresponding part in the website (#799)

* add optimizers and schedules to rtd

* update ds website and fix links

* add optimizers and schedules to rtd

* update ds website and fix links

* add flops profiler to rtd

* fix

Co-authored-by: Shaden Smith <Shaden.Smith@microsoft.com>",['29853c3eed8bea58bd04f48edb77ee3be913054d'],False,"['config.py', 'profiler.py', 'config-json.md', 'flops-profiler.md', 'cpu-adam.rst', 'flops-profiler.rst', 'index.rst', 'optimizers.rst', 'schedulers.rst']"
18a26f3f6051aa0622971bd71fa00f35ec006c2f,"[WarmupDecayLR] fix log(0) & 1/log(1) bugs (#772)

* fix log(0) & 1/log(1) bugs

* simplify

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>
Co-authored-by: Reza Yazdani <44502768+RezaYazdaniAminabadi@users.noreply.github.com>
Co-authored-by: Cheng Li <pistasable@gmail.com>",['311795d07b2b910059729128b9a54e47590a761f'],False,['lr_schedules.py']
458ff028d929b6abbf7d0955ad4b796dbf9a5156,"Bug fix: Remove client optimizer param_group list item that does not have 'params' (#827)

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['35fd7ccd862adcb93febd546cb5b9fa7cb883d8f'],False,['engine.py']
4601885972be96373066662084ce1bf9c49448b8,"Samyamr/inference hook fix (#851)

* Fix mis-aligned-grad

When a parameter is not divisible by world size, the partitioned gradients are mis-aligned due to incorrect padding handling. This PR should fix for that.

* Formatting fix

* Adding static_scale test back for Z3, and also changing hidden size to be not divisile by world_size

* also removing alignment from flat fp16 buffers

* Testing for hidden dim alignment

* inference hook fix

* Update stage3.py

* formatting

* [bug-fix] move params to gpu if offload params is turned off

Co-authored-by: Samyam Rajbhandari <samyamr@microsoft.com>
Co-authored-by: Shaden Smith <Shaden.Smith@microsoft.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['73d762c8d96525872b8090ffa9b1f5a5e86e4295'],False,"['partition_parameters.py', 'stage3.py', 'test_fp16.py']"
24335d49cec0be439ba4f311c6e7e802e834b84d,"[runner/launch] propagate the error (#854)

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['a75d971bc2f1300c10331ed3b5f6026ecabe1821'],False,['runner.py']
871f3048ad0d05e79f8835849b7a00656a14b3f4,Allow args to be optional in deepspeed.initialize (#825),['547d1c5f8f3f5a3a01bb13b286e7686c5364f9b8'],False,"['__init__.py', 'engine.py', 'test_config.py']"
fa87a73a8a3bead24ad9ea52090646fa620d74e8,"Fix ZeRO3 save_checkpoint (#857)

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['871f3048ad0d05e79f8835849b7a00656a14b3f4'],False,"['stage3.py', 'test_checkpointing.py']"
68c8481bcf63f6fa8481c631eb63fecc5173ee89,"1-bit Adam v2 (#817)

Authors: @awan-10 @conglongli @samyam @jeffra

What's new:

NCCL-based implementation which provides better performance and usability compared to the MPI-based implementation.
Add support to momentum masks for those parameters with constant zero gradients during training.
Bug fixes (e.g., #813).

* NCCL-based 1-bit Adam + Code Refactor for Comm. Backends (#594)

* NCCL based 1-bit Implementation + Refactor to add communication backends (#593)

* add nccl 1-bit optim.

* temporary commit to save stuff.

* Use dist collectives instead of mpi routines.

* remove old code for comm.

* Fix bugs. still does not work.

* modify to test the nccl side code path

* Initial gather impl. Works intra-node.

* Updates to comm. phase 2. nccl comm. passed the tests.

* refactor code to introduce nccl/mpi as backends for onebit adam.

* Refactor updates to test/engine.

* Fix compile/runtime errors.

* simplify support for nccl/mpi backends.

* Add missign file

* Add compression backend in constructor. Revert later.

* modify test with some perf counting.

* Implement a true non-blocking gather for nccl side.

* Revert ""Add compression backend in constructor. Revert later.""

This reverts commit df8c40d3105e9f2542a8aa6619e80d675a09753f.

* improve the 1-bit adam test.

* Refactor comm. and compression backend in 1-bit adam.

* Fix the test.

* Fix runtime errors and typos in nccl backend

* fix mpi backend. modify tests.

* modify nccl perf test.

* fix mpi side errors.

* Add an mpi perf test

* Sync DSE.

* Remove old collectives file.

* Undo a typo.

* Graceful failure for torch versions that don't support nccl pt2pt.

* Revert ""Merge branch 'master' into staging-1bit-nccl-v2""

This reverts commit 78400850703b4b2d84f11b73c109f56919e748ea, reversing
changes made to a6dba72aeafad63661dfe566d3accd03d00be78c.

* Revert ""Revert ""Merge branch 'master' into staging-1bit-nccl-v2""""

This reverts commit 6dbdd9858bafef4d340c089fdc0e3ddde3706f47.

* comm optimization + 1-bit lamb

* Saving/debugging commit.

* finalizing 1-bit lamb

* finalizing 1-bit lamb

* add momentum mask and chkpt handling for 1-bit adam

* Cleanup and modify nccl test to be runnable with deepspeed launcher.

* Fix format.

* fix formatting again.

* make test runnable without mpi4py

* Add dist.alltoall and dist.allgather instead of custom functions.

* remove debug prints.

* formatting and renaming

* renaming

* renaming

* add unit test, fix existing tests

* skip unit test when torch < 1.8

* revert 1-bit lamb

* flatten momentum when dimension is more than 1

* add warning message for 1-bit adam under fp32

* improve version check

* add fp32 test

* 1-bit adam doc

* fix file name

* doc fix

* torch 1.8 is released

* doc fix

* fix tests

* update news

* add doc for momentum mask

* fix checkpoing handling, add unit test

* checkpoint handling doc

* doc final cleanup

* bump dates

* update tests

* url change

* doc fix

* fix test

* doc update

Co-authored-by: Ammar Ahmad Awan <ammar.awan@microsoft.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['12a53b43833b7bea279a205e313f2bd3f0cdfd99'],False,"['README.md', '__init__.py', 'mpi.py', 'nccl.py', '__init__.py', 'cupy.py', 'custom_collectives.py', 'engine.py', '__init__.py', 'adam.py', 'config-json.md', 'onebit-adam.md', 'optimizers.rst', 'index.md', 'test_mpi_backend.py', 'test_mpi_perf.py', 'test_nccl_backend.py', 'test_nccl_perf.py', 'test_server_error.py', 'test_onebit.py']"
9e9f8cbed04ef8708f49316000f1fddc93ee4639,"[doc] launcher (#868)

As discussed in https://github.com/microsoft/DeepSpeed/issues/662 this PR modifies the doc:
* explains what to use instead of CUDA_VISIBLE_DEVICES
* puts the `--hostfile` cl arg in the correct place in the invocation script

Fixes: https://github.com/microsoft/DeepSpeed/issues/662

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['10c0bea6021045c917f8e80eb9f4d8e59ae28dee'],False,['getting-started.md']
7f03282c5170ffca2d1c000776b0334d7fd5c97e,"[debug utils] see_memory_usage fixes (#890)

* see_memory_usage fixes

* didn't expect pt-1.2

* fix the order of things

* fix the order of things",['22d5a1f3e8727d5d444a4d4e4fa6290441851c4a'],False,"['utils.py', 'getting-started.md', 'pipeline.md']"
7fcc8911cbced68a045063a4edc0507688fc78e6,"Fix zero stage2 cpu_offload when some model trainable parameters skipped in training (#861)

* Fix zero stage2 cpu_offload when some model trainable parameters skipped in training, as in https://github.com/microsoft/DeepSpeed/issues/707

As some model trainable parameters skipped in training,
their backward hooks in self.create_reduce_and_remove_grad_hooks() will not run, 
so they have no norm_for_param_grads

* Trim space

* Trim space

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['39013dd2b89d471520be17b405519dfd7f94c1c4'],False,['stage2.py']
8db4fdf8157c688b7f1cca082020684024329e19,"disable pipe test (#915)

This test has been giving us trouble for a bit, seeing nondeterministic failures, skipping for now to not break out CI. Need to revisit soon though.",['5d721e092d60eef027ae4af773c0acab8ae44828'],False,['test_pipe.py']
b5f56b2c6bade2b0bfc80c5fb550d6597f43a17a,"Fix for fragmented linear inputs in ZeRO 3 Linear layers where reshap… (#881)

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['ce14cf1af6b31fea0f8cc4d9cc09aac19a61c6a3'],False,"['linear.py', 'partition_parameters.py']"
c79184ebcc501ad64a8628afe18c0659d1bc4963,"fix cpu_adam memory leak on deepspeed re-use in the same process (#896)

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Reza Yazdani <reyazda@microsoft.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['6d94afb5487be2861e948d1198f42fcbdeab36e4'],False,"['cpu_adam.cpp', 'cpu_adam.py']"
f19cf6714faf867a5d728b7f4e5c0e2f5c25f127,"[zero doc] fix misspelled param (#878)

We really really really need those params to be validated...

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['5ca86ae4eda0d332495721419e92e71ecad32a5a'],False,['zero.md']
e721cb691f6a86d59bf82f1d383fbfebd313c0df,"Supporting different hidden dimensions for transformer kernels-v2 (#934)

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['316992913d3218c0d88364ea4478caa4d53bc7e5'],False,"['ds_transformer_cuda.cpp', 'gelu_kernels.cu', 'test_cuda_backward.py', 'test_cuda_forward.py']"
adac058ac0f6422eed24042f280db93a6841da59,fix double linear override; spelling (#954),['e6999ebd1657d6dc0826d1c51f2986d8ede016a6'],False,['partition_parameters.py']
8b8ed2a7d936c3c95e2dd2e7005b23cc6aaa8f16,"[zero] faster flatten/unflatten (cpp version)  (#910)

* faster flatten/unflatten with apex

* switch to cpp flatten/unflatten

* style

* better comment

* missing import

* switch to build ops at run time

* fixes

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['7003d447208f0c2c4c07bd6df202f8c52bd68fdf'],False,"['stage1.py', 'stage2.py', 'stage3.py']"
2805c3935ae02558f18a71a115185632b087da70,"Fix ZeRO-3 UnboundLocalError (#968)

* Fix UnboundLocalError

* Get full partition size",['c83e49f9ed075175cfd81621fa59efe43b85114b'],False,['partition_parameters.py']
0d4a54a04d658db40a120bc10c6f1f1a4478f6f1,"ZeRO-Infinity (#976)

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Samyam Rajbhandari <samyamr@microsoft.com>
Co-authored-by: Shaden Smith <Shaden.Smith@microsoft.com>",['2805c3935ae02558f18a71a115185632b087da70'],False,"['DeepSpeedExamples', 'README.md', 'deepspeed_aio_common.cpp', 'deepspeed_aio_common.h', 'deepspeed_aio_types.cpp', 'deepspeed_aio_types.h', 'deepspeed_aio_utils.cpp', 'deepspeed_aio_utils.h', 'deepspeed_aio_thread.cpp', 'deepspeed_aio_thread.h', 'deepspeed_py_aio.cpp', 'deepspeed_py_aio.h', 'deepspeed_py_aio_handle.cpp', 'deepspeed_py_aio_handle.h', 'deepspeed_py_copy.cpp', 'deepspeed_py_copy.h', 'py_ds_aio.cpp', 'ds_aio_basic.py', 'ds_aio_handle.py', 'parse_aio_stats.py', 'run_read_sweep.sh', 'run_write_sweep.sh', 'test_ds_aio.py', 'test_ds_aio_utils.py', 'cpu_adam.h', 'runner.py', '__init__.py', 'config.py', 'checkpointing.py', 'config.py', 'config_utils.py', 'engine.py', '__init__.py', 'aio_config.py', 'async_swapper.py', 'constants.py', 'optimizer_utils.py', 'partitioned_optimizer_swapper.py', 'partitioned_param_swapper.py', 'pipelined_optimizer_swapper.py', 'utils.py', '__init__.py', 'config.py', 'constants.py', 'linear.py', 'offload_config.py', 'offload_constants.py', 'partition_parameters.py', 'stage3.py', 'tiling.py', 'navigation.yml', 'config-json.md', 'pipeline.md', 'zero.md', 'optimizers.rst', 'zero3.rst', 'index.md', '__init__.py', 'async_io.py', 'modelingpreln.py', 'test_pipe_module.py', 'test_zero_context.py', 'test_zero_tiled.py']"
fbece50b21ffeb4fe05e498f930aded71de12aed,assert no Z2/Z3 with pipeline and fix some docs links (#980),['2c2a7f31bcc20ae12ce8d2b8af14448939ebdf12'],False,"['engine.py', 'module.py', 'pipeline.md', 'optimizers.rst']"
67a48aaa8906878b2ce244319e219155c85de46c,"1-bit LAMB optimizer (#970)

1-bit LAMB: Communication Efficient Large-Scale Large-Batch Training with LAMB's Convergence Speed.
Author: @conglongli, @awan-10, @samyam, Hanlin Tang, Yuxiong He
Paper: https://arxiv.org/abs/2104.06069

Co-authored-by: sdtblck <46172032+sdtblck@users.noreply.github.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['eecef309cb12528cfa78d932a6f073afb43847e5'],False,"['README.md', 'nccl.py', 'config.py', 'engine.py', 'fused_optimizer.py', 'adam.py', 'lamb.py', 'unfused_optimizer.py', 'engine.py', 'utils.py', '_config.yml', 'navigation.yml', 'config-json.md', 'features.md', 'onebit-adam.md', 'onebit-lamb.md', 'optimizers.rst', 'index.md', 'test_onebit.py']"
894f21daaab90a083d6f193426e551333d20a6bf,"Use odd shape tensor to represent parameter data in partitioned state (#981)

* use wierd shaped tensor to avoid silent failures when not registering externel params

* fix typo

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['67a48aaa8906878b2ce244319e219155c85de46c'],False,"['partition_parameters.py', 'test_zero_context.py']"
0b80ad06b694c82ad4558fd3a23812ebbd17247d,"Make reduce scatter optional for ZeRO-1 as workaround (#971)

* Make reduce scatter optional for ZeRO-1 as workaround

* Make allreduce default for ZeRO 1

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['894f21daaab90a083d6f193426e551333d20a6bf'],False,"['engine.py', 'constants.py']"
669028f0fd5067c9247120cb21fd6e9bea4820a9,Fix all Pipeline Module Parameters being sent to cuda:0 (#687),['0b80ad06b694c82ad4558fd3a23812ebbd17247d'],False,['module.py']
cf5ea8912afed2e0734d90a75b6b384a6ec9a2cf,Add nvme unit/perf tests (#993),['669028f0fd5067c9247120cb21fd6e9bea4820a9'],False,"['ds_aio_basic.py', 'ds_aio_handle.py', 'test_ds_aio.py', 'test_aio.py']"
a711878996483699c10b411ca290899a6b57d1aa,"Fix issue where gradient_predivide_factor was called as a func. (#996)

* Fix issue where gradient_predivide_factor was called as a func.

`gradient_predivide_factor` is a `float`, hence shouldn't be called as func.
This crashes when `reduce_scatter` flag is set to `False`.",['cf5ea8912afed2e0734d90a75b6b384a6ec9a2cf'],False,['stage3.py']
bff4bc72392426ec48325a1e9a443b83a7c6f0a8,"Asynchronous I/O docs (#1000)

* Fix docstring

* Make screenshots clickable for easier viewing

* Navigation menu in alphabetical order; More clicable screenshots

* Rename 1Cycle doc

* Tweak naming

* Remove no longer used flag

* ZeRO3 Offload release

* Single GPU results

* Rearrange figures

* Single GPU text

* tweak intro

* zero3-offload section

* Add asynchronous i/o docs",['ecf2e1bc7d9302ebf7542fc314af2a70b5d7e802'],False,"['navigation.yml', 'config-json.md']"
e88ebbcfc90b3d3619187302f1695f82ac987876,"Use amp autocast in ZeRO3 linear (#990)

* Use amp autocast in ZeRO3 linear

* Fix typo

* Handle specific exceptions

* CI breaks on torch.distributed

* Add autocast unit test

* Format fixes

* Fix skip logic

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['bff4bc72392426ec48325a1e9a443b83a7c6f0a8'],False,"['utils.py', 'linear.py', 'test_autocast.py']"
03d24fe0be9586ce91483ffd6f5092b917ce92d3,"Minor usability tweaks/fixes (#1001)

* Add nvme unit/perf tests

* Minor tweaks/fixes

* Format fixes

* Address PR feedback",['e88ebbcfc90b3d3619187302f1695f82ac987876'],False,"['parse_aio_stats.py', 'run_read_sweep.sh', 'run_write_sweep.sh', 'test_ds_aio_utils.py']"
d0b61f1810c5f7bcf41f3836c373da769ecdde79,"Add find_unused_parameters option to DeepSpeedEngine (#945)

* Add find_unused_parameters option

As unused parameters in modules may not be expected sometimes, 
add an explicit error msg when it occurred and an option to avoid the error: https://github.com/microsoft/DeepSpeed/issues/707

* Add find_unused_parameters option

As unused parameters in modules may not be expected sometimes, 
add an explicit error msg when it occurred and an option to avoid the error: https://github.com/microsoft/DeepSpeed/issues/707

* Fix syntax error

* Fix yapf error

* Fix yapf error

* Fix yapf error

* Fix yapf error

* Move stage2 find_unused_parameters to config file

* Add stage2 find_unused_parameters

* Add stage2 find_unused_parameters

* Add stage2_find_unused_parameters option

* Change error msg to reflect zero_optimization config change

* Fix yapf error

* Fix yapf errors

* Change find_unused_parameters option name

* Change find_unused_parameters option name

* Change find_unused_parameters option name

* Change find_unused_parameters option name

* Change find_unused_parameters option name

* Add UnusedParametersModel for test option find_unused_parameters

* Add unit test for stage2 find_unused_parameters

* Add cpu-adam compatible check

* Remove dups import

* Trim spaces

* Fix yapf errors

* Trim spaces

* Add False Positive test check

* Fix find_unused_parameters test

* Trim spaces

* Fix yapf error",['03d24fe0be9586ce91483ffd6f5092b917ce92d3'],False,"['engine.py', 'config.py', 'constants.py', 'stage2.py', 'config-json.md', 'simple_model.py', 'test_find_unused_parameters.py']"
b7f970617ddca2fc721b0dd46a76bcabfc90871d,"fix gradient_clipping default (#656)

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['d0b61f1810c5f7bcf41f3836c373da769ecdde79'],False,['config-json.md']
41ab660b5df3567966935fe8ac3672497bc8690a,"Refactor param_dict to config (#1008)

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['14a50c68c33968f9cd7fefd29e8fe72d4be200e8'],False,"['__init__.py', 'checkpointing.py', 'config.py', 'engine.py', 'partition_parameters.py', 'test_config.py', 'test_fp16.py', 'test_zero_context.py']"
cab30aa61e3a6802cbea9d85d2c691b035f8e66b,"Transformer - fix unit test (#964)

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['a8cf887d6506a9ded941dd785cd9c7e4c8ac4081'],False,['transformer.py']
18a26e8604c4cb8562ed8d57241ca64dbeb4318a,[fp32] fix default dtype (#1023),['429dfa6c3dc5763008a04fd1c94d7b72817dd93e'],False,['partition_parameters.py']
de694b917f298bf6a3acc3b36c551be5e4486a0a,"[tests] make it easier to run tests (#923)

* make it easier to run tests

* cleanup

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['c5700bc09f6ca9ed0a59009d34af7c0d4b9f16b7'],False,['conftest.py']
b3870363e026717752c119b21870e2886b9ad92d,"[Stage][Fix] Add additional conditions when checking types of output from the model (#1026)

* Add additional conditions when checking types of output from the model

* Add test

* Modify test to use torch.tensor as well

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['c4f4ef5d44f556315d68ee010e9308b22683c7aa'],False,"['stage3.py', 'test_zero_context.py']"
962dbc633a91a3103aae106cd93cc1a9d3146b77,fix format (#1032),['b3870363e026717752c119b21870e2886b9ad92d'],False,['engine.py']
210470724c32130d73b3c856141f19f812c86489,"Change methods to be static (#1038)

Fix #1032",['962dbc633a91a3103aae106cd93cc1a9d3146b77'],False,['engine.py']
47ec97ebd975efc396ff7957712511acdf5b2058,"fix assert message (#1040)

* fix assert

The current assert ""Model must initialized in fp16 mode for ZeRO Stage 3."" needs TLC - I rewrote it completely to match its cousen assert, so now we have 2 consistent matching asserts:

- f""fp16 is enabled but one or several model parameters have dtype that is not fp16""
- f""fp16 is not enabled but one or several model parameters have dtype of fp16""

* remove f",['210470724c32130d73b3c856141f19f812c86489'],False,['engine.py']
be5cd76a2871b1c10bdebf8d54a05875cc7a27b4,"Align optimizer swap buffer sizes (#1036)

* NVMe intra-request validation should be on entire file
Optimizer swap buffer sizes should be aligned

* Add fix message for missing aio lib error.",['e4139a7dc14ba0a1d9263e7f11a16b168a180fe8'],False,"['deepspeed_py_aio_handle.cpp', 'optimizer_utils.py', 'utils.py', 'async_io.py']"
5b393f1555143968fbac78ecbf3c00f65baf7d78,"Avoid unused parameters assert by default (#1039)

* Unused parameters assert should be disabled by default

* Fix message

* Invert assert logic in unit test

* Change option for ignoring unused parameters

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['fb1213fc2a6564d984c3e0e3dd119ec25deb6ca3'],False,"['engine.py', 'config.py', 'constants.py', 'stage2.py', 'test_ignore_unused_parameters.py']"
6124eb37c07dae57f6da01eb07f708fd6e8d4d25,"Auto-tuning aio parameters (#1059)

* Validate env; remove write size from logs

* Performance scripts for auto-tunining/auto-generating aio params of
deepspeed config.

* Formatting fixes

* Address feedback",['5b393f1555143968fbac78ecbf3c00f65baf7d78'],False,"['aio_bench_generate_param.py', 'aio_bench_perf_sweep.py', 'parse_aio_stats.py', 'perf_sweep_utils.py', 'single_process_config.json', 'test_ds_aio.py', 'test_ds_aio_utils.py', 'validate_async_io.py']"
4544b7d2f1c651cb9e06c2f94b07d1470c6b86ae,"Improve flops profiler functionality (#1065)

* use the original function's name as the key to old_functions dict

* update profile output format

* print at global rank 0

* add flops calculation in bwd pass using time from ds timers

* improve aggregated profiling out to show all depth

* print samples/second

* update readme and examples

* update docs

* fix typo and reorder printing

* fix format",['1f82ab78c2206b97f2d27041da8b7b127180e8e0'],False,"['DeepSpeedExamples', 'config.py', 'constants.py', 'README.md', 'profiler.py', 'checkpointing.py', 'config.py', 'engine.py', 'timer.py', 'config-json.md', 'flops-profiler.md']"
63c5070eeecf0492d9aa2d0c312ef362a972b172,"[docs] unused parameter handling (#1060)

* Fix docstring

* Make screenshots clickable for easier viewing

* Navigation menu in alphabetical order; More clicable screenshots

* Rename 1Cycle doc

* Tweak naming

* Remove no longer used flag

* ZeRO3 Offload release

* Single GPU results

* Rearrange figures

* Single GPU text

* tweak intro

* zero3-offload section

* Add asynchronous i/o docs",['4544b7d2f1c651cb9e06c2f94b07d1470c6b86ae'],False,['config-json.md']
ee4deabd75331e9c1bb1383b2068fdebcb33ed08,"ZeRO2-Offload: Load balance gradient copying to CPU (#1067)

* Round robin partitioning to improve ZeRO-2 Offload CPU copy

* Formatting fixes

* Fix index issues in debug dumps

* Remove debug prints

* Code cleanup

* Remove unintended stage3.py changes

* Add TODO",['46f4573b1a8a9cda2b45f4de4e90b631cee3f80b'],False,['stage2.py']
c1550b89ba796d89a10f0445d896896d9206bf26,"fixed two broken links in README.md (#1078)

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['67315ed3f41c8b0bb1819299157cc6d7a12ac6f4'],False,['README.md']
093e59ec436a2641ebab786e38a253eff9d20e37,"fix ZERO_OPTIMIZATION_REDUCE_SCATTER_DEFAULT default value  (#1058)

* fix Reduce Scatter default value

* Update constants.py

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['e9e9d5b82544f61f9a934c95517e2b54a4dc6150'],False,['constants.py']
ed3de0c21b1fea330de9c1a78a23ca33f340ef20,"Quantization + inference release (#1091)


Co-authored-by: Jeff Rasley <jerasley@microsoft.com>
Co-authored-by: eltonzheng <eltonz@microsoft.com>
Co-authored-by: Reza Yazdani <44502768+RezaYazdaniAminabadi@users.noreply.github.com>
Co-authored-by: Shaden Smith <Shaden.Smith@microsoft.com>
Co-authored-by: Reza Yazdani <reyazda@microsoft.com>
Co-authored-by: Shaden Smith <Shaden.Smith@microsoft.com>
Co-authored-by: Elton Zheng <eltonz@microsoft.com>
Co-authored-by: Reza Yazdani <44502768+RezaYazdaniAminabadi@users.noreply.github.com>
Co-authored-by: eltonzheng <eltonz@microsoft.com>
Co-authored-by: Arash Ashari <arashari@microsoft.com>
Co-authored-by: Reza Yazdani <44502768+RezaYazdaniAminabadi@users.noreply.github.com>
Co-authored-by: Shaden Smith <Shaden.Smith@microsoft.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Reza Yazdani <reyazda@microsoft.com>
Co-authored-by: niumanar <60243342+niumanar@users.noreply.github.com>
Co-authored-by: eltonzheng <eltonz@microsoft.com>
Co-authored-by: Reza Yazdani <44502768+RezaYazdaniAminabadi@users.noreply.github.com>
Co-authored-by: Shaden Smith <Shaden.Smith@microsoft.com>
Co-authored-by: Reza Yazdani <reyazda@microsoft.com>
Co-authored-by: Arash Ashari <arashari@microsoft.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: niumanar <60243342+niumanar@users.noreply.github.com>

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>
Co-authored-by: eltonzheng <eltonz@microsoft.com>
Co-authored-by: Shaden Smith <Shaden.Smith@microsoft.com>
Co-authored-by: Arash Ashari <arashari@microsoft.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: niumanar <60243342+niumanar@users.noreply.github.com>",['093e59ec436a2641ebab786e38a253eff9d20e37'],False,"['torch16.yml', 'DeepSpeedExamples', 'custom_cuda_layers.h', 'quantizer.h', 'fused_lamb_cuda_kernel.cu', 'pt_binding.cpp', 'quantizer.cu', 'ds_transformer_cuda.cpp', 'dequantize.cu', 'gelu.cu', 'normalize.cu', 'pt_binding.cpp', 'softmax.cu', 'context.h', 'cublas_wrappers.h', 'custom_cuda_layers.h', '__init__.py', 'constants.py', 'elasticity.py', '__init__.py', 'engine.py', '__init__.py', 'module_quantize.py', 'replace_module.py', 'replace_policy.py', '__init__.py', 'module_inject.py', '__init__.py', 'quantizer.py', '__init__.py', '__init__.py', 'transformer_inference.py', 'transformer.py', 'config.py', 'constants.py', 'eigenvalue.py', 'engine.py', 'fused_optimizer.py', 'unfused_optimizer.py', 'engine.py', 'module.py', 'topology.py', 'quantize.py', 'state_dict_factory.py', 'weight_quantizer.py', '2021-05-05-MoQ.md', '2021-05-05-inference-kernel-optimization.md', 'MoQ-tutorial.md', 'advanced-install.md', 'inference-tutorial.md', 'gpu-numbers.png', 'inference-gemm-scheduling.png', 'inference-kernel-fusion.png', 'inference-latency.png', 'inference-throughput.png', '__init__.py', 'builder.py', 'cpu_adam.py', 'fused_adam.py', 'fused_lamb.py', 'quantizer.py', 'transformer.py', 'transformer_inference.py', 'setup.py', 'deepspeed_bsz24_fp16_eigenvalue_quantize_config.json', 'run_BingBertSquad.sh', 'gpt2-merges.txt', 'gpt2-vocab.json', 'megatron_model.py', 'modeling.py', 'modelingpreln.py', 'test_checkpointing.py', 'test_configurable_parallel.py', 'test_cuda_backward.py', 'test_cuda_forward.py', 'test_find_unused_parameters.py', 'test_fp16.py', 'version.txt']"
95fe2c42e05892bdfa81113183ea0a089cccdb2d,fix inference titles and add MoQ pictures (#1092),['ed3de0c21b1fea330de9c1a78a23ca33f340ef20'],False,"['MoQ-tutorial.md', 'inference-tutorial.md', 'bingbert-mixedbit.png', 'quantization-8bit.png', 'quantization-mixedbit.png']"
7388e29ce3b522d0501190e6b1a99eb1ff52ba7c,Fix Inference and Quantization tutorial links (#1093),['95fe2c42e05892bdfa81113183ea0a089cccdb2d'],False,"['navigation.yml', 'MoQ-tutorial.md']"
f8a65cb5f185afa59f5f9a0948c32eaa86b027f8,"Fix inference Api (#1095)

* Fix Inference and Quantization tutorial links

* fix inference api

* use correct attention scaling

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['b49c99b0c9d48977c2dfaa90e65a873a70b1a20c'],False,['transformer_inference.py']
0449cbd36d7bb79a6a766805e63330adc522b541,formatting fix,['0d701ec4ab4e0edd43a5f9ec0bdc0a763a65e73a'],False,"['inference-engine.rst', 'inference-init.rst']"
96eb5b12e328f9398d57fb577ee088cfcfb5444f,"delay imports for replace policies and fix missing req (#1100)

* delay imports for replace policies and fix missing req

* fix issue with _orig_layer_class always being None",['0449cbd36d7bb79a6a766805e63330adc522b541'],False,"['replace_module.py', 'replace_policy.py', 'requirements-dev.txt']"
11e94e6c72dc9e80372143bc555fbd47fd05c626,fix links for inference tutorial (#1113),['d2cf66a66847aa7f8d25da5708b7016e54f29e0a'],False,['inference-tutorial.md']
26e3841cd4421c8d335dc071a255049123676414,"Change the sparse attention API to be compatible with latest changes of triton (#902)

* Change the sparse attention API to be compatible with latest changes on the triton side

* remove compatibility checks for CUDA 11

* Update requirements-sparse_attn.txt

Co-authored-by: Arash Ashari <arashari@microsoft.com>",['11e94e6c72dc9e80372143bc555fbd47fd05c626'],False,"['matmul.py', 'softmax.py', 'sparse_attn.py', 'requirements-sparse_attn.txt', 'test_sparse_attention.py']"
ccc522c2f87456aaf5f6986682349b262dc60469,fix empty entry in gcc cmd (#1129),['cf82168c4ec02870f2d37f80cd76ddcd3b630c4d'],False,['async_io.py']
c697d7ae1cf5a479a8a85afa3bf9443e7d54ac2b,"fix config name (#1103)

fixes: s/micro_batch_per_gpu/train_micro_batch_size_per_gpu/

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['ccc522c2f87456aaf5f6986682349b262dc60469'],False,['config.py']
5ca81678f2f9b42287619bd803f44ffb6031195a,"[zero] fix missed subclasses partitioning bug (#1135)

* fix missed subclassed partitioning bug

* fix on exit

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['c5d5ce681c40f88ad0b78951c1d28a645518ce80'],False,['partition_parameters.py']
8def3cb3a2219ab42f35d1a6de13cbb97b2954be,"remove old unit test, should have been removed in rebase (#1097)

* remove old unit tests, should have been removed in rebase

* formatting",['6924d840ac0012dec4f0dc6515efc36c376231e0'],False,['test_find_unused_parameters.py']
aca7fc549a415e3e8e0f69ea8ce58735b178b4d0,"Add local attention for GPT-Neo model architecture (#1114)

* fix links for inference tutorial

* Fix automatic injection. Add the local-attention for GPT-Neo

* fix the inference for generation of large sequences (>1K & <32K)

* fix format

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['8def3cb3a2219ab42f35d1a6de13cbb97b2954be'],False,"['pt_binding.cpp', 'softmax.cu', 'custom_cuda_layers.h', 'engine.py', 'replace_module.py', 'transformer_inference.py']"
8e48756c123b037db9c273788f1339d7b2572045,"unit test for bugfix #1135 (#1148)

* unit test for bugfix #1135

* formatter

* fix test in presence of mpi4py

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['5f16b17297525b321208b493e89c665e68a04163'],False,['test_zero_context.py']
aa16828c267dc7a790a5cc2f43f93a67617a2364,"Fix bugs in the tutorial documentation (#1157)

* Add `import os` to inference tutorials

* assign deepspeed-initialized model to hf model",['71ecf7e62583cf8cd3661d30d5f82851df3bb82c'],False,['inference-tutorial.md']
4eaf910616bda4be87dc7777b7f3adff792adc49,"Samyamr/largest partitioned params calculation fix (#1150)

* largest_partitioned_params calculation fix

largest partitioned params was getting calculated incorrectly

* Update stage3.py

* Update stage3.py

* formatting fix

* changing sub-group size default to 1e9

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['aa16828c267dc7a790a5cc2f43f93a67617a2364'],False,"['constants.py', 'stage3.py']"
fa7921e23f15844373dc3225a27a1b8f40084a2a,"[Doc] Fix steps_per_print description (#1163)

* Fix docstring

* Make screenshots clickable for easier viewing

* Navigation menu in alphabetical order; More clicable screenshots

* Rename 1Cycle doc

* Tweak naming

* Remove no longer used flag

* ZeRO3 Offload release

* Single GPU results

* Rearrange figures

* Single GPU text

* tweak intro

* zero3-offload section

* Add asynchronous i/o docs

* Fix print_per_steps doc",['4eaf910616bda4be87dc7777b7f3adff792adc49'],False,['config-json.md']
b1669c0d8f6dbe65162b534f2c6967f64d143dc3,Avoid partitioning small activations (#1154),['1ba3e8e3364049ecfc647aae4715ec9441676c51'],False,['checkpointing.py']
429cbc89afdb1a801dc56af25304dd2e34af3632,"Fix bugs about non-contiguous tensor broadcasting (#1168)

* Fix bugs about non-contiguous tensor broadcasting

* Fix typo

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>
Co-authored-by: Reza Yazdani <44502768+RezaYazdaniAminabadi@users.noreply.github.com>",['c0c4ebf143b60bae28c32beaed0a9f526f25264d'],False,"['engine.py', 'replace_policy.py']"
91f58c068c2bb36a8327532463ff34adf6a49dd6,"[zero3] params_to_reduce isn't always there (#1214)

* [zero3] params_to_reduce isn't always there

Trying to port HF's Electra model's to Deepspeed I'm getting this on the very first backward step (with some extra debug):

```
Incrementing with parameter id 42
------ Before allocating allgather param name=generator_lm_head.weight id=41 shape=torch.Size([1]) status=ZeroParamStatus.NOT_AVAILABLE partition size=327680
------allgather param with name=generator_lm_head.weight id=41 shape=torch.Size([1]) status=ZeroParamStatus.NOT_AVAILABLE partition size=327680
------ Before allocating allgather param name=generator_lm_head.bias id=42 shape=torch.Size([1]) status=ZeroParamStatus.NOT_AVAILABLE partition size=5120
------allgather param with name=generator_lm_head.bias id=42 shape=torch.Size([1]) status=ZeroParamStatus.NOT_AVAILABLE partition size=5120
Backward name=generator_lm_head.weight id=41 shape=torch.Size([5120, 64])
Inside reduce ipg buckets. name=generator_lm_head.weight id=41 shape=torch.Size([5120, 64]), ipg elements 0, reduce bucket size 4096
Params in ipg bucket []
Reducing []
GOT 1
torch.Size([4096])
Traceback (most recent call last):
  File ""examples/pytorch/language-modeling/run_mlm.py"", line 533, in <module>
    main()
  File ""examples/pytorch/language-modeling/run_mlm.py"", line 484, in main
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
  File ""/mnt/nvme1/code/huggingface/transformers-ds-zero_to_fp32-tests/src/transformers/trainer.py"", line 1269, in train
    tr_loss += self.training_step(model, inputs)
  File ""/mnt/nvme1/code/huggingface/transformers-ds-zero_to_fp32-tests/src/transformers/trainer.py"", line 1778, in training_step
    loss = self.deepspeed.backward(loss)
  File ""/mnt/nvme1/code/github/00optimize/DeepSpeed-zero-init-child-only-post_init/deepspeed/runtime/engine.py"", line 1188, in backward
    self.optimizer.backward(loss)
  File ""/mnt/nvme1/code/github/00optimize/DeepSpeed-zero-init-child-only-post_init/deepspeed/runtime/zero/stage3.py"", line 2964, in backward
    self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)
  File ""/mnt/nvme1/code/github/00optimize/DeepSpeed-zero-init-child-only-post_init/deepspeed/runtime/fp16/loss_scaler.py"", line 53, in backward
    scaled_loss.backward(retain_graph=retain_graph)
  File ""/home/stas/anaconda3/envs/py38-pt19/lib/python3.8/site-packages/torch/_tensor.py"", line 255, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File ""/home/stas/anaconda3/envs/py38-pt19/lib/python3.8/site-packages/torch/autograd/__init__.py"", line 147, in backward
    Variable._execution_engine.run_backward(
  File ""/mnt/nvme1/code/github/00optimize/DeepSpeed-zero-init-child-only-post_init/deepspeed/runtime/zero/stage3.py"", line 1867, in reduce_partition_and_remove_grads
    self.reduce_ready_partitions_and_remove_grads(param, i)
  File ""/mnt/nvme1/code/github/00optimize/DeepSpeed-zero-init-child-only-post_init/deepspeed/runtime/zero/stage3.py"", line 2212, in reduce_ready_partitions_and_remove_grads
    self.reduce_independent_p_g_buckets_and_remove_grads(param, i)
  File ""/mnt/nvme1/code/github/00optimize/DeepSpeed-zero-init-child-only-post_init/deepspeed/runtime/zero/stage3.py"", line 1897, in reduce_independent_p_g_buckets_and_remove_grads
    self.reduce_ipg_grads()
  File ""/mnt/nvme1/code/github/00optimize/DeepSpeed-zero-init-child-only-post_init/deepspeed/runtime/zero/stage3.py"", line 2193, in reduce_ipg_grads
    self.average_tensor(reduction_list, params_to_reduce)
  File ""/mnt/nvme1/code/github/00optimize/DeepSpeed-zero-init-child-only-post_init/deepspeed/runtime/zero/stage3.py"", line 1972, in average_tensor
    params_to_reduce[0].reduce_gradients_at_owner(
```

Is it always that `params_to_reduce` is populated?

If I add this check the problem goes away it seems.

* real fix",['aeea85b45071a44fb1d2f9d9a60cdf28bcf05849'],False,['stage3.py']
497b741f1616977b8db670555bd96f88ef926406,"[zero.Init] post_init partitining is to be run only by a child module (#1202)

* post_init to be run only by a child module

* better solution

* add test

* safer attr name

* wants half()

* improve doc

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['91f58c068c2bb36a8327532463ff34adf6a49dd6'],False,"['partition_parameters.py', 'test_zero_context.py']"
f65ff908ab0a5c7c938c680fe7531596a19d4629,"enable cpu adam op on powerpc architectures (#1213)

* enable cpu adam operation on powerpc

* fix formatting

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['5652072e5451077da4179e5398b1c0c71c752c34'],False,"['cpu_adam.h', 'cpu_adam.py']"
2a921069d76e707c52a4a23e5ab1215651902910,"[model weights] zero_to_fp32 multiple improvements (#1181)

* add live zero checkpoint to fp32 consolidation version

* some more docs

* zero2 model states uses a different filename

* fix

* make debug mode cli configurable

* copy the script only on node 0 process 0

* validate that we have the right number of files

* revamp _get_zero_param_shapes, instrument with easier debug

* correct assertion

* rename API; add even simpler API

* style

* docs improve

* update the docs

* revert the unpartitioned_params detection and report as it's most likely persistent params

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['f65ff908ab0a5c7c938c680fe7531596a19d4629'],False,"['engine.py', 'stage3.py', 'zero_to_fp32.py', 'zero.md', 'model-checkpointing.rst']"
bc451c0913d38041eb6be13221c2ce4308141679,"revert part of #1220 (#1221)

https://github.com/microsoft/DeepSpeed/pull/1220 fixed the leak, but lead to another problem. reverting that part so that we could do release and will work on it after the release.

@jeffra",['2660cc4dd43348306c58913775ceb4878379abe5'],False,['engine.py']
d1a7a55ea17ad7752143c4138ff4870efd2a100b,formatting fix for release script,['dba0f431cda2f7cefd535769e576fe8f35b587f8'],False,['bump_patch_version.py']
97207c8c3767ce64aa19a673866c478e3149aa42,"ZeRO2-Offload: Disable copy overlapping (#1219)

* Disable copy stream

* Format fixes

* Remove debug codes

* Remove debug codes

* Fix indent",['d1a7a55ea17ad7752143c4138ff4870efd2a100b'],False,['stage2.py']
3fa24208c4cdd0b939a2d96650b0a509d0b1356c,"[zero3] fix reference counting in backward over multiple forwards (#1227)

* fix reference counting in backward over multiple forwards

* test + cleanup",['810394636aeee8302c87fa1a5c213d3fb87132dc'],False,"['stage3.py', 'test_model.py', 'test_zero.py']"
32e85eda58c560e5d5a596f71fce3682ac2ef80a,"[see_memory_usage] fix deprecation (#1234)

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['89b0fb426dcb43858b6523c69c3d7a7b95ccc3bc'],False,['utils.py']
7392e4590e86704fa889dd9282c717216fe83984,"[zero2] zero_param_shapes: switch to round_robin_fp16_groups (#1240)

* zero_param_shapes: switch to round_robin_fp16_groups

* add test

* old torch workaround",['32e85eda58c560e5d5a596f71fce3682ac2ef80a'],False,"['engine.py', 'stage2.py', 'test_zero.py']"
6ba96289702afc56948aa9f6cff55d7df678ccc0,"Fixing inference api for FP32 and non-masking GPT-based models (#1204)

* fixing inference api for FP32 and non-masking GPT-based models

* use a dummy tensor if input_mask is none

* fix input_mask

* minor fix

* send input_mask to compute_attn func for checking",['7392e4590e86704fa889dd9282c717216fe83984'],False,"['pt_binding.cpp', 'replace_module.py', 'transformer_inference.py']"
bfe7f0db2a02d613d5820c5d714d8de106ab9b5a,"Fix cudaErrorInvalidConfiguration in attn_softmax() for large seq_length*heads values (#1239)

Co-authored-by: Ivan Komarov <dfyz@yandex-team.ru>
Co-authored-by: Reza Yazdani <44502768+RezaYazdaniAminabadi@users.noreply.github.com>",['4d420df56506064a9e385f93fb9f33ace5359130'],False,"['softmax_kernels.cu', 'test_cuda_forward.py']"
e82060d09014868a0d5e6a979ffb7579da78d3b9,"query for libaio package using known package managers (#1250)

* aio: test for libaio with various package managers

* aio: note typical tool used to install libaio package

* setup: abort with error if cannot build requested op

* setup: define op_envvar to return op build environment variable

* setup: call is_compatible once for each op

* setup: only print suggestion to disable op when its envvar not set

* setup: add method to abort from fatal error

* Revert ""setup: add method to abort from fatal error""

This reverts commit 0e4cde6b0a650591c3fafface7e27b4efd9aad4f.

* setup: add method to abort from fatal error

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['97f7ed9e9836f2894387c3a7e38e7db2a8e5dcfc'],False,"['async_io.py', 'builder.py', 'setup.py']"
40c381df0cf88435e80cb42bb6aa283a74124fc0,"[Doc] round_robin_gradients (#1261)

* Fix docstring

* Make screenshots clickable for easier viewing

* Navigation menu in alphabetical order; More clicable screenshots

* Rename 1Cycle doc

* Tweak naming

* Remove no longer used flag

* ZeRO3 Offload release

* Single GPU results

* Rearrange figures

* Single GPU text

* tweak intro

* zero3-offload section

* Add asynchronous i/o docs

* Fix print_per_steps doc

* Document round_robin_gradients

* Tweak description

* Trigger CI",['e82060d09014868a0d5e6a979ffb7579da78d3b9'],False,['config-json.md']
adc21a4dfdca0c540b31068e21610752689ac349,"ZeRO-1 empty grads fix + tests (#1273)

* fix empty grad zero tests

* dont clear grads in stage 1 code path

* prevent none grads from being reduced",['1ff25748b256f675dd26b626e896529e452f99df'],False,"['stage2.py', 'simple_model.py', 'test_fp16.py']"
b2b34ae342d6f851226e995f2e1021d12e761093,"Curriculum learning (#1307)

Co-authored-by: Conglong Li <conglong.li@gmail.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['504893aea40004cf9916ddc3ca0ddbfd0e784c8d'],False,"['README.md', 'config.py', 'constants.py', '__init__.py', 'curriculum_scheduler.py', 'engine.py', '_config.yml', 'navigation.yml', 'config-json.md', 'features.md', 'curriculum-learning.md', 'index.md', 'simple_model.py', 'test_curriculum_learning.py']"
f28432441bae1020157407a4eca1661e9fe35164,"DeepSpeed MoE  (#1310)

Co-authored-by: Alex Muzio <Alex.Muzio@microsoft.com>
Co-authored-by: Ammar Ahmad Awan <ammar.awan@microsoft.com>
Co-authored-by: Conglong Li <conglong.li@gmail.com>
Co-authored-by: Felipe Cruz Salinas <Andres.Cruz@microsoft.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>
Co-authored-by: Reza Yazdani <reyazda@microsoft.com>
Co-authored-by: Samyam Rajbhandari <samyamr@microsoft.com>
Co-authored-by: Shaden Smith <shaden.smith@microsoft.com>
Co-authored-by: Young Jin Kim <youki@microsoft.com>
Co-authored-by: bapatra <bapatra@microsoft.com>
Co-authored-by: Samyam Rajbhandari <samyamr@microsoft.com>
Co-authored-by: Shaden Smith <shaden.smith@microsoft.com>
Co-authored-by: Young Jin Kim <youki@microsoft.com>",['b2b34ae342d6f851226e995f2e1021d12e761093'],False,"['cpu_adam.cpp', 'custom_cuda_kernel.cu', 'cpu_adam.h', 'custom_cuda_layers.h', '__init__.py', 'experts.py', 'layer.py', 'sharded_moe.py', 'utils.py', 'cpu_adam.py', 'config.py', 'constants.py', 'engine.py', 'fused_optimizer.py', 'unfused_optimizer.py', 'utils.py', 'stage2.py', '__init__.py', 'groups.py', 'zero_to_fp32.py', 'mixture-of-experts.md', 'simple_model.py', 'test_fp16.py', 'test_moe.py', 'test_runtime_utils.py', 'util.py', 'version.txt']"
e804f15040d4e02969866f3a4b8932881c28c7f7,Updating the torch version check to numeric (#1314),['e070a09aa5ca66ef03de847db475aa74a5e46ca0'],False,['test_configurable_parallel.py']
10b48405ab0e69a1caf455f445526c4b39b0dbb8,Add issue templates,['6cd5f87b96c361a1ff5ef2d092abc77f041e3d4e'],False,"['bug_report.md', 'feature_request.md']"
274c375c87f35d1c7509dfc6a14f8a84d5efab84,"Support Callable type for client optimizer and lr_scheduler (#1316)

* Callable option for optimizer and scheduler

* Add unit test

* Formatting

* Disable debug prints

* Use base optimizer to construct lr scheduler

* Formatting

* Remove dead import",['aa1212910c8a067e4014e157627fa80b696e824b'],False,"['__init__.py', 'engine.py', 'stage2.py', 'test_ds_initialize.py']"
0ec11daa026138fdbdb7245f2f5053bab878494b,"Add more synchronizations and barriers for the multi-gpu inference case (#1309)

* add more synchronizations and barriers for resolving gpu-halt issue

* removing unuseful broadcasts",['c1b0a4e1ea740fbf189389c2f5aab46611ce06e2'],False,"['engine.py', 'transformer_inference.py']"
336dd089e5e11d46daf3abfad048d71b72912504,Use clone to avoid checkpoint bloat (#1326),['85acf14c58658964a796c5c901b58123f99fb1df'],False,['module.py']
ddffbae021fef24bf64945d45ace9bac7e08508d,"Remove duplicate clip grad function in deepspeed (#1333)

* Remove the wrong function with duplicate name

* fix format.

* add mpu check. fix tests.",['b9ece257c4c6f9970c5b415b50b8875c918239f4'],False,"['utils.py', 'test_runtime_utils.py']"
c0b27fb01938be8c87e5e80a354a324c5e2a0a20,"Added drop_last to DeepSpeedDataLoader (#1321)

* Added drop_last to DeepSpeedDataLoader

This solves issue #326

* Updated drop_last in engine.py

added drop_last as a ds_config as mentioned by @tjruwase

* Update engine.py

* Update engine.py

* updated config.py and constants.py

* Update constants.py

* added dataloader_ prefix

* Update dataloader.py

* corrected yapf test errors

* Update test_data.py

Added dataloader_drop_last unit test

* Corrected yapf and formatting issues

* updated simple_model.py and test_data.py

* Update simple_model.py

* pre-commit fix

* corrected issues

* Update test_data.py

* Update test_data.py

* Update test_data.py

* Update test_data.py

* removed batch_size from test_data.py

* Update simple_model.py

* Update test_data.py

* Update test_data.py

* Fix unit test issues

* Use fp32 to make things work

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['ddffbae021fef24bf64945d45ace9bac7e08508d'],False,"['config.py', 'constants.py', 'dataloader.py', 'engine.py', 'simple_model.py', 'test_data.py']"
2c62843965f777ead99719202cd31f70fd3ad0a3,"Added 4-byte alignment on NCCL/RCCL (#1328)

* Added 4-byte alignment on NCCL/RCCL

* pre-commit formatting fixes

* Fix for checkpoint loading with optimizer partitioning

* Better assert print

* Added unit tests for nccl/rccl 4-byte alignment

* bug

* Updated alignment to implicit

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['3e7d06ad4c8ed3d47e23ad2bf75f611946c1277a'],False,"['stage2.py', 'test_zero.py']"
b712babe5e71429af61647b38861497393bf9619,"Correctness fix PP+ZeRO for gradient accumulation (#1264)

* pass GAS boundary state from PP -> ZeRO

* formatting

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['bff6126f0ddbd1a03da66867571ac87b11c21ac1'],False,['engine.py']
9b915fed07971a5f3a90da35c3bbf9f365c2cb2a,fix unit test failure when merging PP ckpt files (#1359),['9f5939d2a7bcdd2953d52a0baf09ede485221a81'],False,['state_dict_factory.py']
cf22a691d6ce44ed512f953df405d488582b44a1,"[zero Init] fix regression (#1373)

* [zero Init] fix regression

* clean up the warning",['90398a700c0519fcd23f77e2a71b2c41a460b4a5'],False,['partition_parameters.py']
30537e719c9b86556fa1c821cf64880cb6398fca,"[zero_to_fp32] adapt to 4-bytes alignment in z2 (#1372)

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['cf22a691d6ce44ed512f953df405d488582b44a1'],False,"['zero_to_fp32.py', 'test_zero.py']"
4ad8019cdf5b48ff169bacc2ca3dce5f80671709,fix: support three digit layer numbers (#1377),['45a498d09886593bd4cbfc8c0f5f9463880e569e'],False,['module.py']
6996bb015911f5b437e2a732dd7f478f15abd167,"Sparse attn triton v1.0 support + torch1.8 test runner (#1374)

Co-authored-by: Reza Yazdani <reyazda@microsoft.com>",['4ad8019cdf5b48ff169bacc2ca3dce5f80671709'],False,"['main.yml', '__init__.py', 'matmul.py', 'softmax.py', 'sparse_self_attention.py', 'engine.py', 'requirements.txt', 'common.py', 'megatron_model.py', 'simple_model.py', 'test_checkpointing.py', 'test_config.py', 'test_sparse_attention.py']"
364994ad34943f97e5a23da09f8246a461fb17d9,"[zero_to_fp32] fix padding removal (#1380)

* [zero_to_fp32] fix padding removal

* style

* fix comments

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['51a2e916b730cf676c66532b19d973a603377cb0'],False,"['zero_to_fp32.py', 'test_zero.py']"
86dd6a6484a4c3aa8a04fc7e7e6c67652b09dad5,Fix from Felipe and Young for loading checkpoints. (#1389),['364994ad34943f97e5a23da09f8246a461fb17d9'],False,['engine.py']
9e5c0c5c3ecabb68b7e9dffac0e9b8d167e3cab8,"[CI] install fix for HF tests and use dict instead of json for some tests (#1405)

* install HF w. dev extra to get all required packages

* switch ds.init to use param dict instead of json file on disk

* switch back to 'testing' extra",['c1829c45851eb1988684c9a2f4fd054ebd446b95'],False,"['main.yml', 'test_configurable_parallel.py']"
e2fdd254edff5780d00c04111572c913ee698719,"Big science related changes (#1407)

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>
Co-authored-by: Shaden Smith <shaden.smith@microsoft.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Samyam Rajbhandari <samyamr@microsoft.com>
Co-authored-by: Reza Yazdani <44502768+RezaYazdaniAminabadi@users.noreply.github.com>
Co-authored-by: eltonzheng <eltonz@microsoft.com>
Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>",['39c7744026e323471c835e67854d196d45dc3c4d'],False,"['DeepSpeedExamples', 'checkpointing.py', 'engine.py', 'fused_optimizer.py', 'unfused_optimizer.py', 'engine.py', 'module.py', 'p2p.py', 'state_dict_factory.py', 'utils.py', 'partition_parameters.py', 'stage1.py', 'stage2.py', 'stage3.py', 'zero_to_fp32.py', 'megatron_model.py', 'test_configurable_parallel.py', 'test_fp16.py', 'test_zero_context.py']"
56124621ce2646ebb5d95f858c1d7f8c6c9f1387,Fix typo in assert message (#1420),['a4ffc831e69ef8f92bc340e9c07332aa35c4d34f'],False,['partition_parameters.py']
aa8d97e6050ae5d748bb2cc93dc081f5cde22c45,"Improve inference documentation (#1421)

* Fix spelling errors in inference tutorial

* Remove unused imports in inference tutorial

* Fix inference tutorial code to work with 1 GPU",['9f17087fddc93f16f5c6ef16f3f72bc9a30e8b52'],False,['inference-tutorial.md']
0457bb1cb661c4783712489b529317437e086bc7,Add assert to ensure we don't skip unsupported grad dtypes (#1418),['aa8d97e6050ae5d748bb2cc93dc081f5cde22c45'],False,"['engine.py', 'config-json.md', 'common.py', 'test_sparse_grads.py']"
be789b1665b8cafbd1b865534da501b3b287b17f,"Fix many typos (#1423)

* Fix typos in docs/

* Fix typos in code comments and output strings

* Fix typos in the code itself

* Fix typos in tests/

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['30965ea734587e80d130d78876c437844655090c'],False,"['deepspeed_py_aio_handle.cpp', 'ds_aio_basic.py', 'ds_aio_handle.py', 'run_read_sweep.sh', 'run_write_sweep.sh', 'custom_cuda_layers.h', 'ds_transformer_cuda.h', 'softmax.h', 'type_shim.h', 'pt_binding.cpp', 'quantizer.cu', 'engine.py', 'launch.py', 'runner.py', 'replace_module.py', 'sharded_moe.py', 'cpu_adam.py', 'bert_sparse_self_attention.py', 'sparse_attention_utils.py', 'sparse_self_attention.py', 'sparsity_config.py', 'transformer_inference.py', 'transformer.py', 'README.md', 'profiler.py', 'checkpointing.py', 'config.py', 'eigenvalue.py', 'engine.py', 'adam.py', 'lr_schedules.py', 'module.py', 'p2p.py', 'schedule.py', 'topology.py', 'quantize.py', 'state_dict_factory.py', 'optimizer_utils.py', 'partitioned_optimizer_swapper.py', 'partitioned_param_swapper.py', 'constants.py', 'contiguous_memory_allocator.py', 'linear.py', 'partition_parameters.py', 'stage2.py', 'stage3.py', 'groups.py', 'zero_to_fp32.py', 'config-json.md', '2021-05-05-MoQ.md', '2021-05-05-inference-kernel-optimization.md', 'MoQ-tutorial.md', 'advanced-install.md', 'flops-profiler.md', 'pytorch-profiler.md', 'sparse-attention.md', 'initialize.rst', '__init__.py', 'builder.py', 'setup.py', 'flatten_bench.py', 'ds_gpt2_test.sh', 'modeling.py', 'modelingpreln.py', 'test_activation_checkpointing.py', 'test_autocast.py', 'test_cpu_adam.py', 'test_cuda_backward.py', 'test_dist.py', 'test_flops_profiler.py']"
d1e72f29d89a43ae383f8fbc1d90bc7c18f2c269,"Fix non-fp tensor bugs of contiguous activation checkpointing (#1376)

* Update checkpointing.py

* Fix formatting

* Add flexibility of pipeline module and engine

* Separate PRs

* Separate PRs

* Update checkpointing.py

* Update checkpointing.py

* Reflect code review for contiguous activation checkpointing

* remove useless condition

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['be789b1665b8cafbd1b865534da501b3b287b17f'],False,['checkpointing.py']
dd6bf4d0f1dbc1f5c6183e3177a0b1a2d7396553,"[zero_to_fp32] fix to handle world_size (#1422)

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['d1e72f29d89a43ae383f8fbc1d90bc7c18f2c269'],False,"['stage2.py', 'zero_to_fp32.py']"
dd0c8fa73bd84f2fe377aa46ad34b8b0bdd977a6,"Revise param_shapes to be a list of ordered dict (#1424)

* Revise param_shapes to be a list of ordered dict

* test i can push

* add tests; split z2 and z3 into separate funcs

Co-authored-by: Xiaopeng Li <xiaopel@amazon.com>
Co-authored-by: Stas Bekman <stas@stason.org>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['466b0e638cd00daacb43cc6c9ecf6e12ec9f1e73'],False,"['engine.py', 'zero_to_fp32.py', 'test_zero.py']"
86457c2db3ba1521cdd1c94ef5790e310c1e959c,"[CI] HF tests `image-classification` requirements (#1425)

HF tests `image-classification` requirements have been fixed to not require pt-1.9",['dd0c8fa73bd84f2fe377aa46ad34b8b0bdd977a6'],False,['main.yml']
bc7778ea5b6018c4982816a1bf4d11fb46eb111f,"Fix the workspace allocation for the transformer kernel (#1397)

* fix the workspace allocation for the transformer kernel

* change layer-id type & rm one unit test due to OOM",['c6d1418d85dd5fcb2287b09c2438363eb13b6c0b'],False,"['ds_transformer_cuda.h', 'ds_transformer_cuda.cpp', 'test_cuda_forward.py']"
d8e9ef6f99e27bb95e10bd146d145b3372b4cfda,"Fix typo (ValueError) (#1434)

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['5986dbca5ec5acc2e83311b14d54bfaf00d311d7'],False,"['engine.py', 'stage2.py', 'test_checkpointing.py']"
9c672783e95c3729ee7dae72c2afdb0b46ed5ff9,"Big science fix passing multiple tensors (#1400)

Co-authored-by: Thomas <thomas@Thomass-MacBook-Pro.local>
Co-authored-by: Shaden Smith <shaden.smith@microsoft.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>
Co-authored-by: Tunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>",['d8e9ef6f99e27bb95e10bd146d145b3372b4cfda'],False,['engine.py']
cd7967d6b5465e0f1e52d1e111b98a04720e8f37,fix cl for pp support (#1443),['fbea7b493f26b60e3c98dcc0fabd1a3ff8871b61'],False,['engine.py']
80e263c571599a93f412c850f0ff7a44a9051a90,"fix sparse_attention imports (#1446)

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['f88b0e5f6e818070703fac93d0afc98e74c4aaf8'],False,['sparse_self_attention.py']
c64a03db779d516b3bb80e5eaacab6c7107af791,Fix docstrings for lr_schedules.py (#1455),['80e263c571599a93f412c850f0ff7a44a9051a90'],False,['lr_schedules.py']
dd22428465fae8bc5b9220f586d7464a0f698dce,fix typos and add improvements (#1463),['b4e5826a60b275600673a77478ecb749519caaf0'],False,['memory.rst']
17a479dd8c38aacdae5fda2841526856824aff9e,"fix pipeline engine (#1473)

* fix pp

* better fix",['58a8e13ccd701c76346448a29f8842fd6ef74f7d'],False,['engine.py']
29bee73f034926d608a5d495bb421f1111f64e65,fix pp (#1474),['17a479dd8c38aacdae5fda2841526856824aff9e'],False,['engine.py']
0b77d1d98a7b6f96937cd70db2adf6ef19062ba5,"add moe+zero ckpt unit test. (#1429)

* Add unit test to check moe+zero checkpoints
* Fix zero stage2 checkpoint loading logic to deal with experts related state dicts.",['29bee73f034926d608a5d495bb421f1111f64e65'],False,"['stage2.py', 'test_checkpointing.py']"
8abdaee2430497438729daeded370db8a0e03c9b,"Add cpu adagrad (#1358)

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>
Co-authored-by: Reza Yazdani <44502768+RezaYazdaniAminabadi@users.noreply.github.com>",['0b77d1d98a7b6f96937cd70db2adf6ef19062ba5'],False,"['cpu_adagrad.cpp', 'cpu_adam.cpp', 'custom_cuda_kernel.cu', 'compat.h', 'cpu_adagrad.h', 'cpu_adam.h', 'custom_cuda_layers.h', 'simd.h', '__init__.py', '__init__.py', 'cpu_adagrad.py', 'config.py', 'engine.py', '__init__.py', 'cpu_adagrad.py', 'cpu_adam.py', 'test_cpu_adagrad.py']"
fcb3ca5e669e4ca1f1190825e8d90c11c5dbbd9d,"Proposal of how we might use sparse tensors for gradients (#1484)

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['8abdaee2430497438729daeded370db8a0e03c9b'],False,"['engine.py', 'sparse_tensor.py', 'test_checkpointing.py', 'test_csr.py', 'test_sparse_grads.py']"
24dd285ff48dc89c32d49ca03aa468cf73a9ad50,fix read-the-docs based on this issue: https://github.com/sphinx-doc/sphinx/issues/9727 (#1489),['fcb3ca5e669e4ca1f1190825e8d90c11c5dbbd9d'],False,"['requirements-dev.txt', 'requirements-readthedocs.txt']"
56635d5b6c38a986663f2d92b30fe6e680930cb4,"enable/disable moe token dropping. (#1492)

* Add a flag to enable/disable token dropping in moe/top-1 gating.

* fix syntax and formatting.",['99bd592d253c6c2e995c49978c4444f989cfd7be'],False,"['layer.py', 'sharded_moe.py']"
e976accb57a8084ba3801b4de867b4575d1c4735,Fix typo (#1501),['56635d5b6c38a986663f2d92b30fe6e680930cb4'],False,['memory.rst']
db5d8ba2fd4aa25bb3d9a04123a6517e1736fbe9,"Fix OneCycleLR zero division error (#1498)

* Add regression test for onecyclelr zerodivision error

* Wrap computation of lr and mom decay factors in try except block

This handles the case when decay_step_size is set to zero, which is the default case,
and prevents a zero division error

* Use boolean attributes instead of try/except block",['e976accb57a8084ba3801b4de867b4575d1c4735'],False,"['lr_schedules.py', 'test_lr_schedulers.py']"
7f5a3addb648f981377eee8c4a3a2d7f2472a548,"update CL doc (#1506)

* update CL doc

* doc fix",['163f568f63a3b66e2351382de673bb98533a1fc4'],False,"['README.md', 'curriculum-learning.md', 'index.md']"
c0eeb69dfb7326948f7d0063aa1b4112ff575cee,"ZeRO3, improved parameter all-gather operation (#1188)

* remove norm(), avoid memcpy after allgather

1) Removing the norm computation in debug printing
2) Changing _all_gather to be sync op in fetch_sub_module
    Reason: the async version is not async at all, because each
    all_gather calls torch.cuda.synchronize() to guarantee previous
    communication op to be completed
3) Adding new function _allgather_params_split_launch
    the existing _allgather_params has explicit memcpy after the
    all-gather op. We can avoid the explicit memory copy at
    python side, to improve the performance.

Known issue:
    the `torch.distributed.all_gather` will do implicit memcpy
    at the end of each `ncclAllgather`.

* WIP: wrapped ncclAllgather as customized op in DS

micro benchmark shows the improvement of allgather a
transformer layer with 9834560 elements in half precision is about
1.1ms on aws-p4d instance.

* WIP: integrated into partition_parameters

Performance improvement of 5.1B bert on aws-p4d:
fwd: 300ms -> 200ms
bwd: 680ms -> 610ms

* Fix format

* cleaned dead code, modified unit test

* removed customized c++ extension

revert back to use torch distributed API

* change torch.ones to torch empty

* typo

* warn if not cuda tensor for allgather

* fix formatting

* fix: move ds_tensor to cuda device

but it is strange that the ds_tensor haven't been moved to cuda

* remove try clause on the path for fetching params

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['7f5a3addb648f981377eee8c4a3a2d7f2472a548'],False,"['partition_parameters.py', 'stage3.py']"
2c5bba6dc1db84168691f77cb882669d45f8fd8a,"Transformer kernel - fix unit test (#1503)

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['c0eeb69dfb7326948f7d0063aa1b4112ff575cee'],False,"['normalize_kernels.cu', 'transformer_inference.py', 'test_cuda_backward.py']"
648f7bfa5009484b822064d0c28d377da6dd71a0,"Bfloat16 zero2 (#1398)

* Changes for bfloat16 Zero2

* Cleaned up additional comments and debugging code

* Adapted fp16_master_weights_and_grads option to cover BF16

* Reverted fp16_master_weights_and_gradients extension to BFloat16 and minor cleanup

* Fixed formatting and variable naming errors recognized in testing

* Added relevant unit tests for bfloat16 with ZeRO-2

* Updates conditions for skipping BFloat16 unit tests

* Added check for NCCL inconsistent version naming convention

* Update skip message for Bfloat16 tests to mention additional checks

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['2c5bba6dc1db84168691f77cb882669d45f8fd8a'],False,"['type_shim.h', '__init__.py', 'config.py', 'constants.py', 'engine.py', 'stage2.py', 'test_bf16.py', 'test_zero.py', 'util.py']"
bf1725bb5791fab83294df6879af179ebdbd7f65,"[code readability] pipe (#1510)

This PR suggests a small improvement to code readability.

--------------------------

I was puzzling over this code:

https://github.com/microsoft/DeepSpeed/blob/85ce85dd5f4b18c0019a5121b06900e3a2c3933b/deepspeed/runtime/pipe/module.py#L381-L385

I had no idea this construct existed. 

After reading up on it, it appears to be used incorrectly. The only point to using it with `break`.

It's explained here https://docs.python.org/3/tutorial/controlflow.html#break-and-continue-statements-and-else-clauses-on-loops

So I'm proposing to remove the `else` control and just run the code in its branch normally since it *always* gets executed as there is no `break` statement.

And the objective of this code is to always be run if I understand it correctly. So let's make it loud and clear.

Here is a quick proof:
```
for i in []:
    print(i)
else:
    print(""loop did not finish via break"") # runs!

for i in [0]:
    print(i)
else:
    print(""loop did not finish via break"") # runs!


for i in [0]:
    print(i)
    break
else:
    print(""loop did not finish via break"") # does not run
```

@tjruwase",['85ce85dd5f4b18c0019a5121b06900e3a2c3933b'],False,['module.py']
cf1f16016f7c700bc53e2d50df06d063469b5e7d,"Use fstr in launcher (#1521)

* Use fstr in launcher

* Fix wrong condition for word_info

* Fix typo",['f9b378012e0956a0b28b0f939c7a8dec44947f89'],False,['launch.py']
2c62d657a45bce5339f188ddb5c51bd0fa94a897,"typo in profiler.py (#1527)

Fix typos in Flops Profiler message

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['2887349cd456dab19cbe6e1e2b9e519c0eecd29f'],False,['profiler.py']
76f2b5e51d8cf68d1966dceaf1a562a6f02d73fb,"[docs] fix 404 (#1531)

* [docs] fix 404

This PR fixes a few broken links

* fix 404",['2c62d657a45bce5339f188ddb5c51bd0fa94a897'],False,['onebit-adam.md']
af443f63f483f6ea6769b78b4b0f2407023e9aed,"CPU-Adam: Fix compile Issue (#1537)

* fixing the softmax masking when using triangular masking

* move the TILE declaration outside of the SIMD loop

* remove unrelated changes

* fix Adagrad compile issue",['f0122007df11281262efcd1525378db54e0261fd'],False,"['cpu_adagrad.h', 'simd.h']"
2665c8b149147a18e1d03d2a92bb4641d1db4af4,Fix 1bit extra issue (#1542),['bd3ebddf3628f3f77d3460e49626c8af7825a92c'],False,"['main.yml', 'requirements-1bit-mpi.txt', 'setup.py']"
3ed77304df9a56f31779fb13f0ee9347f158d2cc,"Fix sparse attention for small block-sizes (#1545)

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['9ce00a2171af9284b8c3238f2579a4d7d6a4e190'],False,['matmul.py']
76847f42cf1247bbbf4831819e27086475137214,"Add warmup_type arguments in WarmupLR and WarmupDecayLR (#1530)

* Add warmup_type arguments in WarmupLR and WarmupDecayLR

* Add warmup_type unit test

* replace hardcoded constants with global vars

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['3ed77304df9a56f31779fb13f0ee9347f158d2cc'],False,"['DeepSpeedExamples', 'lr_schedules.py', 'test_lr_schedulers.py']"
488105ebd200bbd1f6d7cbe863412e41d9ab4221,Fix zinf none swapper (#1550),['76847f42cf1247bbbf4831819e27086475137214'],False,"['engine.py', 'partitioned_param_swapper.py', 'stage3.py']"
9caa74e577c59fa6cf5de3992ea129c979dbad13,"Autotuning (#1554)

* [squash] Staging autotuning v4

Co-authored-by: Cheng Li <pistasable@gmail.com>
Co-authored-by: Minjia Zhang <minjiaz@microsoft.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>

* add new extra, guard xgboost, cleanup dead files (#268)

* Fix autotuning docs (#1553)

* fix docs

* rewording the goal

* fix typos

* fix typos (#1556)

* fix typos

* fix format

* fix bug (#1557)

* fix bug

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>
Co-authored-by: Minjia Zhang <minjiaz@microsoft.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['b7cc7c8ea785db5fdaa17828a3e901c3827b2c66'],False,"['formatting.yml', 'main.yml', 'MANIFEST.in', '.gitignore', 'README.md', '__init__.py', 'autotuner.py', 'config.py', 'template_zero0.json', 'template_zero1.json', 'template_zero2.json', 'template_zero3.json', 'constants.py', 'scheduler.py', 'README.md', '__init__.py', 'base_tuner.py', 'cost_model.py', 'index_based_tuner.py', 'model_based_tuner.py', 'utils.py', 'utils.py', 'runner.py', 'config.py', 'config_utils.py', 'engine.py', 'utils.py', 'offload_config.py', 'offload_constants.py', 'partition_parameters.py', 'stage2.py', 'logging.py', 'timer.py', '_config.yml', 'navigation.yml', 'config-json.md', 'features.md', 'autotuning.md', 'autotuning.rst', 'index.rst', 'requirements-autotuning-ml.txt', 'requirements-autotuning.txt', 'requirements.txt', 'setup.py', 'test_autotuning.py', 'test_zero.py']"
4bf4ab7ac5b2fa8697e634561256121b8b8b0f89,"Fix partial recovery of sparse_tensor_module_names and dynamically check if gradient data is sparse  (#1562)

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['bda3d0e6b98734c3dd8bef48c71bd4498b2a2dff'],False,"['engine.py', 'test_checkpointing.py']"
74baf5bbb91d51cf027eeeff399d884ef1a147c9,[CI] transformers@master has been fixed (#1573),['236890d6f315864b4e5e3af68335eac20e720039'],False,['main.yml']
2332cb31a7c221e18adcc0604af09acbbb2ecbe8,Enables ZeRO-3 inference (#1514),['74baf5bbb91d51cf027eeeff399d884ef1a147c9'],False,"['engine.py', 'utils.py', 'partition_parameters.py', 'stage3.py', 'simple_model.py', 'test_ds_initialize.py']"
a8a17f234a5f5e49d17c3d5493783ca0aff0cce3,Several fixes for our read-the-docs build (#1579),['2332cb31a7c221e18adcc0604af09acbbb2ecbe8'],False,"['__init__.py', '__init__.py', 'conf.py', 'requirements-readthedocs.txt']"
1bc13fe83fb3dbf8da08faf21ecdc9dc741f4945,"Removing `ImportError` from tutel import try/except (#1583)

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['e2b39ded9f3e0316ef57632497568acefcb6231b'],False,['sharded_moe.py']
499800caa85ef682364d1f4dc323cfbe91588d5e,Fix return code on error (#1540),['a637cc2cd508381d79a522c98fef0d4e747ef28f'],False,['launch.py']
51d42ab9ec826449c39d052669ca33a867c20cb5,"fix partition activations issue when mp=2 and pp=2 (#1589)

* fix partition activations issue when mp=2 and pp=2

* change util function input and fix pre-commit errors

* move print_backward_tensors() to debug.py

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['499800caa85ef682364d1f4dc323cfbe91588d5e'],False,"['checkpointing.py', 'debug.py']"
d14baad9401bc1de8c982c1b3590d91108c4bbf1,"allreduce_always_fp16 (#1487)

* fp16 allreduce

* Undo sparse sum in nan check

* communication_data_type instead of fp32_allreduce and fp16_allreduce

* sparse_allreduce with fp32 or fp16 data type

* FIx communication_data_type checks

* Allow only torch data types for communication_data_type

* Fix Zero assert messages

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['52c7889b0142be0823b60eaae27f252deb73141b'],False,"['config.py', 'constants.py', 'engine.py', 'stage2.py', 'stage3.py', 'config-json.md']"
7a132a9f4b37959f951b7c04a05207aba6054965,port OVERFLOW log to ZeRO-2 (#1593),['d14baad9401bc1de8c982c1b3590d91108c4bbf1'],False,['stage2.py']
a10e4811fe78b707289132c9695bade4715fe59b,force set lf instead of crlf (https://github.com/pre-commit/pre-commit-hooks#mixed-line-ending) (#1598),['c3f1d82b07b7541ee03961b07959b9f38894de5e'],False,"['.clang-format', 'formatting.yml', '.pre-commit-config.yaml', 'CODE_OF_CONDUCT.md', 'LICENSE', 'SECURITY.md', 'cpu_adagrad.cpp', 'cpu_adam.cpp', 'deepspeed_aio_common.cpp', 'deepspeed_aio_common.h', 'deepspeed_aio_types.cpp', 'deepspeed_aio_types.h', 'deepspeed_aio_utils.cpp', 'deepspeed_aio_utils.h', 'deepspeed_aio_thread.cpp', 'deepspeed_aio_thread.h', 'deepspeed_py_aio.cpp', 'deepspeed_py_aio.h', 'deepspeed_py_aio_handle.cpp', 'deepspeed_py_aio_handle.h', 'deepspeed_py_copy.cpp', 'deepspeed_py_copy.h', 'py_ds_aio.cpp', 'ds_aio_basic.py', 'ds_aio_handle.py', 'parse_aio_stats.py', 'test_ds_aio.py', 'test_ds_aio_utils.py', 'custom_cuda_kernel.cu', 'Timer.h', 'cpu_adagrad.h', 'cpu_adam.h', 'dropout.h', 'feed_forward.h', 'gelu.h', 'gemm_test.h', 'general_kernels.h', 'normalize_layer.h', 'simd.h', 'softmax.h', 'strided_batch_gemm.h', 'dropout_kernels.cu', 'gelu_kernels.cu', 'general_kernels.cu', 'dequantize.cu', 'gelu.cu', 'normalize.cu', 'pt_binding.cpp', 'softmax.cu', 'context.h', 'cublas_wrappers.h', 'custom_cuda_layers.h', 'normalize_kernels.cu', 'softmax_kernels.cu', 'transform_kernels.cu', 'constants.py', 'multinode_runner.py', 'module_quantize.py', 'replace_policy.py', 'cpu_adagrad.py', '__init__.py', 'cpu_adam.py', '__init__.py', 'bert_sparse_self_attention.py', 'checkpointing.py', 'config.py', 'config_utils.py', 'eigenvalue.py', 'progressive_layer_drop.py', 'quantize.py', 'stage3.py', 'README.md', '2021-03-08-zero3-offload.md', '2021-05-05-inference-kernel-optimization.md', 'mixture-of-experts.md', 'progressive_layer_dropping.md', 'zero-offload.md', 'zero.md', 'schedulers.rst', 'adam_test.py', 'adam_test1.py', 'ds_batch_config.json', 'modelingpreln.py', 'test_aio.py', 'test_cpu_adagrad.py', 'test_cpu_adam.py', 'test_onebit.py', 'test_pld.py']"
fc2f378ece3055fddd13cdf309faa9ab66a1b302,"Improve pre-commit hooks (#1602)

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['8159c1bc5b5a7aa70d53fdd7190fc6f32ad88943'],False,"['.pre-commit-config.yaml', 'template_zero3.json', 'replace_module.py', 'utils.py', 'fused_adam.py', 'sparse_self_attention.py', 'config.py', 'engine.py', 'utils.py', 'config.py', 'partition_parameters.py', 'stage3.py', '2020-05-28-fastest-bert-training.md', '2021-03-08-zero3-offload.md', '2021-05-05-MoQ.md', '2021-05-05-inference-kernel-optimization.md', 'megatron.md', 'conf.py', 'requirements-autotuning-ml.txt', 'requirements-dev.txt', 'requirements-readthedocs.txt', 'requirements.txt', 'BingBertSquad_run_func_test.py', 'BingBertSquad_test_common.py', '__init__.py', '__init__.py', 'run_checkpoint_test.py', 'run_func_test.py', 'run_perf_baseline.py', 'run_perf_test.py', 'test_common.py', 'run_sanity_check.py', 'test_activation_checkpointing.py', 'test_checkpointing.py', 'test_onebit.py', 'test_pipe.py', 'test_pipe_module.py']"
8e891aa56844b9df705aeb14c87b2c069ed84e01,"Transformer kernel/fix layer norm (#1587)

* fixing the softmax masking when using triangular masking

* fix a bug in the the layernorm backward kernels

* revert back some changes & remove debug code

* change the constants to a macro

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['fc2f378ece3055fddd13cdf309faa9ab66a1b302'],False,"['custom_cuda_layers.h', 'normalize_kernels.cu', 'softmax_kernels.cu', 'engine.py', 'test_cuda_backward.py', 'test_cuda_forward.py']"
cda7c718954e22f22df222872400e29eaf5708c1,Sparse Attention: Fix Triton errors (#1608),['4b854a37cbbc60b3a0082f465c034d077447c963'],False,"['sparse-attention.md', 'requirements-sparse_attn.txt']"
3488b8cdd3ed3218885b271bd78626f681d4342e,"[readme] remove stats badge until PyPI is fixed

see pypi issue: https://status.python.org/incidents/2jj696st6yn5",['cda7c718954e22f22df222872400e29eaf5708c1'],False,['README.md']
91e15593ea4487014114a03c7b4a2a05567fd3f8,"Control ds_report output (#1622)

* Control ds_report output with two flags --hide_operators and --hide_errors_and_warnings
Separate cli and function entry points to ds_report

* Formatting fixes",['3488b8cdd3ed3218885b271bd78626f681d4342e'],False,"['ds_report', 'env_report.py', 'async_io.py', 'builder.py', 'cpu_adagrad.py', 'cpu_adam.py', 'sparse_attn.py']"
1d295ff5f8f271ac24502e1f3221649b84fe2de1,"Refactor ZeRO naming to reduce confusion (#1607)

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['07887f663006944271d3fac4d4f860f23573872e'],False,"['main.yml', 'engine.py', 'stage1.py', 'stage3.py', 'stage_1_and_2.py', 'config-json.md', 'test_checkpointing.py']"
7f58853c2e181604d2e5f5ae308524788ad0cd28,[testing] 3x faster unit tests (#1636),['1d295ff5f8f271ac24502e1f3221649b84fe2de1'],False,"['main.yml', 'requirements-dev.txt', 'common.py', 'test_cuda_forward.py', 'test_onebit.py', 'test_zero.py', 'test_zero_context.py']"
082f392a939896e2fef782eabee2200b5e340902,"Add tensor methods in flops counting and separate macs and flops (#1591)

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['7f58853c2e181604d2e5f5ae308524788ad0cd28'],False,"['README.md', 'profiler.py', 'flops-profiler.md', 'test_flops_profiler.py']"
259936a76c0a2baafdffabbef8b0e2843f969d48,Fix cpu-adam AVX performance (#1637),['082f392a939896e2fef782eabee2200b5e340902'],False,['simd.h']
d93d924a77a3f6a427aad7f1e7d83be9b62d9fb3,"follow-up to #1652, resolved a100-80gb issue (#1655)",['cbd68dc480e4e96d3976e4d10a6203c5fe5382dd'],False,['builder.py']
559c4ce11a6b226f856664071d820a9c97feaf9e,"Convert the fp16_params to group of parameters (#1651)

* convert the fp16_params to group of parameters

* fix typo

* check the type of fp16_params

* fix issue when fp16_param_groups is None

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['40ce131caa6f5a8dfb8f50a933ed8ae5d1ddb0ad'],False,['cpu_adam.py']
4354c3cc675c9c59a4f1edd608371322f1986c38,"Fix largest param numel calculation (#1623)

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['74493b2bee5a615b16d81bdf17ef21859500113d'],False,['stage3.py']
2422ec4885905bacb87628cc5880e7ec87177cfb,add segfault guard for cpu-adam/adagrad (#1681),['cef116f82c8875a2f79ed939eefd5520488a070b'],False,"['cpu_adagrad.py', 'cpu_adam.py', 'test_cpu_adagrad.py', 'test_cpu_adam.py']"
762e697a03baaeeec07c5f06da1dfa5d2259b997,fix the half-precision version of rotary_pos_emb kernel (#1683),['289c3f9ba42f70f049f988a8509fcd13d4b63994'],False,['apply_rotary_pos_emb.cu']
a85dce0728178dbb44f3ccbc495c5b9038d29952,add -lcurand to fix torch-nightly issue w. JIT (#1688),['3a4cb042433a2e8351887922f8362d3752c52a42'],False,['builder.py']
3293cf72a0abd5cf77a831996bd054bc908476a6,"[ZeRO] Default disable elastic ckpt in stage 1+2 and reduce CPU memory overhead during ckpt load (#1525)

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['e4cf40d6172199f304cb006d9d22f9581b900c3a'],False,"['constants.py', 'engine.py', 'fused_optimizer.py', 'unfused_optimizer.py', 'constants.py', 'stage3.py', 'stage_1_and_2.py', 'utils.py', 'simple_model.py', 'test_checkpointing.py']"
e46d808a1b6cb7e04cb2806e38547b1e3e50c25a,"MoE inference + PR-MoE model support (#1705)

Co-authored-by: Reza Yazdani <reyazda@microsoft.com>
Co-authored-by: Zhewei Yao <zheweiy@berkeley.edu>
Co-authored-by: Ammar Ahmad Awan <ammar.awan@microsoft.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>
Co-authored-by: Samyam Rajbhandari <samyamr@microsoft.com>",['3293cf72a0abd5cf77a831996bd054bc908476a6'],False,"['MANIFEST_win.in', 'gelu.cu', 'pt_binding.cpp', 'context.h', 'cublas_wrappers.h', 'custom_cuda_layers.h', '__init__.py', 'engine.py', 'replace_module.py', 'replace_policy.py', 'experts.py', 'layer.py', 'sharded_moe.py', 'utils.py', '__init__.py', '__init__.py', 'moe_inference.py', 'transformer_inference.py', 'engine.py', 'fused_optimizer.py', 'utils.py', 'weight_quantizer.py', 'stage_1_and_2.py', 'groups.py', '__init__.py', 'builder.py', 'setup.py', 'test_checkpointing.py', 'test_moe.py', 'version.txt']"
94de0229fb0d835ca9f3019ff5195edb3c354910,Fix inference api & add more description on inference engine tutorial (#1711),['2662fded2dbb8c62939fe9d339014385b187d9d9'],False,"['engine.py', 'replace_policy.py', 'inference-tutorial.md']"
4912e0ad7efcaf97389ae944259aa0e9f331038a,"Various ZeRO Stage3 Optimizations + Improvements (including bfloat16 support) (#1453)

* Changes for bfloat16 Zero2

* ZeRO stage3 optimizations, with some bug fixes

optimizations for stage3:
- prefetching improvements
- batching allgather calls to amortize fixed overhead and improve
  bandwidth utilization
- batching reduce_scatter calls to amortize fixed overhead and
  improve bandwidth utilization
- using *_base variants of allgather and reduce scatter to reduce memory
  allocations and data movement
- more fine grained synchronization for communication that allows
  blocking on less work
- precomputation of fetching code - using a fetch queue rather than
  deciding what to (pre)fetch at each iteration
- limiting queued coalesced communication ops to reduce memory pressure
  on pytorch cuda caching allocator (not elegant solution)

optimizations for stage3-offload:
- made some host-device tensor copies async to improve performance

bug fixes and qol improvements:
- fix init context method when parent modules modify child weights
- speed up model initialization by moving model to GPU before weight
  initialization
- fixed unit test imports so that unit tests can be run from any
  directory
- change performance logging to include memory consumption
- add logging w/ model size when done partitioning model

new features
- bfloat16 support for ZeRO 3

* fix import in ut

* ran yapf

* improvements to cache flush warn log

* backwards compatibility with older versions of pytorch

* handle edge case where reduced tensor smaller than world size

* moved event synchronization to allgather handle wait() call

* removed unnecessary barrier call

* formatting fix after resolving merge conflict

* skip nvme prefetch when trace not complete

* opportunistically avoid memory allocation in allgather coalesced where possible

* fix indentation after merge

* fixes to account for parameter offload

* accounting for torch.cuda.memory_stats not being available

* moved partition_all_params to optimizer step

* allgathering on params before item gets called

* fix param status checks

needed after moving partition_all_parameters call to optimizer step

* fix grad accumulation with optimizer offload

* grad norm computation fix for optimizer offload

* change post divide in reduce-scatter to pre divide

* fix gradient race condition w/ optimizer offload

* improve inf/nan gradient tracking

* don't prefetch when not in training mode

* format fix after merging

* fix prefetching issue when using NVME offload

* improved defragmentation for fp16 parameters

* relative imports for bf16 tests

* changes for bwd compatibility with pytorch 1.2

* remove buffered_reduce_fallback

* removed unused parameter offset bookkeeping

* fixed tracking for multiple param groups

* unbroke bfloat16 config after merge conflict

* using base allgather params when only 1 param

* cleanup/fixes for fp16 partition defragmentation

* switch to CRLF

* convert to same new-line style as master

* align new line with master

* Fix merge issues

* switch to CRLF

* fix to LF line endings

* minor merge fixes

* remove extra bfloat16_enabled definition

* asserting params inflight for AllGatherHandle

* remove get_cuda_mem_allocated_str

* Format fixes

* fix bfloat16 zero stage check (broken after merge commit)

* +self.communication_data_type, -self.allreduce_always_fp32; delete dead code

* Add self.reduce_scatter

* Format fix

* Fix merge issues

* iterate over params_to_fetch rather than make another iterator

* add some TODOs

* remove unnecessary division by micro_step_id

* rename config keys ""bfloat16"" -> ""bf16""

* rename stage3_gather_fp16_weights_on_model_save -> stage3_gather_16bit_weights_on_model_save

* add unit test to check backwards compatibility for gather_16bit_weights

* added test to confirm bf16 key bwd compatibility

* Format fixes

Co-authored-by: Rana Ali Amjad <raamjad@amazon.com>
Co-authored-by: Justin Chiu <justchiu@amazon.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['2d51f6171b75d13d7b1861d27028e88aec5f9fcc'],False,"['DeepSpeedExamples', 'template_zero3.json', 'coalesced_collectives.py', 'config.py', 'constants.py', 'engine.py', 'utils.py', 'config.py', 'constants.py', 'partition_parameters.py', 'stage3.py', 'utils.py', '__init__.py', 'nvtx.py', 'timer.py', 'config-json.md', 'zero.md', 'training.rst', '__init__.py', 'megatron_model.py', 'test_activation_checkpointing.py', 'test_adamw.py', 'test_aio.py', 'test_autotuning.py', 'test_bf16.py', 'test_checkpointing.py', 'test_coalesced_collectives.py', 'test_config.py', 'test_configurable_parallel.py', 'test_cuda_backward.py', 'test_cuda_forward.py', 'test_curriculum_learning.py', 'test_data.py', 'test_dist.py', 'test_ds_initialize.py', 'test_dynamic_loss_scale.py', 'test_elastic.py', 'test_flops_profiler.py', 'test_fp16.py', 'test_ignore_unused_parameters.py', 'test_lr_schedulers.py', 'test_moe.py', 'test_multi_output_model.py', 'test_onebit.py', 'test_partition.py', 'test_pipe.py', 'test_pipe_module.py', 'test_pld.py', 'test_runtime_utils.py', 'test_sparse_grads.py', 'test_topology.py', 'test_zero.py', 'test_zero_context.py']"
e40558ded241edefed410aa44dbf490ab60f05f6,Fix checkpoint api (#1714),['4912e0ad7efcaf97389ae944259aa0e9f331038a'],False,"['engine.py', 'stage3.py']"
4cf970e6bb3c2ff29b2f03fcddb6f2cf26245a23,Add codespell to pre-commit checks (#1717),['09c065b4c397093983849039af4fb4673015de1b'],False,"['.pre-commit-config.yaml', '.pylintrc', 'py_ds_aio.cpp', 'fused_lamb_cuda.cpp', 'README.md', 'autotuner.py', 'constants.py', 'scheduler.py', 'README.md', 'model_based_tuner.py', 'utils.py', 'env_report.py', 'README.md', 'coalesced_collectives.py', 'engine.py', 'unfused_optimizer.py', 'lr_schedules.py', 'engine.py', 'p2p.py', 'topology.py', 'constants.py', 'contiguous_memory_allocator.py', 'partition_parameters.py', 'stage3.py', 'stage_1_and_2.py', 'config-json.md', '2021-03-08-zero3-offload.md', 'MoQ-tutorial.md', 'flops-profiler.md', 'mixture-of-experts.md', 'moe-inference-tutorial.md', 'sparse-attention.md', 'test_onebit.py', 'test_run.py', 'test_zero.py']"
df724e71e935414bb8e73b78f9f422baca344895,"Add a very simple PyTorch Lightning test! (#1726)

* Add a very simple PyTorch Lightning test

* Run for just one epoch

* Swap to using the plugin API till the strategy API makes it to 1.6",['4cf970e6bb3c2ff29b2f03fcddb6f2cf26245a23'],False,"['main.yml', 'test_simple.py']"
ed4bbe08d630d954ea364d228b1fd8b28c63f5f6,[config] fix assert message (#1734),['9351266f7888ad3059b0fc2908d9bab721cb84b1'],False,['config.py']
fa0735760a14a1d02df7949931e4caf6a33eeb0e,"Fix the tensor-slicing with multi-GPU inference and kernel-injection (#1724)

* use the right tensor-copy function when adding tensor-slicing

* small fix in inference tutorial",['bea701a1fc87a13f26f9d2f97ff4fd779b5d8b77'],False,"['replace_module.py', 'inference-tutorial.md']"
ba9c4cc75c30dc6f7b2ea14786875fc18165761b,"separate add and mul flops compute function (#1745)

* separate add and mul flops compute function

* fix format",['fa0735760a14a1d02df7949931e4caf6a33eeb0e'],False,['profiler.py']
97f8a9eb6688fa36dbe548e02eecdc55acc821a7,fixing a bf16 support issue (#1760),['dbe8ee167b5b73ddabbf1c20ddb7b1d286f0b732'],False,['engine.py']
56eac3829f4b0dd2487e3c567321bf80153c75cf,fix pytest issue (#1764),['97f8a9eb6688fa36dbe548e02eecdc55acc821a7'],False,['test_fp16.py']
cccc5450e0c27b6f12a516c669e5a780ce7e6772,"Fix autotuning config (#1660)

* remove force=multi and fix None val check in base tuner

* fix format

* ignoring optimizer dict when generating combinations of tuning configs

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['d3cad05105d59e5c9ae51b8096fc7076bd533235'],False,"['autotuner.py', 'scheduler.py', 'base_tuner.py', 'utils.py']"
5fe5b38ea095fca3679a212668ae3b4c7e8a48f0,"Prepare zero-3 optimizer for ckpt load/save (#1750)

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['674c75882d93a4feb37b259ea981da38c1cd3bb7'],False,"['engine.py', 'stage3.py', 'test_checkpointing.py']"
baef92e26fef5aa0da63f26d444b91c2a7aa0bd3,"[save_16bit_model] add missing prologue (#1741)

* [save_16bit_model] add missing prologue

* fix

* fix

* adjust the API rename

* add test

* style

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['5fe5b38ea095fca3679a212668ae3b4c7e8a48f0'],False,"['engine.py', 'test_checkpointing.py']"
5ca2627743162a5e8efa934782150f40a38f31ed,"Fix CPU-Offload: Send groups of parameter lists as the FP16 parameters (#1774)

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['baef92e26fef5aa0da63f26d444b91c2a7aa0bd3'],False,['stage_1_and_2.py']
8eef742f0c90f7dce49861b0025f3533a4219f8e,"bf16 is supported w. zero 1, fix assert (#1779)",['5ca2627743162a5e8efa934782150f40a38f31ed'],False,['config.py']
2151c787a27166f795eb4516f1e191e6730e823d,"Generalize the  model input format of the flops profiler get_model_profile() API (#1768)

* generalize profiler model input format
* fix timining of module forward
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['8eef742f0c90f7dce49861b0025f3533a4219f8e'],False,"['README.md', 'profiler.py', 'flops-profiler.md']"
c0af6d90f7e63b14d9ccb68d51d42cadc60301da,"Refactor MoE and Groups API to simplify model creation and mangement (#1798)

Co-authored-by: yaozhewei <zheweiy@berkeley.edu>
Co-authored-by: Reza Yazdani <reyazda@microsoft.com>",['a254f39ef03ce3b670d7fe2d2545934b107ee356'],False,"['replace_module.py', 'layer.py', 'sharded_moe.py', 'utils.py', 'engine.py', 'fused_optimizer.py', 'utils.py', 'stage_1_and_2.py', 'exceptions.py', 'groups.py', 'mixture-of-experts.md', 'moe-inference-tutorial.md', 'moe.rst', 'simple_model.py', 'test_checkpointing.py', 'test_fp16.py', 'test_moe.py', 'test_runtime_utils.py', 'test_sparse_grads.py']"
c3c8d5dd93104d4b532fa68b2df217bf0e599ffa,"AMD support (#1430)

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Jithun Nair <jithun.nair@amd.com>
Co-authored-by: rraminen <rraminen@amd.com>
Co-authored-by: Jeff Daily <jeff.daily@amd.com>
Co-authored-by: okakarpa <okakarpa@amd.com>
Co-authored-by: rraminen <rraminen@amd.com>
Co-authored-by: Jithun Nair <37884920+jithunnair-amd@users.noreply.github.com>
Co-authored-by: Jeff Daily <jeff.daily@amd.com>
Co-authored-by: okakarpa <okakarpa@amd.com>
Co-authored-by: Ramya Ramineni <62723901+rraminen@users.noreply.github.com>",['f0304bd1033075f8b6a0a166aa2e7f31343519b4'],False,"['main.yml', 'cublas_wrappers.h', 'custom_cuda_layers.h', 'feed_forward.h', 'gemm_test.h', 'general_kernels.h', 'hip_cooperative_groups.h', 'hip_cooperative_groups_helper.h', 'strided_batch_gemm.h', 'fused_lamb_cuda_kernel.cu', 'cublas_wrappers.cu', 'ds_transformer_cuda.cpp', 'gelu_kernels.cu', 'general_kernels.cu', 'dequantize.cu', 'gelu.cu', 'normalize.cu', 'softmax.cu', 'custom_cuda_layers.h', 'normalize_kernels.cu', 'softmax_kernels.cu', 'transform_kernels.cu', 'env_report.py', 'git_version_info.py', '__init__.py', 'engine.py', 'Dockerfile.rocm', 'install.sh', '__init__.py', 'builder.py', 'cpu_adagrad.py', 'cpu_adam.py', 'fused_adam.py', 'fused_lamb.py', 'sparse_attn.py', 'transformer.py', 'setup.py', 'common.py', 'test_checkpointing.py', 'test_configurable_parallel.py', 'test_cuda_forward.py', 'test_onebit.py', 'test_sparse_attention.py', 'test_zero.py']"
d7684f4e8169d9f61143a9b80a0e107e7389dd00,"add GitHub URL for PyPi (#1812)

* add GitHub URL for PyPi

* add GitHub URL for PyPi fix formatting",['9f7126fc10a1f231c695db11632cbcb84f1af4d3'],False,['setup.py']
b80e5624e2823e3ad4a0bd625716eeb24921c6c9,"01 adam optimizer (#1790)

Co-authored-by: Conglong Li <conglong.li@gmail.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['ac71a1a461ca71a7bbe7a421c6b379f658d6bf78'],False,"['main.yml', 'README.md', 'config.py', 'engine.py', 'adam.py', 'zoadam.py', '_config.yml', 'navigation.yml', 'config-json.md', 'features.md', 'onebit-adam.md', 'zero-one-adam.md', 'optimizers.rst', 'index.md', 'test_onebit.py']"
908d61607271bc1b24a3e59e87076a0991a42bd4,"Website posts and tutorial improvements (#1799)

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['7bcb4fabeb6c2a84998ab41af62bd0cc9eb25f2e'],False,"['_config.yml', 'navigation.yml', 'analytics.html', 'archive-single.html', 'author-profile-custom-links.html', 'author-profile.html', 'breadcrumbs.html', 'browser-upgrade.html', 'category-list.html', 'comment.html', 'comments.html', 'documents-collection.html', 'feature_row', 'figure', 'footer.html', 'gallery', 'group-by-array', 'head.html', 'masthead.html', 'nav_list', 'page__date.html', 'page__hero.html', 'page__hero_video.html', 'page__meta.html', 'page__taxonomy.html', 'paginator.html', 'post_pagination.html', 'posts-category.html', 'posts-tag.html', 'scripts.html', 'seo.html', 'sidebar.html', 'skip-links.html', 'social-share.html', 'tag-list.html', 'toc', 'toc.html', 'video', 'news-home.html', 'single-full.html', 'posts-landing.md', 'posts_list_landing.md', 'tutorials-landing.md', '2020-02-13-release.md', '2020-02-13-turing-nlg.md', '2020-05-19-bert-record.md', '2020-05-19-press-release.md', '2020-05-19-zero-stage2.md', '2020-05-28-fastest-bert-training.md', '2020-07-24-deepspeed-webinar.md', '2020-08-07-webinar-on-demand.md', '2020-09-08-sparse-attention-news.md', '2020-09-09-ZeRO-Offload.md', '2020-09-09-onebit-adam-blog-post.md', '2020-09-09-onebit-adam-news.md', '2020-09-09-pipeline-parallelism.md', '2020-09-09-sparse-attention.md', '2020-10-28-progressive-layer-dropping-news.md', '2021-03-08-zero3-offload.md', '2021-05-05-MoQ.md', '2021-05-05-inference-kernel-optimization.md', '2021-05-14-inference-release.md', '2021-08-18-deepspeed-moe.md', '2021-11-15-autotuning.md', '2021-12-09-deepspeed-moe-nlg.md', '2022-01-19-moe-inference.md', 'button-group.scss', 'minimal-mistakes.scss', '_archive.scss', '_navigation.scss', '_page.scss', '_sidebar.scss', '_variables.scss', '_air.scss', 'MoQ-tutorial.md', 'advanced-install.md', 'autotuning.md', 'azure.md', 'bert-finetuning.md', 'bert-pretraining.md', 'cifar-10.md', 'curriculum-learning.md', 'flops-profiler.md', 'gan.md', 'getting-started.md', 'inference-tutorial.md', 'large-models-w-deepspeed.md', 'lrrt.md', 'megatron.md', 'mixture-of-experts-inference.md', 'mixture-of-experts-nlg.md', 'mixture-of-experts.md', 'one-cycle.md', 'onebit-adam.md', 'onebit-lamb.md', 'pipeline.md', 'progressive_layer_dropping.md', 'pytorch-profiler.md', 'sparse-attention.md', 'transformer_kernel.md', 'zero-offload.md', 'zero.md', 'main.scss', 'deepspeed-logo-uppercase-bold-white-1.15.svg', 'deepspeed-logo-uppercase-bold-white.svg', 'deepspeed-logo-uppercase-white.svg', 'index.md', 'index.html']"
28434c00263708398fcba9145c922561aff86cb2,[ZeRO-1] fix bug w. cpu-offload + > 1 GPU (#1841),['18ea8b7904d072efef83f14f368a187da81e84b3'],False,"['stage_1_and_2.py', 'test_zero.py']"
5fb4256a7ae9a34b7d79f968b2ba7cb26b5dce10,"fix: fix undefined variable in MoE top2gating (#1827)

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['b4e8f18c27b5c93c0c832a9851a37e8eeb70ffbd'],False,['sharded_moe.py']
1e61c7a860e7ac63cd4024a654b4f319063b9374,"fix: Fix undefined variable in _create_expert_data_and_model_parallel and make it easier to understand (#1826)

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['5fb4256a7ae9a34b7d79f968b2ba7cb26b5dce10'],False,"['layer.py', 'groups.py', 'test_groups.py']"
2e1847d6c810822546801d6131b5b3381e4acd46,"Add concurrency policy to CI workflow (#1844)

* added concurrency to github actions

* fixed problem where one workflow can cancel another workflow

* added tmp file to help test concurrency policy on CI

* removed tmp file to finish testing concurrency policy",['b84edef23feb7343d1eac5ea52e3a6c308cd5fb6'],False,"['amd.yml', 'formatting.yml', 'nv-lightning-v100.yml', 'nv-torch12-p40.yml', 'nv-torch18-v100.yml', 'nv-transformers-v100.yml']"
208d45bbf7cbde2abfb233e7d10803553fbcf126,fix dead MoQ link (#1855),['b61d7199c42523b8359374b07cedd81396878f4b'],False,['MoQ-tutorial.md']
73b5d9833a113c47758fbe9f8538614ce34ea95e,"Fix setup.py crash when torch is not installed. (#1866)

Co-authored-by: Blaine Rogers <blaine.rogers@five.ai>",['398f06035aaa896afcb82f0e806225ca6854e061'],False,"['builder.py', 'fused_adam.py', 'fused_lamb.py', 'transformer.py']"
d6db6510526e05788c563d58d916734e9335f6ed,"Fix broken links (#1873)

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['b47e25bf95250a863edb2c466200c697e15178fd'],False,"['advanced-install.md', 'index.md']"
0149bd4fc21b267ab4f2525a9b11df10f9a83c84,[docs] fix dead links (#1877),['c13457b7565ceb7284595d6d4e540500995aaeee'],False,"['README.md', 'index.md']"
9bf1e9af3a3a958fc74b5d5d57e56b72559f5458,[docs] fix commonmarker security issue (#1892),['ea9c0575825fae1d191614c1794925dd23bb9c55'],False,['Gemfile.lock']
56c522386878df2513ccb548de76b50191ae3af7,"bf16+pipeline parallelism (#1801)

* bf16 updates

* Got bf16 working

* fp32 reduction; flattened tensors

* bf16+zero_stage_1 first cut

* finish zero_stage 1 sharding

* Matching fp16 with debugging codes

* Matching loss with fp16

* Fix gradient clipping

* bf16 gradient clipping fix
bf16 checkpoint save/load

* Unscale grad norm

* Fix grad norm scaling

* Enable loading fp16_zero_1 into bf16_zero_1 engine and vice versa

* Fix clip_grad key error

* Reduce tied weight gradients

* Fix grad norm for moe

* Reduce specified gradients

* Use O(n) instead of O(n^2)

* Remove optimizer restriction for bf16

* Link bf16 & fp32 params

* Clip gradients of last stage tied weights

* Simplify tied weights reduction logic

* Also clip all tp rank parameters

* lp to hp mapping

* Link lp/hp/optim state; Refresh links after checkpoint load

* Remove debug print

* Remove debug print

* Simplify zero_grad logic

* fp32 accessors

* Fix update bug

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['9bf1e9af3a3a958fc74b5d5d57e56b72559f5458'],False,"['.gitignore', 'constants.py', 'bf16_optimizer.py', 'config.py', 'constants.py', 'engine.py', 'fused_optimizer.py', 'engine.py', 'module.py', 'utils.py', 'stage3.py', 'stage_1_and_2.py', 'utils.py']"
801c1723450320ca589cc9b6620b9ee5618a5f38,"fix file ordering (#1822)

Co-authored-by: Shuai Zheng <shzheng@amazon.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['56c522386878df2513ccb548de76b50191ae3af7'],False,"['zero_to_fp32.py', 'test_get_optim_files.py']"
ef17c89570ceae5b26a5f886e9d8cd0941afc0ac,"Fix multiple zero 3 tracing errors (#1901)

* Fix zero3 tracing issues

* Remove debug prints

* Code clarity",['fee73135980e78f8be7e1a3ff556751623ef6aaa'],False,"['engine.py', 'partitioned_param_coordinator.py', 'stage3.py']"
4575b2b792f6424d68677f9b896b55be9241aeac,"fix launcher for reading env vars (#1907)

Co-authored-by: Shuai Zheng <shzheng@amazon.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['8cc8c003cbeb0a4c95c705de1a24425fe8868820'],False,['multinode_runner.py']
32d97976ce08f00228ceb55a2d93005bd8b2769a,"Fix OOM and type mismatch (#1884)

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['4575b2b792f6424d68677f9b896b55be9241aeac'],False,"['partitioned_param_swapper.py', 'partition_parameters.py', 'partitioned_param_coordinator.py', 'stage3.py']"
a52cbf80f706728f5bfd274c7d8b55f60044906a,[zero-3] add bwd support for list/dict types returned in fwd (#1857),['b4fcd98ff096cd367dcfeb3c1d28a15d34447c66'],False,"['stage3.py', 'test_zero.py']"
a3b90030fd2bea119ea0d4b521057fd84a48705f,Fix time error (#1934),['a8d26d6ab5415011b057203569f259b238f7cba3'],False,['engine.py']
673cb60808a23f1f925bc445047720df3f0ff689,"Improve z3 trace management (#1916)

* Fix OOM and type mismatch

* Toggle prefetching

* Disable z3 prefetching for inference (temp workaround)

* Fix zero3 tracing issues

* Remove debug prints

* Enable prefetch for inference

* Code clarity

* Invalidate trace cache

* Trace cache invalidation when needed
Separate nvme prefetch from all-gather prefetch

* Track last used step id

* Use debug name in error message

* Construct param trace from module trace

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['a3b90030fd2bea119ea0d4b521057fd84a48705f'],False,"['partition_parameters.py', 'partitioned_param_coordinator.py', 'stage3.py']"
73c0798bd725cc275ee59cf7b936ff3597fc46c8,"GatheredParameters - accept a tuple of params (#1941)

* GatheredParameters - accept any iterable

* torch tensor is an iterable, so can't use collections.abc.Iterable

* fix",['ff908ed79361b87eea9614db8b7a1df1140fab51'],False,['partition_parameters.py']
de88718501ee25d934b1f33c309d1e4437c6ef53,"fix step in adam (#1823)

* fix step in adam

* fix backward compatibility and add unittest

* add unittest

* fix unbounded error when there are more than 1 param groups

* fix typo

* remove trailing whitespace

* fix end of file

Co-authored-by: Shuai Zheng <shzheng@amazon.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['ae43ba1270af086a42635835068056909e251185'],False,"['fused_adam.py', 'test_zero.py']"
50893458d66f27289934dfcc8fd0d1e02a8dcbd7,Fairseq support (#1915),['dbeadf16b5d55d04fe21a9d25bae844ab55fcb70'],False,"['__init__.py', '__init__.py', 'bf16_optimizer.py', 'engine.py', 'fused_optimizer.py', 'unfused_optimizer.py', 'engine.py', 'linear.py', 'partition_parameters.py', 'stage3.py', 'stage_1_and_2.py', 'test_zero_tiled.py']"
5053217e5dc31cf1f3fb9f75faa68604c984d748,"trivial fix (#1954)

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['09f1ad5e58f122363573765387f7f07a7d50c9d8'],False,"['partition_parameters.py', 'stage_1_and_2.py']"
44085856a8da65b8db4cec548fff55f95aa6311a,Add loss scale guard to avoid inf loop (#1958),['a5adb90d7209aa2a3ccafada2c8471ad0919e474'],False,['loss_scaler.py']
0d3689328154ab9452b07bbfe4a444d7374a749b,"Fix timer typo (#1964)

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['e329e9858fc35c4f685495f3bff87f9e799adba3'],False,['timer.py']
b8ff4825aae4bced15a29a4298cb3e59098df999,"Fix: Sparse tensors not updating (#1914)

* Fix do not updated sparse grads

* Remove call .data for sparse grads

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['5208eb73da5269034ded69c4dd7c4bff81df81e7'],False,['engine.py']
8164ea9e6d35dfbbe62b0f15448daf05268b9e81,"Fixing several bugs in the inference-api and the kernels (#1951)

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['b8ff4825aae4bced15a29a4298cb3e59098df999'],False,"['amd.yml', 'nv-torch12-p40.yml', 'nv-torch18-v100.yml', 'apply_rotary_pos_emb.cu', 'gelu.cu', 'pt_binding.cpp', 'transform.cu', 'custom_cuda_layers.h', 'replace_module.py', 'replace_policy.py', 'transformer_inference.py', 'transformer_inference.py', 'requirements-dev.txt', 'test_inference.py']"
0ebd81dfa981621fefbe4875b4d1db98d8e274c6,small fix for the HF Bert models (#1984),['3da841853ca07abf3a09e7bd325a576c4e642c11'],False,"['pt_binding.cpp', 'transformer_inference.py']"
5d3c67189b18bb0359c44ad18584eda895a3cb36,"Add unit test for various model families and inference tasks (#1981)

* added unit test for various HF model families and tasks

* formatting

* added missing import

* fixed broken pytest global vars

* modified test to conform to other test structure

* removed gpt-j. it cannot run on V100s (OOM)

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['0ebd81dfa981621fefbe4875b4d1db98d8e274c6'],False,['test_inference.py']
b6f2a5602bc0e07dd8c99e6afd5a9e49784c7905,added temp fix for our CI jobs (#1988),['5d3c67189b18bb0359c44ad18584eda895a3cb36'],False,['nv-lightning-v100.yml']
6719b46bd8792755cfc75eaa448bf7fa65fd99eb,fix typo when getting kernel dim in conv calculation (#1989),['b6f2a5602bc0e07dd8c99e6afd5a9e49784c7905'],False,['profiler.py']
7fc3065074885b0a2ec59c9c49e04296dd0e0fcd,"Add torch-latest and torch-nightly CI workflows (#1990)

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['6719b46bd8792755cfc75eaa448bf7fa65fd99eb'],False,"['nv-lightning-v100.yml', 'nv-torch-latest-v100.yml', 'nv-torch-nightly-v100.yml', 'nv-torch12-p40.yml', 'nv-torch18-v100.yml', 'nv-transformers-v100.yml', 'setup.py', 'util.py']"
3678ee177886bd815c6f77e02416abaac629f76a,"[bug] Add user-defined launcher args for MPI launcher (#1933)

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['7fc3065074885b0a2ec59c9c49e04296dd0e0fcd'],False,['multinode_runner.py']
d0eae5ad7a163f23592f8a316431e4d8b07bc47a,"Propagate max errorcode to deepspeed when using PDSH launcher (#1994)

Add '-S' argument to pdsh command to return the largest error code from the ssh sessions",['3678ee177886bd815c6f77e02416abaac629f76a'],False,['multinode_runner.py']
36ad3119d5420c6b25b456626a40f4ded8c05a81,"DeepSpeed comm backend v1 (#1985)

Co-authored-by: Quentin Anthony <qganthony@yahoo.com>
Co-authored-by: Ammar Ahmad Awan <ammar.awan@microsoft.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['828ab7185affe51a7c9637cffdd751d83b9dd4ae'],False,"['.pre-commit-config.yaml', '__init__.py', 'scheduler.py', '__init__.py', 'backend.py', 'comm.py', 'constants.py', 'torch.py', 'utils.py', 'engine.py', 'launch.py', 'replace_module.py', 'layer.py', 'sharded_moe.py', 'moe_inference.py', 'transformer_inference.py', 'checkpointing.py', 'bf16_optimizer.py', 'coalesced_collectives.py', 'nccl.py', 'config.py', 'engine.py', 'fused_optimizer.py', 'adam.py', 'lamb.py', 'zoadam.py', 'unfused_optimizer.py', 'engine.py', 'module.py', 'p2p.py', 'topology.py', 'async_swapper.py', 'optimizer_utils.py', 'partitioned_optimizer_swapper.py', 'partitioned_param_swapper.py', 'pipelined_optimizer_swapper.py', 'utils.py', 'utils.py', 'contiguous_memory_allocator.py', 'linear.py', 'partition_parameters.py', 'partitioned_param_coordinator.py', 'stage3.py', 'stage_1_and_2.py', 'utils.py', '__init__.py', 'debug.py', 'distributed.py', 'groups.py', 'logging.py', 'timer.py', 'check-torchdist.py', 'test_mpi_backend.py', 'test_mpi_perf.py', 'test_nccl_backend.py', 'test_nccl_perf.py', 'test_model.py', 'common.py', 'modeling.py', 'modelingpreln.py', 'simple_model.py', 'test_activation_checkpointing.py', 'test_aio.py', 'test_checkpointing.py', 'test_coalesced_collectives.py', 'test_config.py', 'test_configurable_parallel.py', 'test_curriculum_learning.py', 'test_dist.py', 'test_fp16.py', 'test_moe.py', 'test_onebit.py', 'test_partition.py', 'test_pipe.py', 'test_pipe_module.py', 'test_runtime_utils.py', 'test_sparse_grads.py', 'test_topology.py', 'test_zero.py', 'test_zero_context.py']"
25b2fc29fb6f74ce2f5420d431f63b680e5e71c2,"Relax assertion to allow Megatron-DeepSpeed MoE to use ZeRO-1 (#2007)

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['36ad3119d5420c6b25b456626a40f4ded8c05a81'],False,['stage_1_and_2.py']
e6f444aee2589b6d97250888978fc6e5dbeb70e9,[CI] force upgrade HF dependencies & output py env (#2015),['117c9cdf2522179b9750d8a76c27a896c896e8fe'],False,"['amd.yml', 'nv-lightning-v100.yml', 'nv-torch-latest-v100.yml', 'nv-torch-nightly-v100.yml', 'nv-torch12-p40.yml', 'nv-torch18-v100.yml', 'nv-transformers-v100.yml']"
b666d5cd73b25e88afe9591f4e9c010e7219df44,"[inference] test suite for ds-kernels (bert, roberta, gpt2, gpt-neo, gpt-j) (#1992)

Co-authored-by: Reza Yazdani <reyazda@microsoft.com>
Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>
Co-authored-by: Reza Yazdani <44502768+RezaYazdaniAminabadi@users.noreply.github.com>",['e6f444aee2589b6d97250888978fc6e5dbeb70e9'],False,"['amd.yml', 'nv-inference.yml', 'nv-nightly.yml', 'nv-torch-latest-v100.yml', 'nv-torch-nightly-v100.yml', 'nv-torch18-v100.yml', 'nv-transformers-v100.yml', 'dsr', 'gelu.cu', 'pt_binding.cpp', 'custom_cuda_layers.h', 'engine.py', 'replace_module.py', 'transformer_inference.py', 'requirements-inf.txt', 'setup.py', 'pytest.ini', 'test_inference.py']"
5dce73fa6181c587d7fb8f89670209a017057e0d,"Fix transformer API for training-evaluation pipeline (#2018)

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['7c3344e2152d83af5dc2e8c46fa2226033188f54'],False,['transformer.py']
ae198e20f74276d67aca0c26496e821b38ded2aa,DataLoader Length Fix (#1718),['5dce73fa6181c587d7fb8f89670209a017057e0d'],False,['dataloader.py']
c87f6ee209e2fed141ac4e2e28d4169f159ea487,"DeepSpeed Monitor Module (Master) (#2013)

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['ae198e20f74276d67aca0c26496e821b38ded2aa'],False,"['__init__.py', 'config.py', 'constants.py', 'csv_monitor.py', 'monitor.py', 'tensorboard.py', 'utils.py', 'wandb.py', 'config.py', 'constants.py', 'engine.py', 'engine.py', '_config.yml', 'navigation.yml', 'config-json.md', 'features.md', 'monitor.md', 'tensorboard_monitor.PNG', 'wandb_monitor.PNG', 'requirements.txt', 'test_monitor.py']"
735406e5365056a1c0c47d087b714e82160dcc99,"fix import errors (#2026)

Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>",['d86a2de993f69c97752e0730c308c0b920b9ed9d'],False,"['scheduler.py', '__init__.py', 'comm.py', 'torch.py', 'engine.py', 'layer.py', 'sharded_moe.py', 'utils.py', 'moe_inference.py', 'transformer_inference.py', 'checkpointing.py', 'bf16_optimizer.py', 'coalesced_collectives.py', 'nccl.py', 'config.py', 'engine.py', 'fused_optimizer.py', 'adam.py', 'lamb.py', 'zoadam.py', 'unfused_optimizer.py', 'engine.py', 'module.py', 'p2p.py', 'topology.py', 'async_swapper.py', 'optimizer_utils.py', 'partitioned_optimizer_swapper.py', 'partitioned_param_swapper.py', 'pipelined_optimizer_swapper.py', 'utils.py', 'utils.py', 'contiguous_memory_allocator.py', 'linear.py', 'partition_parameters.py', 'partitioned_param_coordinator.py', 'stage3.py', 'stage_1_and_2.py', 'utils.py', 'groups.py', 'logging.py', 'timer.py']"
ec1ec204c8f5dbe8c622a07c5ddb1d9e9d1a428b,"Fix inference unit test import error catching (#2024)

* fix to catch assert error for inference test imports

* fix wrong syntax

* changed to sequential inf tests

* fix for lm_eval import

* added environment check fixture

* added expected torch and cuda version

* check various version depth for cuda/torch

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['735406e5365056a1c0c47d087b714e82160dcc99'],False,"['nv-inference.yml', 'nv-nightly.yml', 'nv-torch12-p40.yml', 'nv-torch18-v100.yml', 'conftest.py', 'test_inference.py']"
2a1a4096448833484c1da4c27e13b92b5e5e8461,"Retain available params until last use (#2016)

* Retain prefetched params until last use

* Unit tests fixes",['ec1ec204c8f5dbe8c622a07c5ddb1d9e9d1a428b'],False,"['async_swapper.py', 'partitioned_param_swapper.py', 'partitioned_param_coordinator.py', 'test_zero.py']"
678c3fe3308e666a40baeea3f84be0e5d7aba7c4,"Split parameter offload from z3 (#2009)

* Split parameter offload from z3

* Format fixes

* Bug fixes

* Cleanup

* Remove dead code",['2a1a4096448833484c1da4c27e13b92b5e5e8461'],False,"['engine.py', 'parameter_offload.py', 'stage3.py']"
5218177922a4be5c14cf0db893dbfcb139179ba5,"fixed print statement (#2038)

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['678c3fe3308e666a40baeea3f84be0e5d7aba7c4'],False,['profiler.py']
a04480e192e00951c4d04010999825ec9e21de1c,"Fix the half-precision version of CPU-Adam (#2032)

* Fix the half-precision version of CPU-Adam

* remove unexpected return

* fix the increase width (fp32/fp16)

* support fp16 tests for cpu-adam

* fix the fp16 data-loading

* change unit-test for fp16 check & slight change to parameter size

* fix for numpy error

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>",['ff87c4e1f15d53abb0b95c0dd9fa331941b4e177'],False,"['cpu_adam.h', 'simd.h', 'test_cpu_adam.py']"
7bae53d154f44877d17c9c5efe40b144440272bf,"Fix for AMD unit tests (#2047)

* assert no FP16 with AMD CPUs

* add unit test for AMD assert error

* missing import

* downgrade assert to warning",['a04480e192e00951c4d04010999825ec9e21de1c'],False,"['amd.yml', 'cpu_adam.py', 'test_cpu_adam.py']"
76ea0534c1469ec10ad612f1ff5852c86fa880d0,"Fix missing import in replace_module.py (#2050)

* Fix missing import in replace_module.py

* Change import from torch.distributed to deepspeed.comm",['38a00bee9e2b2608ebeebde1db163b9eb64d52f9'],False,['replace_module.py']
559fb8e51525231f58f50ed1f49835ff5250def0,[docs] fix broken read-the-docs build (#2075),['9305916d6b858b20fca3ebe03940ceaaa13cdfbd'],False,"['conf.py', 'memory.rst', 'requirements-readthedocs.txt']"
b3388e1418e80141a902886dec39127b851a1f0a,"Fix partition id in the fp32->fp16 param copying step for z2+cpu-offload (#2059)

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['3540ce74d9006aaaccac82e6409ee452f6d0db59'],False,['stage_1_and_2.py']
50a652e7575cde3c6e6c288a8b6dce416a02757a,"Codeowner addendum and fix to small model debugging script (#2076)

* Add github username to CODEOWNERS

* add import torch.distributed to small model debugging test script

* Replace torch.dist with deepspeed.com",['b3388e1418e80141a902886dec39127b851a1f0a'],False,"['CODEOWNERS', 'test_model.py']"
b05237876e86ea67dcbb71b38cface7b6f928b87,"fixed ""None type has no len()"" (#2091)

Co-authored-by: Cheng Li <pistasable@gmail.com>",['db3252b06a6b4347dee8bbe1375b41b5f3dd5b90'],False,['timer.py']
aa88137b8d093f222f6c925d70b68024ae26ef0c,"Add Inference support for running the BigScience-BLOOM Architecture (#2083)

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>
Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>",['2feaf6d6aa131be00c52738ce94c769b8bb0c999'],False,"['gelu.cu', 'normalize.cu', 'pt_binding.cpp', 'softmax.cu', 'context.h', 'custom_cuda_layers.h', '__init__.py', 'engine.py', '__init__.py', 'layers.py', 'load_checkpoint.py', 'replace_module.py', 'replace_policy.py', 'transformer_inference.py', 'engine.py', 'state_dict_factory.py', '__init__.py', 'init_on_device.py', 'test_inference.py', 'test_init_on_device.py']"
0f4f2f982c3b55e61d2d7d0631e01050316c9aca,"Adding DeepSpeed Compression Composer  (#2105)

Co-authored-by: yaozhewei <zheweiy@berkeley.edu>
Co-authored-by: xiaoxiawu <yxiaoxiawu@microsoft.com>
Co-authored-by: Conglong Li <conglong.li@gmail.com>
Co-authored-by: Xiaoxia (Shirley) Wu <94406484+xiaoxiawu-microsoft@users.noreply.github.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>",['6b9df564df6460116745588788fa73c9a28e2cf0'],False,"['__init__.py', 'basic_layer.py', 'compress.py', 'config.py', 'constants.py', 'helper.py', 'scheduler.py', 'utils.py', 'config.py', 'constants.py', 'engine.py', 'quantize.py', '_config.yml', 'navigation.yml', 'config-json.md', 'model-compression.md', 'xtc-1.png', 'xtc-2.png', 'xtc-3.png', 'xtc-4.png', 'test_compression.py']"
69b7c970d262bdc248d87061ceb2c3431c6dbd78,"Remove hardcoded ROCm install path (#2093)

* fix hard-coded rocm install path

* added fix for newest torch+rocm install

* added backup for not detecting rocm at all",['0f4f2f982c3b55e61d2d7d0631e01050316c9aca'],False,['builder.py']
b4513f63107bb54bef694de2612260f72e8b0eaf,"fix softmax dim of Residual MoE in moe/layer.py (#2110)

Thanks a lot for finding this issue and fixed it :)
Co-authored-by: Zhewei Yao <zheweiyao@gmail.com>",['69b7c970d262bdc248d87061ceb2c3431c6dbd78'],False,['layer.py']
80d0a32f0bf2f3974a13e02ff21addabbcc37734,"Checkpoint reshaping (#1953)

* unit test, remove exception, add notes

* Move param_shapes to model files

* Remove hard-coded constants

* Conditioned to zero optimizer

* Add zero checkpoint merging

* Print checkpoint version

* Reshape zero_* ckpt files

* Merge zero* files contraction

* Utils for 3D contraction reshaping

* Remove bogus import

* Support bf16_zero ckpts

* Add param slice mappings

* Load universal checkpoints

* Per group mappings from Stas

* Hack to load bf16 zero files

* Param attributes

* WIP

* Fix api bug

* Update lp with local/remote hp

* Disable vocab padding handling

* Update z2 checkpoint

* Remove debug prints

* Remove debug prints; Rebase unit test

* Add reshape assert

* Padding

* Typo

* Catch nonexistent checkpoint path

* Cleanup

* Restore checkpoint state comparisons

* Add torch version guards

* More precise avoidance of false positives.

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['46fffc0bbdac603a903cf4749f1624603c396e1b'],False,"['__init__.py', 'constants.py', 'deepspeed_checkpoint.py', 'reshape_3d_utils.py', 'reshape_meg_2d.py', 'reshape_utils.py', 'utils.py', 'zero_checkpoint.py', 'bf16_optimizer.py', 'config.py', 'constants.py', 'engine.py', 'stage3.py', 'stage_1_and_2.py', 'test_checkpointing.py', 'test_reshape_checkpoint.py', 'util.py']"
9f5895cb7a7db8248295038543d75a4334ee5c44,"fix issue in accelerate. (#2121)

Co-authored-by: Ammar Ahmad Awan <ammar.awan@microsoft.com>",['80d0a32f0bf2f3974a13e02ff21addabbcc37734'],False,['torch.py']
ed02d38d20ca2cbfb2f3e7f694ea87837cc50554,[docs] link fixes (#2124),['a2506b545ab4f9af2e940eb0ab355397ac76c250'],False,"['README.md', 'index.md']"
316c4a43e0802a979951ee17f735daf77ea9780f,Add flake8 to pre-commit checks (#2051),['9c6cdecc1f76d4be8437ab01774a64db9e80ec30'],False,"['.pre-commit-config.yaml', 'all_gather.py', 'all_reduce.py', 'all_to_all.py', 'broadcast.py', 'constants.py', 'pt2pt.py', 'run_all.py', 'ds_bench', 'aio_bench_perf_sweep.py', 'parse_aio_stats.py', 'test_ds_aio.py', 'test_ds_aio_utils.py', 'validate_async_io.py', 'autotuner.py', 'scheduler.py', 'base_tuner.py', 'cost_model.py', 'index_based_tuner.py', 'model_based_tuner.py', 'utils.py', 'comm.py', 'config.py', 'torch.py', 'utils.py', 'basic_layer.py', 'compress.py', 'elasticity.py', 'env_report.py', 'git_version_info.py', 'engine.py', 'multinode_runner.py', 'layers.py', 'load_checkpoint.py', 'module_quantize.py', 'replace_module.py', 'replace_policy.py', 'layer.py', 'sharded_moe.py', 'utils.py', 'config.py', 'csv_monitor.py', 'utils.py', 'cpu_adagrad.py', 'cpu_adam.py', 'fused_adam.py', 'multi_tensor_apply.py', 'quantizer.py', 'matmul.py', 'softmax.py', 'sparse_attention_utils.py', 'sparse_self_attention.py', 'moe_inference.py', 'transformer_inference.py', 'transformer.py', 'profiler.py', 'bf16_optimizer.py', 'coalesced_collectives.py', 'nccl.py', 'engine.py', 'fused_optimizer.py', 'adam.py', 'lamb.py', 'zoadam.py', 'lr_schedules.py', 'engine.py', 'module.py', 'topology.py', 'quantize.py', 'optimizer_utils.py', 'partitioned_optimizer_swapper.py', 'partitioned_param_swapper.py', 'pipelined_optimizer_swapper.py', 'utils.py', 'utils.py', 'weight_quantizer.py', 'offload_config.py', 'partition_parameters.py', 'partitioned_param_coordinator.py', 'stage3.py', 'comms_logging.py', 'groups.py', 'timer.py', 'zero_to_fp32.py', 'conf.py', 'builder.py', 'sparse_attn.py', 'check-torchdist.py', 'setup.py', 'flatten_bench.py', 'unflatten_bench.py', 'BingBertSquad_run_func_test.py', 'BingBertSquad_test_common.py', 'test_e2e_squad.py', 'run_checkpoint_test.py', 'run_func_test.py', 'run_perf_baseline.py', 'run_perf_test.py', 'test_common.py', 'run_sanity_check.py', 'test_mpi_backend.py', 'test_mpi_perf.py', 'test_nccl_backend.py', 'test_nccl_perf.py', 'test.py', 'common.py', 'megatron_model.py', 'modeling.py', 'modelingpreln.py', 'multi_output_model.py', 'test_autocast.py', 'test_autotuning.py', 'test_bf16.py', 'test_checkpointing.py', 'test_coalesced_collectives.py', 'test_compression.py', 'test_configurable_parallel.py', 'test_cpu_adam.py', 'test_cuda_backward.py', 'test_cuda_forward.py', 'test_curriculum_learning.py', 'test_dynamic_loss_scale.py', 'test_elastic.py', 'test_flops_profiler.py', 'test_fp16.py', 'test_get_optim_files.py', 'test_ignore_unused_parameters.py', 'test_inference.py', 'test_lr_schedulers.py', 'test_moe.py', 'test_monitor.py', 'test_multi_output_model.py', 'test_onebit.py', 'test_pipe.py', 'test_pipe_module.py', 'test_pld.py', 'test_reshape_checkpoint.py', 'test_runtime_utils.py', 'test_sparse_attention.py', 'test_sparse_grads.py', 'test_zero.py', 'test_zero_tiled.py']"
31582d77287db43b0e08702baf84e9f696096e65,"Fix conflict between Tutel and top-2 gate in MoE layer (#2053)

* fix conflit between tutel and top-2 gate
Co-authored-by: Ammar Ahmad Awan <ammar.awan@microsoft.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['0e49b197ad42987a20858260f0f5b6a5726d0b18'],False,['sharded_moe.py']
ddd2113d213d554648c4a5a1c779b9236c9c4ff0,turn off time check for now (#2142),['d9ec8ef49ad62db0a95bfe88839b0abf8d9ce5a9'],False,['test_inference.py']
76a01d5d0b06cda022ffd4bead9e172a1cf3f5ca,[docs] README formatting fix,['56a0d186bbe5bab769c8cddebf1cb95869644a62'],False,['README.md']
6f7137cb2390ca6de0a0bf2430fbb84e4070af12,fix broken links (#2144),['76a01d5d0b06cda022ffd4bead9e172a1cf3f5ca'],False,['README.md']
59975896830e8e51c296d072e857a35a71683185,"Refactor ZeRO configs to use Pydantic (#2004)

* first pass at pydanticifying Zero Configs

* added pydantic to reqs

* fixed bug with deprecated values not being type-checked

* fixing zero config bugs from unit tests

* fixed access of Config values

* removing zero constants

* formatting/fix broken import

* fixed bad merge

* fixed issue with missing aliased field

* fix for failing tests

* fix how deprecated fields are processed

* only process dep params when they are set

* fix mistyped field name

* fixes, docs, removed more constants

* fix merge

* more fixes after merge w master

* added unit tests

* formatting

* added fix for transformers unit tests

* separated offload config from zero config

* fixed bad import

* formatting and flake fixes

* implement suggestion from review

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['6f7137cb2390ca6de0a0bf2430fbb84e4070af12'],False,"['autotuner.py', 'config.py', 'config_utils.py', 'engine.py', 'optimizer_utils.py', 'partitioned_param_swapper.py', 'pipelined_optimizer_swapper.py', 'config.py', 'constants.py', 'offload_config.py', 'offload_constants.py', 'parameter_offload.py', 'partition_parameters.py', 'partitioned_param_coordinator.py', 'stage3.py', 'stage_1_and_2.py', 'test_config.py', 'test_ds_config.py', 'test_zero_config.py']"
be46ff6db207ab0d66baaed6a938a48838102d77,"Add purely-local sliding window sparse attention config (#1962)

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['59975896830e8e51c296d072e857a35a71683185'],False,"['__init__.py', 'sparsity_config.py', 'test_sparse_attention.py']"
b442264dc935afc1b768e9c3abcc448695de2e77,formatting fix for #1962,['be46ff6db207ab0d66baaed6a938a48838102d77'],False,['test_sparse_attention.py']
e669aaf55b166a5adc3e33aa92c6d81662fdf908,"Trajepl/nebula ckpt engine (#2085)

* enable checkpoint engine

* seprated nebula config

* add __init__.py for nebula importing

* linter fix

* fix: ds_config is None

* fix: ds config

* fix: get sd loader fix

* align the API with torch raw code

* linter fix

* remove duplicate tag params

* make checkpoint_engine as required args

* fix args

* extract parameters out to config

* fix: load state dict

* separate load engine

* linter fix

* extract checkpoint engine to abstract calss

* linter fix

* construct function args fix

* add docs for dev/customers

* linter fix

* remove load engine

* print->log_dist

* linter fix

* add tag flag to distinguish the loading order

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['a54661a06f71e85bc9320066ca8e2be0cda4f779'],False,"['engine.py', 'launch.py', 'runner.py', '__init__.py', 'config.py', 'constants.py', 'README.md', '__init__.py', 'checkpoint_engine.py', 'nebula_checkpoint_engine.py', 'torch_checkpoint_engine.py', 'config.py', 'engine.py', 'engine.py', 'module.py', 'state_dict_factory.py', 'test_checkpointing.py']"
66d29b0a6c896e8764ff90ba074604e776c9e589,"Graceful exit on failures for multi-node runs (#2008)

* Use Popen.terminate() to stop the child processes gracefully; Kill them if terminate doesn't work

* The Popen.kill() command cause the training processes to end abruptly. This may cause the child processes to become zombies without communicating properly to the parent process about the kill signal. So the ssh session continue to wait for signals from the child processes, causing it to not return back to the pdsh command

Fixes microsoft#1995

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['e669aaf55b166a5adc3e33aa92c6d81662fdf908'],False,['launch.py']
57140e8e951e8e149e998505d6b7ba6adeeaaede,"fix: fix BF16_Optimizer compatibility issue with optimizer state 0-dim tensor (#2152)

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['66d29b0a6c896e8764ff90ba074604e776c9e589'],False,['bf16_optimizer.py']
556f0051521ec539026977e91c0e4b42445340ea,"Fix random token-generation issue + MP-checkpoint loading/saving (#2132)

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['57140e8e951e8e149e998505d6b7ba6adeeaaede'],False,"['__init__.py', 'engine.py', 'load_checkpoint.py', 'replace_module.py', 'replace_policy.py', 'transformer_inference.py', 'state_dict_factory.py']"
1ed5aa96a8fc41a68f7be3741fc846b861e11a52,"Elastic Training support in DeepSpeed (#2153) (#2156)

Co-authored-by: Arpan Jain <t-arpanjain@microsoft.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['ba67bd9a141f0277d451b876b63862c34e30bfea'],False,"['.pre-commit-config.yaml', 'constants.py', '__init__.py', 'config.py', 'constants.py', 'elastic_agent.py', 'elasticity.py', 'utils.py', 'constants.py', 'launch.py', 'multinode_runner.py', 'runner.py', 'config.py', 'engine.py', 'engine.py', 'config-json.md', 'test_elastic.py']"
46401b3884d0a745a10db2a0de7ab7665cf906c7,[zero-3] shutdown zero.Init from within ds.init (#2150),['63f470eeb61cf7a5b3cd0b20056ff5f83393e33a'],False,"['__init__.py', 'partition_parameters.py', 'transformer_inference.py', 'test_zero_context.py']"
2210ebe70f68135b6b43e91323a7d96a403a2299,"Release swap buffers for persisted params (#2089)

* Split parameter offload from z3

* Format fixes

* Bug fixes

* Cleanup

* Remove dead code

* Release swap buffers for persisted params

* Format fixes

* Format fixes

* Pass args correctly

* Use pinned memory for nvme offload

* Merge with masster

* Fix missing import

* model pesistence params

* Fix merge issues

* Handle none device

* Usse log_dist",['a039e2261a462ce0d8c34c57c3a0927c8ff8ff5e'],False,"['engine.py', 'config.py', 'parameter_offload.py', 'partition_parameters.py', 'partitioned_param_coordinator.py', 'stage3.py']"
5fe9d61065b00473ad08e155f4ad7e1a4dc7e482,"Tensor parallelism for Mixture of Experts (#2074)

* tensor parallelism for mixture of experts
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Ammar Ahmad Awan <ammar.awan@microsoft.com>",['2210ebe70f68135b6b43e91323a7d96a403a2299'],False,"['layer.py', 'mappings.py', 'sharded_moe.py', 'groups.py', 'test_moe_tp.py']"
1a71e77dc2ab93b9b0b55539da029f583ddfa854,"Fix for distributed tests on pytorch>=1.12 (#2141)

Fix for distributed tests
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['b005db86fc31a2edae378f634c937fa22ef24bea'],False,"['nv-inference.yml', 'nv-torch-latest-v100.yml', 'nv-transformers-v100.yml', 'requirements-dev.txt', '__init__.py', 'conftest.py', 'test_dist.py', 'common.py', 'test_inference.py', 'test_monitor.py', 'test_cpu_adagrad.py', 'test_adamw.py', 'test_cpu_adam.py', 'test_flops_profiler.py', 'test_topology.py', 'test_dist.py']"
6bfcf3c694b9ad170251caf90bf8ced96d0056f9,"Fix wrong unit of latency in flops-profiler (#2090) (#2095)

Co-authored-by: Cheng Li <pistasable@gmail.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['776e36988dde2874a79a6dd269d5a3ccbafebf3f'],False,['profiler.py']
d1cd18e5fbb280229d9fe447ca62e01187208286,"Update for AMD CI workflow (#2172)

re-enable AMD CI with some modifications",['bb49dc73f548cfd59aa3666e9a146b4f92124ba4'],False,"['amd.yml', 'test_cpu_adagrad.py', 'test_cpu_adam.py']"
e419f7cbcda6339f712e1657cd55ed0800ec6171,"Match compute and reduce dtype (#2145)

* Match compute and reduce dtype

* Unit tests

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['e7d995954056e0b1437a50fa7ea1dc439bc3d46a'],False,"['stage3.py', 'stage_1_and_2.py', 'test_bf16.py']"
ee5ce524606dfd8c6e5acf65f2bb38fa988e4a88,fix missing import (#2175),['80b5b9259b070e6192a3e817044ade7e3d6a9cc3'],False,['autotuner.py']
0f5c2012ce19c36936be562094ef3d73b230e364,"Update README.md (#2192)

Fix typos.",['1223b13c9add8d9806633689634f8c9728db4739'],False,['README.md']
54a9e1b924da468905b69b3bada6034943dd2f2f,Fix the layer-past for GPT based models (#2196),['0f5c2012ce19c36936be562094ef3d73b230e364'],False,['transformer_inference.py']
b62e0cc5a8dc4323a8c28e2f66ef5e06f67ecf80,"Add gradient_average flag support for sparse grads (#2188)

* Add gradient_average flag support for sparse grads

* formatting fixes

* Add tests

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['54a9e1b924da468905b69b3bada6034943dd2f2f'],False,"['engine.py', 'test_averaging.py']"
8920308c669c4b3a47807de1ef74651cf31b52f1,"Fix the tensor-slicing copy for qkv parameters (#2198)

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['28dfca8a13313b570e1ad145cf14476d8d5d8e16'],False,['replace_module.py']
ac9951985f2647f905d76063d1b292e6dc4a117b,"Refactor Distributed Tests (#2180)

Refactor Distributed unit tests",['8920308c669c4b3a47807de1ef74651cf31b52f1'],False,"['amd.yml', 'nv-torch-latest-v100.yml', 'alexnet_model.py', 'test_autotuning.py', 'test_reshape_checkpoint.py', 'test_dist.py', 'common.py', 'test_compression.py', 'test_elastic.py', 'test_run.py', 'test_cpu_adagrad.py', 'test_cpu_adam.py', 'test_sparse_attention.py', 'test_pipe_module.py', 'test_onebit.py', 'test_pipe.py', 'test_pipe_schedule.py', 'test_csr.py', 'test_autocast.py', 'test_bf16.py', 'test_data.py', 'test_ds_config.py', 'test_pld.py', 'test_runtime_utils.py', 'test_zero_config.py', 'test_zero_tiled.py', 'test_bf16.py', 'test_groups.py', 'test_onebit.py', 'test_sparse_grads.py', 'test_get_optim_files.py', 'test_groups.py', 'test_init_on_device.py']"
87b201330f90d4102c93e7a5a6b2e34f28a784cd,"fix table syntax (#2204)

Co-authored-by: Conglong Li <conglong.li@gmail.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['ac9951985f2647f905d76063d1b292e6dc4a117b'],False,['model-compression.md']
8b2a63717a08801d9bab55b617a3e2ef892679dc,"Add support of OPT models (#2205)

* add opt replace policy

* simplify inf. api

* fix opt replace policy

* fix use-cash & add relu

* Add support of custom MLP act. function

* Revert ""simplify inf. api""

This reverts commit 9e910fcbd5471dec9b3c92008426f5ba590bf0b6.

* fix the inference API (temp. solution)

* fix code formatting

* add unit tests for OPT models.

* refactor pre-attention layer norm configuration

* add support of opt-350m model

* refactor the HF model config initialization

* fix hf model config issue

Co-authored-by: Reza Yazdani <reyazda@microsoft.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>
Co-authored-by: Reza Yazdani <44502768+RezaYazdaniAminabadi@users.noreply.github.com>",['b5ac0d542d253901e997158540478b5b672030b7'],False,"['pt_binding.cpp', 'relu.cu', 'custom_cuda_layers.h', 'engine.py', 'replace_module.py', 'replace_policy.py', 'transformer_inference.py', 'types.py', 'transformer_inference.py', 'test_inference.py']"
9b418c1e1c3f115be41a4098bc30969d988e1ae7,"fix typos in readme. (#2218)

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['8b2a63717a08801d9bab55b617a3e2ef892679dc'],False,['README.md']
7d8ad45d6ac730c56309a917fa667f76eea0b27d,Fix regression w. dist_init_required (#2225),['9b418c1e1c3f115be41a4098bc30969d988e1ae7'],False,"['.pre-commit-config.yaml', 'comm.py', 'test_dist.py', 'common.py']"
fda63432ba67643d2f54885e45f0b2058fa2b937,"Remove the random-generator from context during inference (#2228)

* Fix the tensor-slicing copy for qkv parameters

* remove the random-generator from context during inference

* formatting

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['5e42cc8be22f58ef6be022fcc6d69fe11c7bfe25'],False,"['pt_binding.cpp', 'context.h']"
dce3acaac7cca9509cf8092cdd7657267d09fbb9,allow saving ckpt w/o ckpt json + bloom copy fix (#2237),['fda63432ba67643d2f54885e45f0b2058fa2b937'],False,['replace_module.py']
217338beb6efd57e260d7cdb07a38c520289ffae,"Refactor dist tests: Checkpointing (#2202)

Refactor distributed tests: checkpointing

Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>",['86164c487e6266c2528a615835a6020c57f7bb22'],False,"['engine.py', 'common.py', 'test_latest_checkpoint.py', 'test_lr_scheduler.py', 'test_moe.py', 'test_other_optimizer.py', 'test_pipeline.py', 'test_sparse.py', 'test_tag_validation.py', 'test_zero_optimizer.py']"
c35bfe89f699500518258bd75499643a00d09281,"fix ds-inference without policy (#2247)

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['fae896ef600465a0ea9ea6b10d7710709c48ca27'],False,['replace_module.py']
80f94c10c552ec79473775adb8902b210656ed76,"fix #2240: wrong time unit in flops_profiler (#2241)

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['2a64448830375528009d2d8c81e8a40d7e09396d'],False,"['sharded_moe.py', 'engine.py']"
a7ee688a6f00d5fc97acc6db523ce6c6996aaf10,"Update replace_module.py, test-gptj.py related fix (#2269)

Fix RuntimeError: Boolean value of Tensor with more than one value is ambiguous when running test-gptj.py",['55b7b9e008943b8b93d4903d90b255313bb9d82c'],False,['replace_module.py']
4671cce558f80b11b00ff4e0d2c3d3d6005ee5d5,"Fix OrderedDict import for python3.6 (#2267)

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['a7ee688a6f00d5fc97acc6db523ce6c6996aaf10'],False,['bf16_optimizer.py']
d154cc0f55a78edaac9faa967aa17666790efd07,Ds inference/fix mp2 (#2270),['4671cce558f80b11b00ff4e0d2c3d3d6005ee5d5'],False,['gelu.cu']
aca34a9b5b8a2c946a96248f04b0cf7e13755a3f,"Trajepl: nebula load fix (#2182)

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: chenguo <chenguo@microsoft.com>",['d154cc0f55a78edaac9faa967aa17666790efd07'],False,['nebula_checkpoint_engine.py']
91eba15a01de2ae7f42804249240dedb0e892a5e,"add a new unit test for cuda ops (#2278)

Co-authored-by: cmikeh2 <connorholmes@microsoft.com>",['afdc72879f66c419ee02d32decf07185506bff85'],False,"['nv-inference.yml', 'test_bias_gelu.py']"
c84bca37b18f0d6581b1fe6dbf3f41aefe404fe6,"Memory Access Utility (#2276)

Co-authored-by: Ammar Ahmad Awan <ammar.awan@microsoft.com>",['5505e2473e81b875e30fd5b57fc7e22d3940b5e4'],False,"['memory_access_utils.h', 'apply_rotary_pos_emb.cu', 'dequantize.cu', 'gelu.cu', 'normalize.cu', 'pt_binding.cpp', 'relu.cu', 'softmax.cu', 'transform.cu', 'inference_context.h', 'inference_cublas_wrappers.h', 'inference_cuda_layers.h', 'transformer_inference.py', 'test_bias_gelu.py']"
47e030f54da26f91304a1f78134535b3d1fb905d,"Fp32 accuracy bug fix (#2285)

Co-authored-by: Arash Bakhtiari <arash@bakhtiari.org>
Co-authored-by: Arash Bakhtiari <arashb@users.noreply.github.com>",['c84bca37b18f0d6581b1fe6dbf3f41aefe404fe6'],False,"['pt_binding.cpp', 'softmax.cu']"
b146aa3523526e2bddb3e783824085d003d71e2d,"[ds-inference] fix progress bar (#2286)

when loading the non-sharded checkpoint update the progress bar (fix by @RezaYazdaniAminabadi) - I've just tested it to work.

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['53182531ed7e33d980ded64fad2fb209d9919672'],False,['replace_module.py']
5102c4aba9f9ee27cfaf2a2c260de3b788518589,fused bias relu unittest (#2297),['c8641032107dbaf97594ccce61b00736ec50a35a'],False,['test_bias_relu.py']
7e085b62584bb54ead49a91a178ac5916614ac3e,fix for pytest picking up local deepspeed dir instead of installed deepspeed (#2299),['5102c4aba9f9ee27cfaf2a2c260de3b788518589'],False,"['__init__.py', 'test_autotuning.py', 'common.py', 'test_latest_checkpoint.py', 'test_lr_scheduler.py', 'test_moe.py', 'test_other_optimizer.py', 'test_pipeline.py', 'test_sparse.py', 'test_tag_validation.py', 'test_zero_optimizer.py', 'test_dist.py', 'test_compression.py', 'test_elastic.py', 'test_inference.py', 'test_monitor.py', 'test_adamw.py', 'test_pipe_module.py', 'test_flops_profiler.py', 'test_onebit.py', 'test_pipe.py', 'test_topology.py', 'test_bf16.py', 'test_data.py', 'test_pld.py', 'test_runtime_utils.py', 'test_sparse_grads.py', 'test_init_on_device.py']"
4524b8dbc663719ceefd2a377d2b49b45d288684,"Fix for Zero3 when MP>1 and at least one batch param undefined (#2289)

Co-authored-by: anthony.301 <anthony.301@mri.cluster>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['7e085b62584bb54ead49a91a178ac5916614ac3e'],False,['stage3.py']
b2d550ab850948458abacb167577603bd7b3ab5f,"Unit test for bias add kernel (#2298)

* added unit test

* Update pt_binding.cpp

* formatting

* Update test_bias_add.py",['4524b8dbc663719ceefd2a377d2b49b45d288684'],False,"['gelu.cu', 'pt_binding.cpp', 'test_bias_add.py']"
a691ec605b2383ecac21ecf33a9ba1c0cba4dd01,"Add tensor parallel inference unit tests (#2232)

Co-authored-by: Reza Yazdani <44502768+RezaYazdaniAminabadi@users.noreply.github.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>
Co-authored-by: Sam Ade Jacobs <samjacobs@microsoft.com>",['d0dfe38d53d6b64e1108d87d89371edd2fbe135f'],False,"['amd.yml', 'nv-accelerate-v100.yml', 'nv-inference.yml', 'nv-lightning-v100.yml', 'nv-nightly.yml', 'nv-torch-latest-v100.yml', 'nv-torch-nightly-v100.yml', 'nv-torch12-p40.yml', 'nv-torch18-v100.yml', 'nv-transformers-v100.yml', 'pytest.ini', 'test_inference.py']"
efa8aded4a3280c5df67b322970c34a05d71f153,Fix the residual add mp scaling for  GPTNeoX (#2310),['a691ec605b2383ecac21ecf33a9ba1c0cba4dd01'],False,['gelu.cu']
9d541a63dcfc51b15a8e7aa7314c9a92dc8ac892,Add unit tests for residual_add kernels (#2307),['efa8aded4a3280c5df67b322970c34a05d71f153'],False,['test_residual_add.py']
276eec7beb10dd7534a5755c8ca91e31e61a4919,"ZeRO-Inference blog (#2271)

* ZeRO-Inference blog

* ZeRO-Inference blog

* Format fixes

* Apply feedback

* Feedback

* Update docs/_posts/2022-08-27-zero-inference.md

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>

* Update docs/_posts/2022-08-27-zero-inference.md

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>

* Address feedback

* Format fixes

* More tweaks

* long sequence, nvme offload

* Add image

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['18ee381eb344be9c504a09bd3b3a03c0199d595f'],False,"['2022-03-21-amd-support.md', '2022-07-26-deepspeed-azure.md', '2022-09-10-zero-inference.md', 'zero_inference_full_offload.png', 'zero_inference_model_scale.png', 'zero_inference_models.png', 'zero_inference_multi_gpu.png', 'zero_inference_prefetch.png', 'zero_inference_token_count_batch_size.png', 'zero_inference_token_count_cpu_throughput.png', 'zero_inference_token_count_nvme_throughput.png']"
95d1151733c8340159e2e610ea788c177e99b6b4,"add quant unit test (#2315)

* add quant unit test

* add codeowner

* format fix

* fix undefined symbol: curandSetPseudoRandomGeneratorSeed

* modify ref fn name and add comment

* add comments

* add 4bit quant 16groups

* fix

* modify groups in ref code

* parameterize tensor shape

* single param

* detach tensor

* remove -lcurand flag

* add back -lcurand flag

Co-authored-by: Ammar Ahmad Awan <ammar.awan@microsoft.com>",['c199edac8210e730acfd004c6e2bc3a98c0db903'],False,"['CODEOWNERS', 'quantizer.py', 'test_quant.py']"
cf638be99803682933cb4040850765d46832ee78,only override forward if using cuda-graph (#2291),['95d1151733c8340159e2e610ea788c177e99b6b4'],False,"['nv-inference.yml', 'engine.py', 'test_inference.py']"
80b10d0c6985fb1792a044678c3c5c306cc0922e,"MOE residual matmult unit test (#2323)

MOE residual matmul unit tests

Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>
Co-authored-by: Ammar Ahmad Awan <ammar.awan@microsoft.com>",['0f0a7a5d0bfc774179cad952a9dcfa6622de6496'],False,"['gelu.cu', 'test_moe_res_matmult.py']"
12e1cb82628bd13c82464b3503928a69941c2665,"MOE matmult with memaccess (#2336)

* Fix formatting

* Remove redundant variable",['80b10d0c6985fb1792a044678c3c5c306cc0922e'],False,['gelu.cu']
48c5220b525d750949d854cebb4b39345611bfe7,"Refactor residual add kernels (#2333)

Co-authored-by: Ammar Ahmad Awan <ammar.awan@microsoft.com>",['12e1cb82628bd13c82464b3503928a69941c2665'],False,"['pt_binding.cpp', 'transformer_inference.py', 'test_residual_add.py']"
954e0c61f196af595a764e9036777fc0dd428e34,"mem access for quantize kernel (#2331)

* mem access for quantize kernel

* format

* format fp32

* modify quant kernel

* modify quant kernel2

* modify format

* format

* fix comments in pytest

* fix comments in pytest

* format

* rerun

Co-authored-by: Ammar Ahmad Awan <ammar.awan@microsoft.com>
Co-authored-by: Connor Holmes <connorholmes@microsoft.com>",['48c5220b525d750949d854cebb4b39345611bfe7'],False,"['quantizer.cu', 'test_quant.py']"
76de924b93e5c3b4790d15b106e16bb5eaeb80be,fix zero docs (#2350),['3d097bb8657fb78c18df2b7e9e56093764b7a564'],False,['config-json.md']
993264388db97b63a77547c3092fc82fc9425ac7,"Inference profiling updates/fixes (#2348) (#2349)

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>
Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>",['76de924b93e5c3b4790d15b106e16bb5eaeb80be'],False,"['bert-bench.py', 'gpt-bench.py', 'engine.py', 'replace_module.py', 'replace_policy.py', 'test_inference.py']"
9aa7b638b7a37f6ca03b2b5f1311c9a5759ea9a8,"Kernel Data Conversion Utility (#2327)

* Unify macro definitions and constants in a single file

* Conversion utility implementation.

* Fix reversion from formatting

* Bugfixes after testing with correct DeepSpeed

* Inline markers are available on both HIP + CUDA",['993264388db97b63a77547c3092fc82fc9425ac7'],False,"['conversion_utils.h', 'custom_cuda_layers.h', 'ds_kernel_utils.h', 'memory_access_utils.h', 'gelu.cu', 'inference_context.h', 'inference_cuda_layers.h']"
2b1b0d2e86bf401fa633f20eab30bee368efd6ea,"docs(mixture-of-experts-inference): fix typo in tuto (#2345)

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['f210256a320ca63c281a2d29759a851c9a077009'],False,['mixture-of-experts-inference.md']
70e883a103ea9b224816fecf42e2c896c7cde95d,Updated issue templates (#2363),['9df604bf51ee3d713094faacf64fb2e6f77c1c1c'],False,"['inference_bug_report.md', 'training_bug_report.md']"
8e8c866ddf69d138c7f4465a80a3d446e940a718,Update issue templates,['70e883a103ea9b224816fecf42e2c896c7cde95d'],False,"['compression_bug_report.md', 'inference_bug_report.md', 'training_bug_report.md']"
3486afb1a3927c1d6f10bc805aaf605ad6d82724,"fix cuda invalid config error in dequant kernel (#2362)

* format

* remove round fn",['8e8c866ddf69d138c7f4465a80a3d446e940a718'],False,['dequantize.cu']
b450da4f70f65d83469098399d91823c6a86d008,"Add missing pytest fixture scope (#2353)

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>",['3486afb1a3927c1d6f10bc805aaf605ad6d82724'],False,['test_residual_add.py']
79692af1ea287494849f754e79024a4417325aa6,"Extend residual_add kernel tests to conver pre_attn_norm (#2354)

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['b450da4f70f65d83469098399d91823c6a86d008'],False,['test_residual_add.py']
eed40324dba2a4fc14bee17c824e5089bbaa941d,"Capture error message during sweep tests (#2351)

* Collect error messages in results.csv

Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['e14d40e5f3aee400ef24d31dee67e2a4c456618a'],False,"['collect_results.py', 'run_model.sh', 'sweep.sh']"
b609a29412ba4cfbbbf4d6a5c725d2597315c9ff,fix an exception when recursively casting dicts to fp16 (#2370),['eed40324dba2a4fc14bee17c824e5089bbaa941d'],False,['engine.py']
ff427438651943ee473ab37547337f5f3d8c2279,"Refactor remaining distributed tests (#2216)

* batch of refactored tests

* more test refactoring

* fp16 test refactor

* more refactors

* added DistributedFixture class

* applied DistributedFixture to first batch of tests as a trial

* added DistributedFixture test and documentation

* last tests

* fixes for refactored tests

* remove subdirs in workflow files

* fix pytest syntax error

* fix another syntax error

* update imports

* use DistFixture with elastic checkpoint test

* missing import

* update to shared class tmpdir for elastic test

* moved test files

* avoid duplicate test file name

* last refactor and moving test files

* formatting

* fix broken import

* testing forked AMD tests

* update abstract method

* use blob storage for accelerate and transformers tests

* upgrade torch for acclerate CI

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['b609a29412ba4cfbbbf4d6a5c725d2597315c9ff'],False,"['amd.yml', 'nv-accelerate-v100.yml', 'nv-torch-latest-v100.yml', 'nv-transformers-v100.yml', 'conftest.py', 'alexnet_model.py', 'test_moe_checkpoint.py', 'test_zero_optimizer.py', 'test_dist.py', 'common.py', 'ds_batch_config.json', 'gpt2-merges.txt', 'gpt2-vocab.json', 'test_inference.py', 'test_ds_arguments.py', 'test_configurable_parallel_mp.py', 'test_configurable_parallel_pp.py', 'modeling.py', 'modelingpreln.py', 'test_moe.py', 'test_moe_tp.py', 'multi_output_model.py', 'test_aio.py', 'test_cuda_backward.py', 'test_cuda_forward.py', 'test_activation_checkpointing.py', 'test_coalesced_collectives.py', 'test_onebit.py', 'test_bf16.py', 'test_dynamic_loss_scale.py', 'test_fp16.py', 'test_averaging_sparse_gradients.py', 'test_sparse_grads.py', 'test_curriculum_learning.py', 'test_ds_config_dict.py', 'test_ds_config_model.py', 'test_ds_initialize.py', 'test_lr_schedulers.py', 'test_multi_output_model.py', 'test_partition.py', 'test_ignore_unused_parameters.py', 'test_zero.py', 'test_zero_context.py', 'simple_model.py', 'test_averaging.py', 'test_checkpointing.py', 'test_coalesced_collectives.py', 'test_configurable_parallel.py', 'test_curriculum_learning.py', 'test_ds_initialize.py', 'test_fp16.py', 'test_ignore_unused_parameters.py', 'test_lr_schedulers.py', 'test_moe.py', 'test_moe_tp.py']"
0a2ae2ef456c741e895e35c0131d76efbc7b55a4,Fix the MLP output tensor's shape (#2380),['ff427438651943ee473ab37547337f5f3d8c2279'],False,['pt_binding.cpp']
f4a92a19a61a1dd096fb94eb607e1c441e35bac8,Checkpoint backwards-compatbility workaround (#2384),['46a886c068c504d7fa06dc52e02aee061aa53384'],False,"['bf16_optimizer.py', '__init__.py']"
d0c6b2958f1629edf89ee204e0b53e216a593d02,Fix figure reference (#2419),['b8b98c604653a55d8ec2de54a672081aeb7d75a7'],False,['2022-10-11-mii.md']
3db0b5e2de88bff1fd751f0778bada1243856aff,"Add SLURM Multinode Runner (#2404)

Signed-off-by: Dashiell Stander <dstander@protonmail.com>
Co-authored-by: Dashiell Stander <dashiell@ip-172-31-45-20.ec2.internal>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['5a16213201f76826df7bcb5fca9b9c56206aee90'],False,"['constants.py', 'multinode_runner.py', 'runner.py', 'test_multinode_runner.py']"
d5d10b0ce8459597ceea03ce43f6278cfafc1b54,"Fix issue with corrupted output on long generation for GPT (#2359)

Co-authored-by: Reza Yazdani <44502768+RezaYazdaniAminabadi@users.noreply.github.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['3db0b5e2de88bff1fd751f0778bada1243856aff'],False,"['pt_binding.cpp', 'inference_context.h', 'transformer_inference.py']"
cd3a70953a0a2f517f165928cd10b0d11f666dcc,"Fix GPT Neo-X multi-gpu inference (#2401)

Co-authored-by: Reza Yazdani <44502768+RezaYazdaniAminabadi@users.noreply.github.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['843fd4c19d520fd08977ee73074f29faa63d5fa0'],False,['replace_module.py']
1c39fe13c18cabe42d628ce04b6f52db21011cb5,CI fixes related to triton (#2422),['1ffcfebf1ae5fb043db59603dd24dfc4b0035a5f'],False,"['amd.yml', 'nv-accelerate-v100.yml', 'nv-inference.yml', 'nv-nightly.yml', 'nv-torch-latest-v100.yml', 'nv-torch-nightly-v100.yml', 'nv-torch18-p40.yml', 'nv-torch18-v100.yml', 'nv-transformers-v100.yml']"
537e8581fe3f8332c8a8ab9a5404b629d8a4f924,fix checkpoint loading when it is a dictionary (#2425),['ec13da6ba7cabc44bb4745a64a208b8580792954'],False,['engine.py']
5eafb8c78db46a888f62f58ceb2bb9757c7095c1,"Make error regex more generic in collect_results.py (#2415)

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['537e8581fe3f8332c8a8ab9a5404b629d8a4f924'],False,['collect_results.py']
cfead551325cdf1e6d4f4da2146dcd0f238e5e71,"fixes #2389 (#2411)

truncating expert param storage for checkpointing

Co-authored-by: Alexander Jipa <azzhipa@amazon.com>
Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>",['5eafb8c78db46a888f62f58ceb2bb9757c7095c1'],False,"['engine.py', 'common.py', 'simple_model.py']"
34fb6d198056fe169d6fca6d6c45f66d0264fe08,"Fix for inference gpt-j test (#2430)

* fix for gpt-j failing due to tokenizer error

* limit number of gpt-j tokens generated due to low memory",['cfead551325cdf1e6d4f4da2146dcd0f238e5e71'],False,['test_inference.py']
906b4a025fee7c519e0f4e3539901a43b16516b6,"Fixing bug 2361 (#2410)

* fixing bug 2361

* adding pytest for config initialization

* chaning expected output to FusedAdam

* remove print statement

* running yapf on modified files

* running pre-commit formatting

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['34fb6d198056fe169d6fca6d6c45f66d0264fe08'],False,"['engine.py', 'test_ds_initialize.py']"
799120e7e44cefde7db8005295d40c2fa1eaf28d,"Universal checkpoint for zero stage 1 (#2284)

* Refactor universal checkpointing and tensor fragments

* Formatting

* Support zero stage1; Expand TP dim

* Remove debug prints

* Detect sharded optimizer state

* Format fixes

* Encode reshaping guide

* More symbolic constants

Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>",['906b4a025fee7c519e0f4e3539901a43b16516b6'],False,"['__init__.py', 'constants.py', 'deepspeed_checkpoint.py', 'reshape_3d_utils.py', 'reshape_utils.py', 'universal_checkpoint.py', 'zero_checkpoint.py', 'bf16_optimizer.py', 'engine.py', 'stage_1_and_2.py', 'tensor_fragment.py']"
b2a724e257e435b4b29faf913b689a7f16208c98,"Add TestInjectionPolicy inference unittest class for testing custom injection policies (#2426)

This PR adds a TestInjectionPolicy inference unittest class for testing custom injection policies.

This test differs from the existing tests in that the injection_policy dictionary is explicitly specified when calling the DeepSpeed init_inference API.

The google/t5-v1_1-small text2text-generation model and the roberta-large fill-mask model are added as tests with the injection policy explicitly specified.

This is done to expand our unittest coverage to test the path where the replace_wo_policy function is invoked (see GH-2387).

Co-authored-by: Lev Kurilenko <lekurile@microsoft.com>
Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>",['1b7c6791d57d6508bc4de4a5a9d210308eb75b38'],False,"['requirements-inf.txt', 'conftest.py', 'test_inference.py']"
b8fb9c3f1a8a8e3e574a7d53e83ce2b72d471aa3,"parallelize writing of layer checkpoint files across data parallel instances (#1419)

* parallelize layer checkpoints across data parallel groups

* use partition_uniform to determine start/end index values

* formatting fix

* config: add option for parallel write of layer checkpoints in pipeline stage

* yapf fixes

* enable parallel layer write according to config param

* avoid extraneous makedir when rank 0 writes all layers

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['99fde3b7a573f2a261dc5ef9380870ce30a1db9d'],False,"['config.py', 'constants.py', 'engine.py', 'engine.py', 'module.py']"
877a8818a7cdc994188dde55db6901d79457339e,"Fix broken link to DeepSpeed Megatron fork (#2440)

Co-authored-by: Lev Kurilenko <lekurile@microsoft.com>",['b8fb9c3f1a8a8e3e574a7d53e83ce2b72d471aa3'],False,['megatron.md']
7d113633e4cd0e430d4a07cb78ee700b3c03423f,"Fix Bug #2319 (#2438)

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['a5248643571581c6ca3b453a3b9d01ac157ac00a'],False,"['config.py', 'constants.py', 'engine.py', 'config-json.md']"
5d1f595c94962de56ccb060def37ce7768a5e91b,"update pytorch pool operator function signiture (#2443)

* update pytorch pool operator function signiture

* fix the case where kwargs is None",['7d113633e4cd0e430d4a07cb78ee700b3c03423f'],False,['profiler.py']
b85eb3b979637730d7168e1dda5eab970a6500b1,"Fix build issues on Windows (#2428)

* Fix build issues on Windows

* small fix to complie with new version of Microsoft C++ Build Tools

Co-authored-by: Reza Yazdani <reyazda@microsoft.com>
Co-authored-by: Reza Yazdani <44502768+RezaYazdaniAminabadi@users.noreply.github.com>",['5d1f595c94962de56ccb060def37ce7768a5e91b'],False,"['build_win.bat', 'inference_context.h', 'builder.py', 'fused_adam.py', 'fused_lamb.py', 'setup.py']"
8da0238b7a7bb8cfa658fda5a71603f99ae1fa3c,"rollback ds config changes (#2395)

* rollback ds config changes

* fix format

* Fix error when output_file is a relative path without a prefix (#2397)

Co-authored-by: Benjamin Steenhoek <benjaminjsteenhoek@gmail.com>

* fix restuls and exprs path to use absolute path

* write out optimial config after tuning

* fix format

* assert tuning result dir creation

Co-authored-by: Benjamin Steenhoek <benjaminjsteenhoek@gmail.com>
Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>",['b85eb3b979637730d7168e1dda5eab970a6500b1'],False,"['autotuner.py', 'config.py', 'constants.py', 'scheduler.py', 'runner.py', 'profiler.py']"
e772f166651ff03b3310c05130a088b7d2a65db6,"Use CUDA events for inference model profiling (#2371)

* use cuda event timers for model profiling",['8da0238b7a7bb8cfa658fda5a71603f99ae1fa3c'],False,"['engine.py', 'test_model_profiling.py']"
3b3ba3c2566a66d651393028af6c923b32eeffaa,Fixing a mismatch in basic adam test. (#2447),['e772f166651ff03b3310c05130a088b7d2a65db6'],False,['test_fp16.py']
825f9d488b7c39350b24d5a673425ed73363507c,"Fixes for various CI problems (#2457)

* check only major CUDA version in CI

* update expected torch latest version

* pin torch latest to 1.12 until issues with 1.13 are resolve

* wrong expected torch version

* Update nv-torch18-v100.yml

* remove forked from pytest option due to cuda re-initialization errors

* removed expected torch version from inference tests, causing errors currently

* fix various bugs that popped up

* move all tests over to cu111 runners, cu113 runners having problems",['3432c740e99cab9e78c67b59ca3c53d49e4b29b7'],False,"['nv-inference.yml', 'nv-nightly.yml', 'nv-torch-latest-v100.yml', 'nv-torch18-v100.yml', 'test_simple.py', 'test_inference.py']"
10e9d04c23f52056f8b2f1ad72847cfca1c63f35,"Cache Allocation and Softmax Fixes (#2433)

Co-authored-by: Reza Yazdani <reyazda@microsoft.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['825f9d488b7c39350b24d5a673425ed73363507c'],False,"['pt_binding.cpp', 'softmax.cu', 'inference_context.h', '__init__.py', 'engine.py', 'replace_module.py', 'transformer_inference.py']"
a47c3e03cf45b2ba0dc29800a47105418cdc91e7,fix accelerate link (#2481),['ffb6d9876254c0ff39bae224c66d2b4f49d391e1'],False,['README.md']
521d329b975de97ec0b52395f02bb32466b8dc35,"Fix CI issues related to cupy install (#2483)

* remove any cupy install when setting up environments

* revert previous changes to run on cu111 runners

* fix for when no cupy is installed

* remove cupy uninstall for workflows not using latest torch version

* update to cu116 for inference tests

* fix pip uninstall line

* move python environment list to after DS install

* remove cupy uninstall

* re-add --forked

* fix how we get cupy version (should be based on nvcc version)",['9cfcf7431a02ad9c75e180e273d82566df6b7e34'],False,"['amd.yml', 'nv-accelerate-v100.yml', 'nv-inference.yml', 'nv-lightning-v100.yml', 'nv-nightly.yml', 'nv-torch-latest-v100.yml', 'nv-torch-nightly-v100.yml', 'nv-torch18-p40.yml', 'nv-torch18-v100.yml', 'nv-transformers-v100.yml', 'setup.py']"
6f77da1bae506cfa54636a03ad74f21d1e90a1ac,"Add `scale_attn_by_inverse_layer_idx` feature (#2486)

* Add scale_attn_by_inverse_layer_idx feature

* Fix layer_id bug

* Fix scaling value

Co-authored-by: Connor Holmes <connorholmes@microsoft.com>
Co-authored-by: Reza Yazdani <44502768+RezaYazdaniAminabadi@users.noreply.github.com>",['d2d1b4c3c7171291711600062fba9ace58a484fa'],False,"['replace_module.py', 'attention.py', 'config.py', 'ds_attention.py', 'moe_inference.py']"
e7e7595502033a8a62612ab8f657395d5912528e,"Stable Diffusion Enhancements (#2491)

Co-authored-by: cmikeh2 <connorholmes@microsoft.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>
Co-authored-by: Reza Yazdani <reyazda@microsoft.com>",['6f77da1bae506cfa54636a03ad74f21d1e90a1ac'],False,"['opt_bias_add.cu', 'pt_binding.cpp', 'spatial_cuda_layers.h', 'gelu.cu', 'layer_norm.cu', 'normalize.cu', 'pt_binding.cpp', 'inference_context.h', 'inference_cuda_layers.h', 'engine.py', 'unet.py', 'vae.py', 'clip_encoder.py', 'ds_transformer.py', '__init__.py', 'replace_module.py', 'replace_policy.py', '__init__.py', '__init__.py', 'bias_add.py', 'diffusers_2d_transformer.py', 'diffusers_attention.py', 'diffusers_transformer_block.py', 'ds_attention.py', '__init__.py', 'spatial_inference.py', 'transformer_inference.py', 'test_nhwc_bias_add.py', 'test_bias_geglu.py', 'test_layer_norm.py']"
be5ec506bd5219adea80bf3ac89678914ec32cd1,"Fix nightly CI tests (#2493)

* fix for lm-eval nightly tests and add gpt-j to MPtest because OOM on single GPU

* add nv-nightly badge",['ee39187d8f07f7efc615f64affa8cc60ccf41eb5'],False,"['nv-nightly.yml', 'README.md', 'test_inference.py']"
06e00f61ce12238ccca1cedbeb1e9391d7199c42,Fix typos: deepseed -> deepspeed (#2499),['f2710bbe1dc19717b3c99e910cb453ea20ae8b08'],False,"['module.py', 'partition_parameters.py', 'large-models-w-deepspeed.md']"
78d4ca1f4bb2f0e8a7f2cfd052b0e7733ab41898,"Deepspeed quantization library v0.1 (#2450)

* Initial commit Deepspeed quantization library

* Match function signatures

* Add Quantization Kernel

* adding offset comparision and precommit changes

* format fixes

* FIle name changes

* pt_binding_changes

* test name change

* Integer quantization, minor refactors

* Add directed test_case

* format fixes

* Move param calculation to constructor of params class

* Use local function and add elemsPerBlock

* change function to be specalized

* sub block reduce

* add new schedule

* Add new schedule test case

* fix illegal writes in sch1

* Style fixes in comments

Co-authored-by: Connor Holmes <connorholmes@microsoft.com>",['d40a15fcf017d0675ca1c4759ea70af6a4f89247'],False,"['custom_cuda_layers.h', 'quantization.h', 'quantization_utils.h', 'reduction_utils.h', 'fake_quantizer.cu', 'pt_binding.cpp', 'quantize.cu', 'quantizer.py', 'test_fake_quantization.py', 'test_quantize.py']"
e59f80549e30db0b6c088fd3bb2289ac98d91510,"Fix backward compatibility for InferenceConfig (#2516)

* Make new InferenceConfig backwards compatible with previous init_inference API

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['78d4ca1f4bb2f0e8a7f2cfd052b0e7733ab41898'],False,"['nv-accelerate-v100.yml', 'nv-transformers-v100.yml', 'config.py', 'config_utils.py']"
30c8d8a8818ee3585d320469e0777083530a1cc4,Initial dequant library implementation (#2521),['0b265326127bb4faa9658b8ede91615aaf6e3e44'],False,"['dequantization_utils.h', 'quantization.h', 'quantization_utils.h', 'reduction_utils.h', 'dequantize.cu', 'pt_binding.cpp', 'quantize.cu', 'quantizer.py', 'test_dequantize.py', 'test_quantize.py']"
211055216792cbb52ab6d355f698c194f9c55efb,"Fixes for torch 1.14 due to new torch.numel return type (#2522)

* fixes for new torch.numel return type

* address comment",['30c8d8a8818ee3585d320469e0777083530a1cc4'],False,"['profiler.py', 'nccl.py']"
8b4318b950d09d975c62cd8f4285821fa670ce87,Make DS-Inference config readable from JSON (#2537),['57e0a55066f0229ea4bba59c520267643aab792f'],False,"['__init__.py', 'test_inference_config.py']"
c5ee27f737388feaa442d3d7534c8b93d02bbb41,"Add MII tests (#2533)

Adding MII tests to ensure changes to DS-Inference do not break MII",['8b4318b950d09d975c62cd8f4285821fa670ce87'],False,"['nv-mii.yml', 'config.py']"
4abf637f96cd36d450125a38a15d21df1cf0b8db,"Remove mutable default parameter in init_inference() (#2540)

A mutable default value is dangerous because editing it will change the
value for all future calls to the function. The value is itself edited
later in the function, so this problem will likely be encountered sooner
or later.

Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>",['c5ee27f737388feaa442d3d7534c8b93d02bbb41'],False,['__init__.py']
90ae6884424232870154b49967c3e61f0db550d6,"Pass down the new DS inference config to replace_transformer_layer. (#2539)

* pass down the new DS inference config to replace_transformer_layer.

* remove quantize_settings and rename the ep_mp_group.

* Fix model_config passing. Fixes gptj issue with wrong output.

* fix small bug in gpt-neo.

Co-authored-by: Reza Yazdani and Michael Wyatt",['5df1eea74272d9fd7e15e8f40044a9cda6187de9'],False,"['config.py', 'engine.py', 'replace_module.py']"
21c28029648df4c98acd6331a8fdc0d29f297fa7,"Adding Gradient Accumulation Data Type Config (#2512)

* Adding gradient accumulation dtype config.

* Switching to new DtypeEnum

* Adding standalone check function, and unit tests

* Variable disambiguation

* Adding checks for unsupported states.

* Updating for PR comments.

* Reorganizing unit test.

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['90ae6884424232870154b49967c3e61f0db550d6'],False,"['config.py', 'constants.py', 'engine.py', 'config-json.md', 'test_ds_initialize.py', 'util.py']"
abe4fc6b55a8e3b78f46479389887f1942cc93cf,"encoded ds config into command line argument when launching child processes in autotuning (#2524)

* rollback ds config changes

* fix format

* Fix error when output_file is a relative path without a prefix (#2397)

Co-authored-by: Benjamin Steenhoek <benjaminjsteenhoek@gmail.com>

* fix restuls and exprs path to use absolute path

* use base64 encoded ds config as cmd arg

* fix format

* remove assert

* write out optimial config after tuning

* fix format

* no need to update ds config path when encoding ds config

* udpate

* do not use abs path for result and expr dir

* fix conflicts

* fix run mode

* fix format

* fix format

Co-authored-by: Benjamin Steenhoek <benjaminjsteenhoek@gmail.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['340fc0cf1926f4cb721297bd3c4dd1fb11203668'],False,"['autotuner.py', 'config.py', 'scheduler.py', 'config.py', 'engine.py']"
aeda7f9f8c5bdaee318e7b6094279484d41c659c,"Fix invalid check of recorded parameter orders in zero stage3. (#2550)

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['ffcf3846739108b0010de319438b17bb4e70b4c9'],False,['partitioned_param_coordinator.py']
06938835ebf47df6aacab30aa7c0eb049bc5f228,"Support fp32 gradaccum for bf16 model (#2566)

* allow bf16 model with fp32 gradient accumulation datatype

* allow fp32 gradient accumulation and bfloat16 model in amp mode

* alternative fix for grad accumulation type mismatch.  In the case of zero optimizer we should have grad accum type == model data type

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['2d8f3f564ddc1cce2f458546b595dc240034071a'],False,['engine.py']
b8416282072e977fdc0f9c77019852faf586a418,"Drop Maxwell Support (#2574)

* Officially drop Maxwell support

* Formatting

* Comparison mismatch fix",['06938835ebf47df6aacab30aa7c0eb049bc5f228'],False,"['apply_rotary_pos_emb.cu', 'dequantize.cu', 'gelu.cu', 'relu.cu', 'softmax.cu', 'transform.cu', 'builder.py', 'transformer_inference.py']"
35b350b28ca99498edd0d1644ee4dde6247a5409,"Fix quantized-inference & Add generic support of checkpoint loading (#2547)

* fix checkpoint loading when it is a dictionary

* fix some issues with saving ckpt & int8 inference

* fix quantized-inference & add generic support of checkpoint loading

* remove int8 hard-coded flag

* fix mlp return tensors

* fix several issue to load checkpoints of GPT-J, GPT-NEOX, and OPT with different TP-size

* add more comments & description for checkpoint-loading module

Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>",['b8416282072e977fdc0f9c77019852faf586a418'],False,"['pt_binding.cpp', 'ds_transformer.py', 'layers.py', 'load_checkpoint.py', 'replace_module.py', 'replace_policy.py', 'ds_attention.py', 'ds_mlp.py']"
731965db330d9ee0d393526412134063dbd3709c,"Fix MegatronLayerPolicy to have megatron_v2=True (#2579)

This PR updates the MegatronLayerPolicy to set megatron_v2=True, which is required in order to properly transpose in the replace_with_policy() function.

After the change in this PR, in conjunction with PR #99 in the Megatron-DeepSpeed fork, the Megatron text-generation example works with DS inference.",['35b350b28ca99498edd0d1644ee4dde6247a5409'],False,['replace_policy.py']
591744eba33f2ece04c15c73c02edaf384dca226,"Support N-dimension input in quantization kernel (#2575)

* Add support for inputs > 2D

* use vec

* Add N-Dim support to Dequant kernel

* merge master and fix format

* Bug Fix

* fix num_bits

* Fix dequant

Co-authored-by: Connor Holmes <connorholmes@microsoft.com>",['18d55e54b0f7d889e34cd2d2dffd50fcf1ba99a0'],False,['pt_binding.cpp']
ccb8eb81fb86c7d2447364854d53fcf02aea56a4,"Add checkpoint sharding unit tests (#2561)

* added checkpopint sharding tests",['591744eba33f2ece04c15c73c02edaf384dca226'],False,"['replace_module.py', 'replace_policy.py', 'state_dict_factory.py', 'test_checkpoint_sharding.py']"
35eabb0a336e7a8e9950a550475ceaebda42066c,Fix issues w. python 3.6 + add py-version checks to CI (#2589),['18713c6838acfa779c2f58e9d97f85be2d592f6b'],False,"['python.yml', 'unet.py', 'replace_module.py', '__init__.py', '__init__.py', 'diffusers_transformer_block.py', 'setup.py']"
ef869377e91660a649d16d8f416841ed9b5dbed3,"DeepSpeed Data Efficiency Library (#2585)

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['2600db54f9396a8726065954e7aa918c97f4bf7b'],False,"['README.md', 'custom_cuda_layers.h', 'gather_scatter.cu', 'pt_binding.cpp', 'slice_attn_masks.cu', 'token_sort.cu', '__init__.py', 'dropping_utils.py', 'config.py', 'constants.py', 'config.py', 'constants.py', 'curriculum_scheduler.py', '__init__.py', 'basic_layer.py', 'helper.py', 'scheduler.py', 'utils.py', '__init__.py', 'data_analyzer.py', 'data_sampler.py', 'indexed_dataset.py', 'utils.py', 'dataloader.py', 'engine.py', 'engine.py', '_config.yml', 'navigation.yml', 'config-json.md', 'training.md', '2022-12-12-data-efficiency.md', 'curriculum-learning.md', 'data-efficiency.md', 'data_efficiecy_fig0.png', 'data_efficiecy_fig1.png', 'data_efficiecy_fig2.png', 'data_efficiecy_fig3.png', 'index.md', '__init__.py', 'random_ltd.py', 'test_curriculum_learning.py', 'test_data_efficiency.py', 'version.txt']"
acde873cdc1b87f01d26422173da54755bad1208,fix blog link (#2600),['ef869377e91660a649d16d8f416841ed9b5dbed3'],False,"['README.md', 'data-efficiency.md', 'index.md']"
7425a3652e55d532fbdb0540a05c2467c82c8973,"Migrate ops tests to new inference_ops marker (#2599)

* Migrate ops tests to new inference_ops marker

* Disable by default

* Add missing test cases

* Reorder such that inference_ops will run[fail] first",['acde873cdc1b87f01d26422173da54755bad1208'],False,"['nv-inference.yml', 'pytest.ini', 'test_dequantize.py', 'test_fake_quantization.py', 'test_quantize.py', 'test_nhwc_bias_add.py', 'test_bias_add.py', 'test_bias_geglu.py', 'test_bias_gelu.py', 'test_bias_relu.py', 'test_layer_norm.py', 'test_moe_res_matmult.py', 'test_residual_add.py']"
3a3dfe66bb8100978bd38e3605735e15457113a8,"Move layer norm to new schedule (#2590)

* Move layer norm to new schedule

* Pre-commit fixes

* fix comments

* format fixes

* Merge unrolls

* format fixes

* camelCase

* format fixes

* revert unwanted file

* move pow2 function

* format fixes

Co-authored-by: Connor Holmes <connorholmes@microsoft.com>",['7425a3652e55d532fbdb0540a05c2467c82c8973'],False,"['ds_kernel_utils.h', 'quantize.cu', 'layer_norm.cu', 'pt_binding.cpp', 'inference_cuda_layers.h', 'diffusers_transformer_block.py', 'test_layer_norm.py']"
384f17b05bdf2c73fa6b5da66399d8c848779e89,"[deepspeed/autotuner] Bug fix for binary search for batch size (#2162)

* bug fix for binary search for batch size

* fix binary search termination condition",['3a3dfe66bb8100978bd38e3605735e15457113a8'],False,['autotuner.py']
2e596e679d28a660c9a06e38ed8088ecd9e3c730,add fix for older pydantic versions (#2611),['384f17b05bdf2c73fa6b5da66399d8c848779e89'],False,['config_utils.py']
0f0e38c52072cbcb12ed410a0e2704f2723123e2,"fixes #2498 (#2603)

taking gradient accumulation steps into account for throughput calculation

Co-authored-by: Alexander Jipa <azzhipa@amazon.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['503706ac44f3702f3c890e3b89b59d0bd77933dd'],False,"['engine.py', 'engine.py', 'timer.py', 'test_zero_context.py']"
8c56c25d84764dccf5979cb40a318bc3ccfac31a,[launcher] parse hostfile via regex and added error checks (#2626),['5676f5ec9c68fa18ff015bb5826fb269777b58de'],False,"['runner.py', 'test_run.py']"
6fff50f52b85dda0686bbe365fcc27be986de4a5,"Unit tests setup own venv (#2628)

add reusable workflow that sets up fresh venv for each test and prints relevant environment info",['8c56c25d84764dccf5979cb40a318bc3ccfac31a'],False,"['amd.yml', 'formatting.yml', 'nv-accelerate-v100.yml', 'nv-inference.yml', 'nv-lightning-v100.yml', 'nv-megatron.yml', 'nv-mii.yml', 'nv-nightly.yml', 'nv-torch-latest-v100.yml', 'nv-torch-nightly-v100.yml', 'nv-torch18-p40.yml', 'nv-torch18-v100.yml', 'nv-transformers-v100.yml', 'pre-compile-ops.yml', 'action.yml']"
6273dffc2f192275a08268b683c309a328b52191,Fix typo in autotuner.py (#2639),['11f5daba5e9589880856efec6de733cb84f42d08'],False,['autotuner.py']
a298a43af22b9f971ff63e414887e659980889d9,"[zero-3] Handle forward parameter return correctly in nested cases (#2642)

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['6273dffc2f192275a08268b683c309a328b52191'],False,"['parameter_offload.py', 'test_zero_context.py']"
e0aa84c5b5ea16c2f316ec7c494c60328d179f63,Fix issue w. bloom when changing tp size (#2645),['bb68c526ad2c267dfb235db9c0d0fb1413d19a34'],False,['replace_module.py']
6375cb3f34d9c5c10d56ffb8dad27734cb4d85b0,fix assertion error in zero stage 3 (#2647),['e0aa84c5b5ea16c2f316ec7c494c60328d179f63'],False,['stage3.py']
f2ea7a3818b28e60d689e75635a7c9ad4cd9a805,"[doc] fix `min_loss_scale` default (#2660)

* [doc] fix `min_loss_scale` default

* align",['d9b788d773ce97281ee63064cc99993cb82397e2'],False,['config-json.md']
a091bc223c01e94448f443456a6c15684644b966,"[launcher] fail gracefully if hostname -i doesn't work as expected (#2631)

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['f2ea7a3818b28e60d689e75635a7c9ad4cd9a805'],False,['runner.py']
95d9a1b6c3ec990dd04d02bda3878b1154408f2f,"Fix Opt injection (#2541)

* fix Opt injection & add injection verification check at inference test

* fix several issues

* remove fixture

* remove check_injection when no kerenl is injected

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['a091bc223c01e94448f443456a6c15684644b966'],False,"['engine.py', 'replace_policy.py', 'test_inference.py']"
9548d48f48a53b97bdbeada374b7f35ad2f1c655,"Abstract accelerator (step 2) (#2560)

* Abstract accelerator (step 2)

* more flex op_builder path for both installation and runtime

* add SpatialInferenceBuilder into cuda_accelerator.py

* use reflection to make cuda_accelerator adapt to CUDA op builder change automatically

* clean up deepspeed/__init__.py

* add comments in cuda_accelerator for no torch path

* Update deepspeed/env_report.py

Change env_report.py according to suggestion

Co-authored-by: Michael Wyatt <mrwyattii@gmail.com>

* reduce the range of try...except for better code clarity

* Add porting for deepspeed/ops/random_ltd/dropping_utils.py

* move accelerator to top directory and create symlink under deepspeed

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Michael Wyatt <mrwyattii@gmail.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['95d9a1b6c3ec990dd04d02bda3878b1154408f2f'],False,"['MANIFEST.in', 'MANIFEST_win.in', '__init__.py', 'abstract_accelerator.py', 'cuda_accelerator.py', 'real_accelerator.py', 'aio_bench_perf_sweep.py', 'ds_aio_basic.py', 'ds_aio_handle.py', 'validate_async_io.py', 'accelerator', 'comm.py', 'env_report.py', 'git_version_info.py', 'cpu_adagrad.py', 'cpu_adam.py', 'fused_adam.py', 'fused_lamb.py', 'quantizer.py', 'dropping_utils.py', 'matmul.py', 'bias_add.py', 'diffusers_attention.py', 'diffusers_transformer_block.py', 'ds_attention.py', 'ds_mlp.py', 'moe_inference.py', 'transformer.py', '__init__.py', 'all_ops.py', 'builder_names.py', 'setup.py']"
323c266cfe65f16f522092f1bc84998e04ed7f94,"[Bug Fixed] use torch.cuda.is_available() (#2661)

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['97deaaec5783b85956b7330d0d0b8f94dad92a14'],False,"['fused_optimizer.py', 'unfused_optimizer.py', 'stage3.py', 'stage_1_and_2.py']"
be6d19f0f1d929f8d28df5392be588ad592fde48,"fix  Tensor contiguous bug in model_compression (#2671)

double check the unit tests",['f30a03086189fbc65fd5d1d611ed3ec327debeb5'],False,['helper.py']
a3d7f106f59293c71c02aa1b1b327a6d811e5790,"Add mlflow logging for aml (#2495)

* add logging changes

* try w/out abspath

* undo last change

* start mlflow debug

* remove mlflow from export_envs

* add mlflow logging for reversed

* remove mlflow.start_run

* add back start run

* don't clean cmd

* print os environment variables

* remove first start run

* add run_id to mlflow star

* remove context managers

* move last end run

* add extra parent start_runs

* add run id logging

* add logging to run_ds_config

* change run_id to run_name

* add back context managers and run_id logs

* remove context mng

* debug environment variable

* reset environment variables

* add env variable deletion

* clean up

* remove unused import

* fix yapf/whitespace errors

Co-authored-by: Cheng Li <pistasable@gmail.com>",['89da037e80fc433e91e4480ffc84542936ae6074'],False,['autotuner.py']
cf9e433fd95bdd89b90123b7ab671ba03a591b1e,"fix import path to op_builder (#2687)

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['a3d7f106f59293c71c02aa1b1b327a6d811e5790'],False,['builder.py']
aef8a8560c420fe79bf8340fd544a941e9ae85f8,"Extend quantization utils features (#2683)

* Extend quantization utils features

* remove unwanted files

* fix cahce setting

Co-authored-by: Connor Holmes <connorholmes@microsoft.com>",['e7c14026d3b7a71e8624e5d15c3c4aae1e7b5bb0'],False,"['dequantization_utils.h', 'memory_access_utils.h', 'quantization.h', 'quantization_utils.h', 'dequantize.cu', 'pt_binding.cpp', 'quantize.cu', 'test_quantize.py']"
217cc07bb5984b59da175824e9b91f1982f1ba2e,"[GatheredParameters] add support for any iterator (#2664)

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['aef8a8560c420fe79bf8340fd544a941e9ae85f8'],False,"['partition_parameters.py', 'test_zero_context.py']"
c9c6ab9e32b054136c3a125900d6e2ed937432be,"fix for latest diffusers (#2699)

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['217cc07bb5984b59da175824e9b91f1982f1ba2e'],False,['diffusers_transformer_block.py']
bf6b9802cd04fba23f627e042682d69e2ae41045,ZeRO3 handling frozen weights] (#2653),['35575bce4e6ac37f49182f640df420f0bd4bade0'],False,"['stage3.py', 'test_zero.py']"
3f210c971590d5d23acc57004330f9ae38001748,"CUDA optional deepspeed ops (#2507)

* CPU-Adam: add compile-flag to enable param-copy from CPU to GPU

* guarde the CUDA-related include files and variables

* remove CUDA dependency from op_builder when building against CPU

* fixing the builder issues

* fix formatting

* return true when there is no mismatch on the cuda version

* guard for when cuda is not available & test with cpu-only environment

* Update cpu_adam and cpu_adagrad

* Format fixes

* Add configurable half precision type; Build/run in CUDA environment

* Run cpu_adam and cpu_adagrad in cpu only environment

* Mark CUDA only unit tests

* CPU environment CI

* Format fixes

* Remove --forked

* Add --forked

* CPU only CI should pass

* Format fixes

* Format fixes

* Remove scattered pytest.skip

* Fix cpu_adam unit test

* Update .github/workflows/nv-torch-latest-cpu.yml

Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>

* Update .github/workflows/nv-torch-latest-cpu.yml

Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>

* Address PR feedback

* OpenMP linking

* Fix unit tests

Co-authored-by: Reza Yazdani <reyazda@microsoft.com>
Co-authored-by: Reza Yazdani <44502768+RezaYazdaniAminabadi@users.noreply.github.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>
Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>",['7d0e4270f615c9c09564f24e742ccc71a924bb2e'],False,"['nv-torch-latest-cpu.yml', 'cpu_adagrad.cpp', 'cpu_adam.cpp', 'cpu_adagrad.h', 'cpu_adam.h', 'builder.py', 'cpu_adagrad.py', 'cpu_adam.py', 'adagrad_test.py', 'adam_test.py', 'common.py', 'test_compression.py', 'test_checkpoint_sharding.py', 'test_cpu_adagrad.py', 'test_cpu_adam.py', 'test_aio.py', 'test_cuda_backward.py', 'test_cuda_forward.py', 'test_flops_profiler.py', 'test_fp16.py', 'test_autocast.py', 'test_ds_config_dict.py', 'test_zero_context.py', 'test_zero_context_ancestry.py', 'test_zero_context_return.py', 'utils.py', 'test_init_on_device.py']"
df2495ca7801a737e9d04bcedd5b0776580b7fa3,remove master branch from CI triggers (#2712),['3f210c971590d5d23acc57004330f9ae38001748'],False,"['amd.yml', 'formatting.yml', 'nv-accelerate-v100.yml', 'nv-inference.yml', 'nv-lightning-v100.yml', 'nv-megatron.yml', 'nv-mii.yml', 'nv-torch-latest-v100.yml', 'nv-torch18-p40.yml', 'nv-torch18-v100.yml', 'nv-transformers-v100.yml', 'python.yml']"
23e5133c353c00f39257b853f5c9a4a69d039009,"update for lm-eval==0.3.0 (#2713)

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['0b549ad70a8966b5bfd3a0be5da472d8bb05222d'],False,"['nv-nightly.yml', 'requirements-inf.txt', 'test_inference.py']"
8d87c89e4246f50a3ed5c8fadf9269bd348fa83a,"BF16 optimizer for BF16+ZeRO Stage 1 (#2706)

* BF16 optimizer only with ZeRO stage 1.

* Updating to grad accum of fp32 for BF16 ZeRO1 case.

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['23e5133c353c00f39257b853f5c9a4a69d039009'],False,"['engine.py', 'test_ds_initialize.py']"
8df50a269122e85dca2d0b72e98de2763b60e87c,"fix typo (#2718)

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['8d87c89e4246f50a3ed5c8fadf9269bd348fa83a'],False,['diffusers_transformer_block.py']
d59b572911d121b4e296c18ccbc4ef3bc2794b15,"Automatic tensor parallelism v2 (#2670)

* loop through pipe.model

* tp_parser first draft

* client_module must be type object

* Simplify layernorm tracking. Add unittest.

* cleanup

* Add more models to unittest

* cleanup inference pytest for merging

* Add unittest

* cleanup

* pre-commit

* unittest id and pytest marker

* try marian for unittest

* precommit

* Move tp code to seperate file

* Add new auto tp file

* pre-commit and type

* Update deepspeed/module_inject/auto_tp.py

Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>

* Update deepspeed/module_inject/auto_tp.py

Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>

* Update tests/unit/inference/test_inference.py

Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>

* remove unused fillmask function

Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>",['34a11688c4b3dd98fe23d6222fc4c54e7ac74fe3'],False,"['engine.py', 'auto_tp.py', 'test_inference.py']"
4be8df721ae631264abb62216692c7eb64b49e20,"fixing optimizer sanity check (#2742)

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['d59b572911d121b4e296c18ccbc4ef3bc2794b15'],False,"['engine.py', 'test_ds_initialize.py']"
ddd48b36ac4586d08f747de09e021916569ac5b3,"[GatheredParameters] fix memory leak (#2665)

* [GatheredParameters] fix memory leak

* simplify

* cleanup and move

* style

* Formatting

* fix test

* fix test

* fix test take 2

* Trigger CI

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Joe Mayer <114769929+jomayeri@users.noreply.github.com>",['4be8df721ae631264abb62216692c7eb64b49e20'],False,"['partition_parameters.py', 'test_zero.py', 'test_zero_context.py']"
98cc35b6a8e53e15829ce64fd8da835db0c61da9,"Abstract accelerator (step 3) (#2677)

* Integrate accelerator abstraction interface into deepspeed/

* Fix error message in fp16/fused_optimizer

* fix error message in fp16/unfused_optimizer.py

* assign get_accelerator().pin_memory() result to input Tensor name

* no need to check cuda and whether nvtx supported

* move try-except into inner most block

* call Event() and Stream() in get_accelerator() for data type

* Make Stream and Event as properties of abstract interface so they can be used as data type in deepspeed

* Apply op_builder backend api change from #2705 from @jeffra

* fix tests where Builder NAME is used

* keep original ...Builder.NAME interface instead of ...Builder().NAME interface

* fix builder closure for installation

* fix randomltd builder

* add comments to clarify create_op_builder and get_op_builder

* fix compatibility with pip install -e

Co-authored-by: Cheng Li <pistasable@gmail.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['ddd48b36ac4586d08f747de09e021916569ac5b3'],False,"['abstract_accelerator.py', 'cuda_accelerator.py', 'aio_bench_perf_sweep.py', 'ds_aio_basic.py', 'ds_aio_handle.py', 'validate_async_io.py', 'autotuner.py', 'engine.py', 'multinode_runner.py', 'runner.py', 'clip_encoder.py', 'ds_transformer.py', 'layers.py', 'load_checkpoint.py', 'replace_module.py', 'cpu_adagrad.py', 'cpu_adam.py', 'fused_adam.py', 'fused_lamb.py', 'quantizer.py', 'dropping_utils.py', 'bias_add.py', 'diffusers_attention.py', 'diffusers_transformer_block.py', 'moe_inference.py', 'base.py', 'transformer.py', 'profiler.py', 'checkpointing.py', 'nccl.py', 'data_sampler.py', 'dataloader.py', 'engine.py', 'fused_optimizer.py', 'adam.py', 'lamb.py', 'zoadam.py', 'unfused_optimizer.py', 'engine.py', 'module.py', 'p2p.py', 'partitioned_optimizer_swapper.py', 'partitioned_param_swapper.py', 'pipelined_optimizer_swapper.py', 'utils.py', 'utils.py', 'weight_quantizer.py', 'linear.py', 'parameter_offload.py', 'partition_parameters.py', 'partitioned_param_coordinator.py', 'stage3.py', 'stage_1_and_2.py', 'utils.py', 'nvtx.py', 'timer.py', '__init__.py', 'all_ops.py', 'builder_names.py']"
d4bfae415d4b967877d16e2d7e889cd6dd42016b,"Fix autotuning so that it records Floating Point Operations per second, not microsecond (#2711)

* Fix how autotuning reports TFLOPS so that they are reported in FLOPS per second, not millisecond

Co-authored-by:  Nick Sarkauskas <nsarka00@gmail.com>
Co-authored-by: Quentin Anthony <anthony.301@osu.edu>
Signed-off-by: Dashiell Stander <dstander@protonmail.com>

* Actually it is microseconds -> seconds

Signed-off-by: Dashiell Stander <dstander@protonmail.com>

* Actually it is microseconds -> seconds

Signed-off-by: Dashiell Stander <dstander@protonmail.com>

Signed-off-by: Dashiell Stander <dstander@protonmail.com>
Co-authored-by: Nick Sarkauskas <nsarka00@gmail.com>
Co-authored-by: Quentin Anthony <anthony.301@osu.edu>",['98cc35b6a8e53e15829ce64fd8da835db0c61da9'],False,['engine.py']
30d3f5df7a60b809f8f655677ea033a2398de156,"fix a mispelled attribute (#2750)

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['d4bfae415d4b967877d16e2d7e889cd6dd42016b'],False,['partition_parameters.py']
0b06e0cbb0f1ec3f34e60136ce7c803df6780cd5,"Fix softmax backward (#2709)

* Reset KV-cache at the beginning of text-generation

* Add new backward kernel to handle large softmax-length

* remove unrelated changes

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Connor Holmes <connorholmes@microsoft.com>",['a60e31a7f2f1df9f58fa20c5e2573ec52d3b394d'],False,['softmax_kernels.cu']
cc3d7cb9982b0130c1b9a0bb464cb06a99b000f8,"Skip test_bias_gelu unit test if torch < 1.12 (#2754)

This PR adds a torch version check in the test_bias_gelu unit test to skip if the torch version < 1.12. This is due to gelu implementation differences in versions prior to 1.12.",['0b06e0cbb0f1ec3f34e60136ce7c803df6780cd5'],False,['test_bias_gelu.py']
86477538a62f8b192d3c4978a8da747f67f4a3d3,"Fix hardcoded instances to fp16 in optimizer creation log messages to the correct dtype. (#2743)

* Remove hardcoded instances to fp16 in log messages.

* Add model_dtype to print the correct format

* Respond to PR feedback

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['1db4ade30b207b530f41158ed2114e5b8cf81612'],False,['engine.py']
d923f7c895cfe071fffd760af04a6773d702d8a5,"Refactor/Pydantify monitoring config (#2640)

* pydantify monitoring configs

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['86477538a62f8b192d3c4978a8da747f67f4a3d3'],False,"['config.py', 'constants.py', 'csv_monitor.py', 'monitor.py', 'tensorboard.py', 'wandb.py', 'config.py', 'index.rst', 'monitor.rst', 'test_monitor.py']"
ef6a958e70fe0106afbff9c2c885878cc659f4ac,"Fix for diffusers v0.12.0 (#2753)

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['02e95e6ab47b5a8a0f2b7e9aef6a6ad8cedd1dc7'],False,"['vae.py', 'diffusers_transformer_block.py']"
e2a31d800be31e94e22d516b9ff94ec1177d16fb,"some fix in flops_profiler (#2068)

* bugs in profiler:
1. Tensor.bmm missed in _patch_tensor_methods function
2. missed funtions in _reload_functionals and _reload_tensor_methods functions
3. torch.mm and torch.Tensor.mm will have same __name__ in wrapFunc, my suggustion is use __str__ instead.

* formatting

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Cheng Li <pistasable@gmail.com>",['ef6a958e70fe0106afbff9c2c885878cc659f4ac'],False,['profiler.py']
b5750b64497cacbaf469f02db9cd76d9eda495c9,"fix upsample flops compute by skipping unused kargs (#2773)

* fix upsample flops compute by skipping unused kargs

* fix format",['e2a31d800be31e94e22d516b9ff94ec1177d16fb'],False,['profiler.py']
c5b983e92ec71d0bbd1deb110537db4eafbe1fb5,Fix broken kernel inject bug (#2776),['b5750b64497cacbaf469f02db9cd76d9eda495c9'],False,['engine.py']
2c6e819450282e7509d8ac55682edd1336026e72,"Fix Checkpoint-loading with Meta-tensor (#2781)

* Reset KV-cache at the beginning of text-generation

* Pass the ckpt-loading arguments to work with meta-tensor

* remove unrelated changes",['c5b983e92ec71d0bbd1deb110537db4eafbe1fb5'],False,"['bloom.py', 'gptneox.py', 'opt.py']"
4079077c2c3ccfe5c501a517743feed6dee8d78d,"add support for hjson config files (#2783)

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['2c6e819450282e7509d8ac55682edd1336026e72'],False,"['config.py', 'test_ds_config_dict.py']"
f376daea5215ce74d7f9e979c41e813c2f6127ee,Fixing broken link to azureml-examples recipes (#2795),['7d9fae4da957778f3578f855c2c8b1ec9e8fa3fe'],False,['README.md']
c9b08888d0c776af9f8054840ce833e18028f03e,"Enable page-locked tensors without CUDA (#2775)

* Enable page-locked memory in cpu only env

* Enable page-locked memory in cpu only env

* Formatting

* Add TODOs; Release page-locked memory

* Update perf microbenchmark; Reduce unit test memory

* Reduce CI mem usage",['d323abd80f62bebb9924bb85feb72b57c25af50d'],False,"['deepspeed_aio_utils.cpp', 'deepspeed_pin_tensor.cpp', 'deepspeed_pin_tensor.h', 'deepspeed_py_aio_handle.cpp', 'deepspeed_py_aio_handle.h', 'py_ds_aio.cpp', 'ds_aio_handle.py', 'test_ds_aio.py', 'async_io.py', 'test_aio.py']"
10f3c301a00984f039f1c5d0ee90134eedac2181,"Add container load checkpoint error reporting + refactor (#2792)

This PR refactors the organization of meta tensor checkpoint loading as follows:

- Move get_param_names() abstract method definition from TransformerPolicy into MetaTensorContainer
- Model-specific get_param_names() definitions moved from policy into model-specific container
- selected_policy_g, megatron_v2_g, and transformer_config_g globals replaced with a single container_g global, since the container will contain all of the information those globals previously captured
- ckpt_load_enabled flag added to containers that's set to False by default in the base.py container and gets set to True when the MetaTensorContainer feature is inherited
- Assertion added to replace_transformer_layer before performing checkpoint loading to check if ckpt_load_enabled ==True, otherwise an error message will be printed saying that the container does not support meta tensor checkpoint loading.

The aim of these changes is to more closely couple meta tensor checkpoint loading code to the MetaTensorContainer and to allow for better error reporting of load checkpoint use on model types that don't support this feature.",['c9b08888d0c776af9f8054840ce833e18028f03e'],False,"['base.py', 'bert.py', 'bloom.py', 'clip.py', 'distil_bert.py', 'meta_tensor.py', 'gpt2.py', 'gptj.py', 'gptneo.py', 'gptneox.py', 'megatron_gpt.py', 'megatron_gpt_moe.py', 'opt.py', 'policy.py', 'replace_module.py']"
4af1f76a99f6807b396f1f55727a3065fea48e7a,"Add user defined launcher args for PDSH launcher (#2804)

* Add user defined launcher args for PDSH launcher

* Formatting fixes",['10f3c301a00984f039f1c5d0ee90134eedac2181'],False,['multinode_runner.py']
d038dbd268c3f81aaf8a650f599f7102a0541592,"Fix Slurm launcher user args (#2806)

Fix missing connections from --launcher_args to Slurm srun command.",['4af1f76a99f6807b396f1f55727a3065fea48e7a'],False,['multinode_runner.py']
93e528979365be809f47540f56037f99581f2911,Fix inference CI device error (#2824),['349f845b838c6992c5ea19e80fc728bef9645962'],False,['test_inference.py']
d3f30199e10bf3c03adbdbc11cfad2d9fe86b637,"Fix permissions issue with pip upgrade (#2823)

* fix permissions issue with pip upgrade

* install to .local instead of use sudo

* upgrade pip in venv

* Update action.yml

* fix typos",['93e528979365be809f47540f56037f99581f2911'],False,['action.yml']
639aa7b302f1b39887d9d41f11812bce219dd4d5,"Fix cpu-only CI hangs (#2825)

* don't run tests in parallel

* make AsyncIO test sequential",['d3f30199e10bf3c03adbdbc11cfad2d9fe86b637'],False,['test_aio.py']
cc1054d9e2051356ab9562158ec5b4d1e77063cd,"Fix Pipeline Parallel resize unit test (#2833)

* fix overlapping checkpoint names in unit tests

* remove running cpu-only on master merge",['639aa7b302f1b39887d9d41f11812bce219dd4d5'],False,"['nv-torch-latest-cpu.yml', 'test_configurable_parallel_pp.py']"
46784cb58edf7bbe9b6bbec95212de7b81e55b01,"Fix auto TP for duplicate modules with different gems (#2784)

* Fix auto TP for duplicate modules with different gems

* precommit and comments

* Comment

* Combine gem list of same named modules

* remove duplicates from gem_list before updating policy

* Add module attribute with name variation for ProphetNet

---------

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['cc1054d9e2051356ab9562158ec5b4d1e77063cd'],False,"['auto_tp.py', 'replace_module.py']"
e4b3b610badade98cb3f59a223e406c13e7fe67c,"Refactor DS inference API. No longer need replace_method. (#2831)

Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>",['46784cb58edf7bbe9b6bbec95212de7b81e55b01'],False,"['bert-bench.py', 'gpt-bench.py', 'config.py', 'engine.py', 'config_utils.py', 'inference-tutorial.md', 'mixture-of-experts-inference.md', 'inference-init.rst', 'test_checkpoint_sharding.py', 'test_inference.py', 'test_model_profiling.py']"
fd1449c766f8dc0b0d77ef6389934d094b60a889,"Port Reza's INT8-quantization fix to container architecture (#2725)

Co-authored-by: Reza Yazdani <reyazda@microsoft.com>
Co-authored-by: Reza Yazdani <44502768+RezaYazdaniAminabadi@users.noreply.github.com>
Co-authored-by: Heyang Qin <heyangqin@microsoft.com>
Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>",['e4b3b610badade98cb3f59a223e406c13e7fe67c'],False,"['pt_binding.cpp', 'bloom.py', 'meta_tensor.py', 'gptj.py', 'gptneo.py', 'gptneox.py', 'opt.py', 'load_checkpoint.py', 'policy.py', 'replace_module.py']"
5b7413a4fcc55f5e9e789fb58b88a6ce00270e47,"Fix gpt-Neox rotary embedding implementation (#2782)

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['fd1449c766f8dc0b0d77ef6389934d094b60a889'],False,['apply_rotary_pos_emb.cu']
6c85fe6cc3392aa55f834cdfe02a265f8d43e197,fix for cpu-only tests (#2849),['5b7413a4fcc55f5e9e789fb58b88a6ce00270e47'],False,['action.yml']
8be8c012c8a247e5512276676b5e7092d88633eb,fix typo in autosync workflow (#2850),['3ea272e646978af53127906b97bcd950f5cb866a'],False,['auto-sync.yml']
b4d40e357b9be06c82db5533564f6aee63f73968,Fix example command when building  wheel with dev version specified (#2815),['8be8c012c8a247e5512276676b5e7092d88633eb'],False,['setup.py']
7c99def0f0f915b4411a4e3fb2b2d54b98890cff,"Data efficiency library update (#2866)

* data efficiency library update

* data efficiency library update

* data efficiency update

* data efficiency update",['3253e7db835e7de5b7aae928974213eb98f9166c'],False,"['data_analyzer.py', 'dataloader.py', 'engine.py', '2022-12-12-data-efficiency.md', 'curriculum-learning.md', 'data-efficiency.md', 'test_data_efficiency.py']"
d3de7375500c42427792f86e818107c427191a29,"Remove deprecated `torch._six` imports (#2863)

* Remove deprecated `torch._six` imports

Closes #2845.

* Support older versions of PyTorch as well.

---------

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['b47c592ae23956f59269785e49b11ae46569296c'],False,"['utils.py', 'stage3.py', 'stage_1_and_2.py']"
8710f0514e0acf947c8942791f8051087e39bce0,Reduce I/O size (#2814),['d3de7375500c42427792f86e818107c427191a29'],False,['test_aio.py']
da84e60d98d2e90f6f2094a219c98c8b41582eb9,"add missing license info to top of all source code (#2889)

Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>
Co-authored-by: Conglong Li <conglong.li@gmail.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['8710f0514e0acf947c8942791f8051087e39bce0'],False,"['.pre-commit-config.yaml', '__init__.py', 'abstract_accelerator.py', 'cuda_accelerator.py', 'real_accelerator.py', '__init__.py', '__init__.py', 'all_gather.py', 'all_reduce.py', 'all_to_all.py', 'broadcast.py', 'constants.py', 'pt2pt.py', 'run_all.py', 'utils.py', 'bert-bench.py', 'collect_results.py', 'gpt-bench.py', 'perf_sweep_utils.py', 'custom_cuda_kernel.cu', 'StopWatch.h', 'Timer.h', 'context.h', 'cpu_adagrad.h', 'cpu_adam.h', 'cublas_wrappers.h', 'dropout.h', 'ds_transformer_cuda.h', 'feed_forward.h', 'gelu.h', 'gemm_test.h', 'general_kernels.h', 'normalize_layer.h', 'quantization.h', 'quantizer.h', 'simd.h', 'softmax.h', 'strided_batch_gemm.h', 'type_shim.h', 'fake_quantizer.cu', 'cublas_wrappers.cu', 'dropout_kernels.cu', 'gelu_kernels.cu', 'general_kernels.cu', 'normalize_kernels.cu', 'softmax_kernels.cu', 'transform_kernels.cu', '__init__.py', 'autotuner.py', 'config.py', 'constants.py', 'scheduler.py', '__init__.py', 'base_tuner.py', 'cost_model.py', 'index_based_tuner.py', 'model_based_tuner.py', 'utils.py', 'utils.py', '__init__.py', 'constants.py', 'deepspeed_checkpoint.py', 'reshape_3d_utils.py', 'reshape_meg_2d.py', 'reshape_utils.py', 'utils.py', 'zero_checkpoint.py', '__init__.py', 'backend.py', 'config.py', 'constants.py', 'utils.py', '__init__.py', 'basic_layer.py', 'compress.py', 'config.py', 'constants.py', 'helper.py', 'scheduler.py', 'utils.py', '__init__.py', 'elastic_agent.py', 'utils.py', 'env_report.py', 'git_version_info.py', '__init__.py', 'config.py', '__init__.py', 'multinode_runner.py', '__init__.py', '__init__.py', '__init__.py', 'ds_base.py', '__init__.py', 'auto_tp.py', '__init__.py', 'base.py', 'base_moe.py', 'bert.py', 'bloom.py', 'clip.py', 'distil_bert.py', '__init__.py', 'megatron.py', 'meta_tensor.py', 'gpt2.py', 'gptj.py', 'gptneo.py', 'gptneox.py', 'megatron_gpt.py', 'megatron_gpt_moe.py', 'opt.py', 'inject.py', 'layers.py', 'load_checkpoint.py', 'module_quantize.py', 'replace_module.py', 'utils.py', '__init__.py', 'utils.py', '__init__.py', 'config.py', 'csv_monitor.py', 'monitor.py', 'tensorboard.py', 'utils.py', 'wandb.py', '__init__.py', 'config.py', 'constants.py', '__init__.py', '__init__.py', '__init__.py', '__init__.py', '__init__.py', '__init__.py', '__init__.py', 'matmul.py', 'softmax.py', '__init__.py', '__init__.py', '__init__.py', '__init__.py', 'base.py', 'gelu_gemm.py', 'linear.py', 'mlp_gemm.py', 'qkv_gemm.py', 'residual_add.py', 'softmax.py', 'softmax_context.py', 'vector_matmul.py', 'triton_ops.py', '__init__.py', '__init__.py', 'config.py', 'constants.py', '__init__.py', 'profiler.py', '__init__.py', '__init__.py', 'checkpointing.py', 'config.py', '__init__.py', 'checkpoint_engine.py', 'nebula_checkpoint_engine.py', 'torch_checkpoint_engine.py', '__init__.py', 'coalesced_collectives.py', '__init__.py', 'config.py', 'config_utils.py', 'constants.py', '__init__.py', '__init__.py', '__init__.py', 'eigenvalue.py', '__init__.py', '__init__.py', '__init__.py', 'module.py', 'schedule.py', 'progressive_layer_drop.py', 'quantize.py', 'weight_quantizer.py', 'config.py', 'contiguous_memory_allocator.py', 'linear.py', 'offload_config.py', 'test.py', 'tiling.py', 'utils.py', '__init__.py', 'comms_logging.py', 'debug.py', 'logging.py', 'nvtx.py', 'types.py', 'zero_to_fp32.py', 'conf.py', 'quantizer.py', 'transformer_inference.py', 'bump_patch_version.py', 'check-license.py', 'check-torchdist.py', 'flatten_bench.py', 'unflatten_bench.py', 'conftest.py', 'test_simple.py', 'test_e2e_squad.py', 'test_mpi_backend.py', 'test_mpi_perf.py', 'test_nccl_backend.py', 'test_nccl_perf.py', 'adagrad_test.py', 'adam_test.py', 'adam_test1.py', 'stage3_test.py', 'test.py', 'test_model.py', '__init__.py', 'alexnet_model.py', 'test_autotuning.py', 'common.py', 'test_latest_checkpoint.py', 'test_lr_scheduler.py', 'test_moe_checkpoint.py', 'test_other_optimizer.py', 'test_pipeline.py', 'test_reshape_checkpoint.py', 'test_sparse.py', 'test_tag_validation.py', 'test_zero_optimizer.py', 'test_dist.py', 'common.py', 'test_compression.py', 'test_elastic.py', 'test_checkpoint_sharding.py', 'test_inference.py', 'test_inference_config.py', 'test_model_profiling.py', 'test_ds_arguments.py', 'test_multinode_runner.py', 'test_run.py', 'megatron_model.py', 'test_configurable_parallel_mp.py', 'test_configurable_parallel_pp.py', 'modeling.py', 'modelingpreln.py', 'test_moe.py', 'test_moe_tp.py', 'test_monitor.py', 'multi_output_model.py', 'test_cpu_adagrad.py', 'test_adamw.py', 'test_cpu_adam.py', 'test_aio.py', 'test_cuda_backward.py', 'test_cuda_forward.py', 'test_fake_quantization.py', 'test_sparse_attention.py', 'test_bias_add.py', 'test_pipe_module.py', 'test_flops_profiler.py', 'test_activation_checkpointing.py', 'test_coalesced_collectives.py', 'test_onebit.py', 'test_bf16.py', 'test_dynamic_loss_scale.py', 'test_fp16.py', 'test_pipe.py', 'test_pipe_schedule.py', 'test_topology.py', 'test_averaging_sparse_gradients.py', 'test_csr.py', 'test_sparse_grads.py', 'test_autocast.py', 'test_data.py', 'test_data_efficiency.py', 'test_ds_config_dict.py', 'test_ds_config_model.py', 'test_ds_initialize.py', 'test_lr_schedulers.py', 'test_multi_output_model.py', 'test_pld.py', 'test_runtime_utils.py', 'test_partition.py', 'test_ignore_unused_parameters.py', 'test_zero.py', 'test_zero_config.py', 'test_zero_context.py', 'test_zero_context_ancestry.py', 'test_zero_context_return.py', 'test_zero_tiled.py', 'utils.py', 'simple_model.py', 'util.py', 'test_get_optim_files.py', 'test_groups.py', 'test_init_on_device.py']"
541e423ae652e217f565f4c41daa4619cb526642,"Enable tensor fragments for zero 2 & 3 (#2727)

* Enable tensor fragments for zero 2

* Update deepspeed/utils/tensor_fragment.py

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>

* Update deepspeed/utils/tensor_fragment.py

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>

* Support offload

* Support multi-gpu

* Cleanup

* WIP

* Update deepspeed/runtime/zero/stage3.py

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>

* Support padding

* Update deepspeed/runtime/zero/stage3.py

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>

* z3 optimizer state support; aligned api

* Support frozen z3 params

* Unit tests

* Check NVMe offload capability

* Formatting

* Docs

* More docs

* More docs

* Update docs/code-docs/source/zero3.rst

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>

* More docs

* Update docs/code-docs/source/zero3.rst

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>

* More docs

* More docs

* Update docs/code-docs/source/zero3.rst

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>

* Update deepspeed/utils/tensor_fragment.py

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>

* More docs

* Support unsharded fp32 grad

* Remove debug prints

* Fix off-by-one detection of empty grads

* Update deepspeed/utils/tensor_fragment.py

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>

* Update deepspeed/utils/tensor_fragment.py

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>

* Update deepspeed/utils/tensor_fragment.py

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>

* Update deepspeed/runtime/zero/stage3.py

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>

* Fix off-by-one error

* Skip ranks with no gradient data

* Formatting

* Add license

* Fix license

---------

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>
Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>",['da84e60d98d2e90f6f2094a219c98c8b41582eb9'],False,"['bf16_optimizer.py', 'stage3.py', 'stage_1_and_2.py', '__init__.py', 'mixed_precision_linkage.py', 'tensor_fragment.py', 'zero_to_fp32.py', 'zero3.rst', 'test_zero_tensor_fragment.py', 'util.py']"
9886d6d9e06e414ceab12ca0279561fb42470ec0,"Fix CPUAdam for when `vendor_id_raw` is not provided (#2836)

* #1213: Fix CPUAdam for when `vendor_id_raw` is not provided

* formatting (yapf) fix

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['dc01cee5cab07562bb48740b112f5cb23a90877c'],False,['cpu_adam.py']
91d7090e4753cd1aac2b105629f7373a1e201ec5,"Fixes `AttributeError` in #2853 (#2854)

Updates `deepspeed/monitor/monitor.py`
to instantiate objects with correct configs

Relevant issue:
https://github.com/microsoft/DeepSpeed/issues/2853

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['17fa0876adcc8fa2acb384727bdc49d7434f1f99'],False,['monitor.py']
8d53ac0cd3a708d1b9a4281b5e01044b5bab6d61,"Add MPICH Multinode Runner (#2839)

* MPICH support

* MPICH changes

* MPICH changes

* MPICH changes

* MPICH changes

* accelerator runtime modifications

* Accelerator runtime changes

* Accelerator runtime modifications

* Remove redundant print from single node

* Move hostfile to tmp

* Code cleanup for MPICH class

* Code cleanup, rm whitespace

* Removing mpiexec environment check details

* Not needed tmp hostfile as pass directly

* Remove debugging comments

* rm print statement

* Revert comm changes as WA not needed

* Use MPICHRunner name for class

* Use MPICHRunner as class name

* No need to use args.force_multi and args.launcher .

This should be set in deepspeedexamples gpt-3.6b .sh script as:
$launcher=MPICH
run_cmd="" deepspeed  --hostfile=${hostfile_ds}  --num_nodes ${NUM_WORKERS} --num_gpus ${NUM_GPUS_PER_WORKER} --launcher=${launcher} --force_multi pretrain_gpt2.py $@ ${gpt_options}""

* Adhere to code pattern

* Rm empty lines in MPICHRunner class

* Uncomment check for num nodes and workers when used hostfile_deepspeed in gpt-3.6b.sh

* pass MPICH hostfile through launcher_args in gpt-3.6b.sh

* Clean code and remove args hostfile

* fix merge

* fix merge

---------

Co-authored-by: Abhilash Majumder <30946547+abhilash1910@users.noreply.github.com>

* clean up and fix format

* add ut

---------

Co-authored-by: Abhilash Majumder <30946547+abhilash1910@users.noreply.github.com>
Co-authored-by: Ammar Ahmad Awan <ammar.awan@microsoft.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['91d7090e4753cd1aac2b105629f7373a1e201ec5'],False,"['constants.py', 'multinode_runner.py', 'runner.py', 'test_multinode_runner.py']"
80d8fcbdb3f1121bef358dac70476648a3c8ca55,"Improve overflow handling (#2944)

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['87eaf8f99abdebac1207565e3c3ee49cf151f29b'],False,"['loss_scaler.py', 'stage3.py', 'stage_1_and_2.py']"
0acf7e9c489def74d3ac05ee91eabfb8280a1dba,"[RFC] add device abstraction to allow other device than CUDA be used (#2221)

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['80d8fcbdb3f1121bef358dac70476648a3c8ca55'],False,"['all_gather.py', 'all_reduce.py', 'all_to_all.py', 'broadcast.py', 'constants.py', 'pt2pt.py', 'utils.py', 'bert-bench.py', 'gpt-bench.py', 'base.py', 'base_moe.py', 'policy.py', 'cifar-10.md', 'ds_config.json', 'test_ds_init.py', 'flatten_bench.py', 'unflatten_bench.py', 'test_mpi_backend.py', 'test_mpi_perf.py', 'test_nccl_backend.py', 'test_nccl_perf.py', 'adam_test1.py', 'test.py', 'alexnet_model.py', 'test_dist.py', 'common.py', 'test_compression.py', 'test_inference.py', 'test_model_profiling.py', 'megatron_model.py', 'test_configurable_parallel_mp.py', 'test_configurable_parallel_pp.py', 'modeling.py', 'modelingpreln.py', 'test_accelerator_backward.py', 'test_accelerator_forward.py', 'test_cpu_adagrad.py', 'test_cpu_adam.py', 'test_aio.py', 'test_dequantize.py', 'test_fake_quantization.py', 'test_quantize.py', 'test_sparse_attention.py', 'test_nhwc_bias_add.py', 'test_bias_add.py', 'test_bias_geglu.py', 'test_bias_gelu.py', 'test_bias_relu.py', 'test_layer_norm.py', 'test_moe_res_matmult.py', 'test_residual_add.py', 'test_pipe_module.py', 'test_activation_checkpointing.py', 'test_coalesced_collectives.py', 'test_onebit.py', 'test_fp16.py', 'test_topology.py', 'test_autocast.py', 'test_data.py', 'test_data_efficiency.py', 'test_ds_config_dict.py', 'test_runtime_utils.py', 'test_partition.py', 'test_zero.py', 'test_zero_context_ancestry.py', 'simple_model.py', 'test_init_on_device.py']"
6379defaef5e9dd2c9ce691c34ac87c5326b936a,"bug fix for skipping mbs (#2171)

Co-authored-by: Rajhans Samdani <rajhans@gmail.com>",['d58b4df39f90972f4b035bba56def0838dbc856d'],False,['autotuner.py']
58a4a4d4c19bda86d489ac171fa10f3ddb27c9d6,"Fix issue between our abstract accelerator and colossalai's version of op_builder (#2963)

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['6379defaef5e9dd2c9ce691c34ac87c5326b936a'],False,"['cuda_accelerator.py', '__init__.py']"
457850dc5ad72960f0e8a8f1597914d682a7792c,[zero] prevent poor configs from running w. zero-offload (#2971),['58a4a4d4c19bda86d489ac171fa10f3ddb27c9d6'],False,"['config.py', 'constants.py', 'engine.py', 'test_fp16.py', 'test_zero.py']"
3798e60519ae10573c765c9ebb1e460da9e596df,"Fix Meta Tensor checkpoint load for OPT models (#2990)

This PR fixes Meta Tensor checkpoint loading for OPT models where the SD keys start with `model.`.",['457850dc5ad72960f0e8a8f1597914d682a7792c'],False,['load_checkpoint.py']
b528f50e3d750ff5b1b7c8beb64a838563ec8f58,"Fix buffer size for pipeline parallel and communication schedule (#2862)

* fix buffer size for pipeline parallel (#2800)

* improve explanation of buffer size for pipeline parallelism

Co-authored-by: Jae-Won Chung <jwnchung@umich.edu>

* fix format of comment

---------

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>
Co-authored-by: Jae-Won Chung <jwnchung@umich.edu>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['43d58d99eba800a809f16efa30a90eb25f7a7003'],False,['schedule.py']
ac2c9ffae4b1b46552a2a535befbe8fd040799b8,"Improve loss overflow logs (#3008)

* Improve overflow logs

* Trigger CI

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['94f7da26b632f53b0a29ac54854cb8899e0d5b9e'],False,"['loss_scaler.py', 'stage3.py', 'stage_1_and_2.py']"
f1e4fb0b878e48d6fe33dbb2e0969d791d698e86,Fix Broken Links (#3048),['bbfd0a6a3e7bd43d816b250b4cecb67504c0c340'],False,"['training.md', '2020-05-19-bert-record.md', '2020-09-08-sparse-attention-news.md', '2020-09-09-onebit-adam-news.md', 'bert-finetuning.md', 'bert-pretraining.md', 'inference-tutorial.md', 'mixture-of-experts.md', 'onebit-adam.md', 'progressive_layer_dropping.md', 'sparse-attention.md', 'transformer_kernel.md']"
4e0686233a330af75cdc744371e18f9958a938ca,"Several fixes to unblock CI (#3047)

Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['b38b3036dd531ec60e180f170bfbe5d7402e9f8a'],False,"['amd.yml', 'nv-accelerate-v100.yml', 'nv-inference.yml', 'nv-lightning-v100.yml', 'nv-megatron.yml', 'nv-mii.yml', 'nv-nightly.yml', 'nv-torch-latest-cpu.yml', 'nv-torch-latest-v100.yml', 'nv-torch-nightly-v100.yml', 'nv-torch18-p40.yml', 'nv-torch18-v100.yml', 'pre-compile-ops.yml', 'conftest.py', 'test_simple.py', 'test_pipeline.py', 'test_compression.py', 'test_configurable_parallel_mp.py', 'test_configurable_parallel_pp.py', 'test_pipe.py']"
36677588b63096c8004688bab00834c0462088c7,[CI] follow-up fixes (#3072),['9ea0fdc2ce86bd97b2fb4d118f3af1165a6e7d0e'],False,"['CODEOWNERS', 'test_pipeline.py', 'test_pipe.py', 'test_averaging_sparse_gradients.py', 'util.py']"
871c8a3f5d2e167cbebca4eedad6d1f7adfd047c,"fix return prev key and value , added strides to from_blob (#2828)

Co-authored-by: Mor Zusman <morz@ai21.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['36677588b63096c8004688bab00834c0462088c7'],False,['pt_binding.cpp']
a78d6b89e046dde0ece45acaeaaac4ab2a3a2d86,"Fix nebula in save_16bit_model issue (#3023)

Co-authored-by: Qinghuan Rao <qinghuanrao@microsoft.com>",['1286e374aba8b4490a7f57f18b3a479753985277'],False,['engine.py']
5cdf35935dd96b94938e0a917ca39b91b5eb07cf,"Goodbye Torch 1.8 (#3082)

* bump torch18 -> torch19
* fix gptj

---------

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['5c2a81c2c1be8b6308781c01226d56df3ead2c1f'],False,"['nv-torch19-p40.yml', 'nv-torch19-v100.yml', 'nv-transformers-v100.yml', 'README.md', 'ds_transformer.py']"
e80ae088868bef18b75a0d0825e15d7f9ea0ad0c,"Empty ZeRO3 partition cache (#3060)

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['5cdf35935dd96b94938e0a917ca39b91b5eb07cf'],False,"['engine.py', 'parameter_offload.py', 'stage3.py', 'zero3.rst', 'test_zero.py']"
b3ec1c9712e1f954288811f78f2340d69abe84d1,"Move cuda check into utils (#3074)

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['090d49e79fef300046ec0ca22dc3e1bffde74ee1'],False,"['test_sparse_attention.py', 'util.py']"
91d63e0228c29d0f8b25ea3bc7173acdcdddd136,update formatter version and style settings (#3098),['b3ec1c9712e1f954288811f78f2340d69abe84d1'],False,"['.pre-commit-config.yaml', '.style.yapf', 'abstract_accelerator.py', 'cuda_accelerator.py', 'real_accelerator.py', 'all_gather.py', 'all_reduce.py', 'all_to_all.py', 'broadcast.py', 'pt2pt.py', 'utils.py', 'collect_results.py', 'gpt-bench.py', 'ds_elastic', 'aio_bench_generate_param.py', 'aio_bench_perf_sweep.py', 'ds_aio_basic.py', 'ds_aio_handle.py', 'parse_aio_stats.py', 'test_ds_aio.py', '__init__.py', 'autotuner.py', 'config.py', 'constants.py', 'scheduler.py', 'base_tuner.py', 'cost_model.py', 'index_based_tuner.py', 'model_based_tuner.py', 'utils.py', 'utils.py', '__init__.py', 'deepspeed_checkpoint.py', 'reshape_3d_utils.py', 'reshape_meg_2d.py', 'reshape_utils.py', 'universal_checkpoint.py', 'utils.py', 'zero_checkpoint.py', '__init__.py', 'backend.py', 'comm.py', 'config.py', 'torch.py', 'utils.py', 'basic_layer.py', 'compress.py', 'config.py', 'helper.py', 'scheduler.py', 'utils.py', 'config.py', 'elastic_agent.py', 'elasticity.py', 'env_report.py', 'config.py', 'engine.py', 'launch.py', 'multinode_runner.py', 'runner.py', 'unet.py', 'vae.py', 'cuda_graph.py', 'clip_encoder.py', 'ds_base.py', 'ds_bert.py', 'ds_bloom.py', 'ds_gpt.py', 'ds_megatron_gpt.py', 'ds_opt.py', 'ds_transformer.py', 'auto_tp.py', 'base.py', 'base_moe.py', 'bert.py', 'bloom.py', 'clip.py', 'distil_bert.py', 'megatron.py', 'meta_tensor.py', 'gpt2.py', 'gptj.py', 'gptneo.py', 'gptneox.py', 'megatron_gpt.py', 'megatron_gpt_moe.py', 'opt.py', 'unet.py', 'vae.py', 'inject.py', 'layers.py', 'load_checkpoint.py', 'module_quantize.py', 'policy.py', 'replace_module.py', 'replace_policy.py', 'experts.py', 'layer.py', 'mappings.py', 'sharded_moe.py', 'utils.py', 'config.py', 'csv_monitor.py', 'monitor.py', 'tensorboard.py', 'wandb.py', 'config.py', 'constants.py', 'cpu_adagrad.py', 'cpu_adam.py', 'fused_adam.py', 'multi_tensor_apply.py', 'fused_lamb.py', 'dropping_utils.py', 'bert_sparse_self_attention.py', 'matmul.py', 'softmax.py', 'sparse_attention_utils.py', 'sparse_self_attention.py', 'sparsity_config.py', 'bias_add.py', 'config.py', 'diffusers_2d_transformer.py', 'diffusers_attention.py', 'diffusers_transformer_block.py', 'ds_attention.py', 'ds_mlp.py', 'moe_inference.py', 'gelu_gemm.py', 'linear.py', 'mlp_gemm.py', 'qkv_gemm.py', 'residual_add.py', 'softmax.py', 'softmax_context.py', 'vector_matmul.py', 'triton_ops.py', 'transformer.py', 'config.py', 'profiler.py', 'checkpointing.py', 'config.py', 'bf16_optimizer.py', 'nebula_checkpoint_engine.py', 'torch_checkpoint_engine.py', 'coalesced_collectives.py', 'mpi.py', 'nccl.py', 'cupy.py', 'config.py', 'config_utils.py', 'constants.py', 'config.py', 'curriculum_scheduler.py', 'basic_layer.py', 'scheduler.py', 'utils.py', 'data_analyzer.py', 'data_sampler.py', 'indexed_dataset.py', 'utils.py', 'dataloader.py', 'eigenvalue.py', 'engine.py', 'fused_optimizer.py', 'loss_scaler.py', 'adam.py', 'lamb.py', 'zoadam.py', 'unfused_optimizer.py', 'lr_schedules.py', 'engine.py', 'module.py', 'p2p.py', 'schedule.py', 'topology.py', 'progressive_layer_drop.py', 'quantize.py', 'sparse_tensor.py', 'state_dict_factory.py', 'aio_config.py', 'async_swapper.py', 'optimizer_utils.py', 'partitioned_optimizer_swapper.py', 'partitioned_param_swapper.py', 'pipelined_optimizer_swapper.py', 'utils.py', 'utils.py', 'weight_quantizer.py', 'config.py', 'contiguous_memory_allocator.py', 'linear.py', 'offload_config.py', 'parameter_offload.py', 'partition_parameters.py', 'partitioned_param_coordinator.py', 'stage3.py', 'stage_1_and_2.py', 'tiling.py', 'utils.py', 'comms_logging.py', 'debug.py', 'groups.py', 'init_on_device.py', 'logging.py', 'mixed_precision_linkage.py', 'nvtx.py', 'tensor_fragment.py', 'timer.py', 'zero_to_fp32.py', '__init__.py', 'async_io.py', 'builder.py', 'cpu_adagrad.py', 'cpu_adam.py', 'fused_adam.py', 'fused_lamb.py', 'random_ltd.py', 'sparse_attn.py', 'spatial_inference.py', 'transformer.py', 'transformer_inference.py', 'check-license.py', 'check-torchcuda.py', 'check-torchdist.py', 'setup.py', 'test_ds_init.py', 'flatten_bench.py', 'unflatten_bench.py', 'conftest.py', 'test_simple.py', 'BingBertSquad_run_func_test.py', 'BingBertSquad_test_common.py', 'test_e2e_squad.py', 'run_checkpoint_test.py', 'run_func_test.py', 'run_perf_baseline.py', 'run_perf_test.py', 'test_common.py', 'test_mpi_backend.py', 'test_nccl_backend.py', 'test_nccl_perf.py', 'adam_test1.py', 'stage3_test.py', 'test.py', 'test_model.py', 'alexnet_model.py', 'test_autotuning.py', 'common.py', 'test_latest_checkpoint.py', 'test_lr_scheduler.py', 'test_moe_checkpoint.py', 'test_other_optimizer.py', 'test_sparse.py', 'test_tag_validation.py', 'test_zero_optimizer.py', 'test_dist.py', 'common.py', 'test_compression.py', 'test_elastic.py', 'test_checkpoint_sharding.py', 'test_inference.py', 'test_model_profiling.py', 'test_ds_arguments.py', 'megatron_model.py', 'test_configurable_parallel_mp.py', 'test_configurable_parallel_pp.py', 'modeling.py', 'modelingpreln.py', 'test_moe.py', 'test_moe_tp.py', 'multi_output_model.py', 'test_accelerator_backward.py', 'test_accelerator_forward.py', 'test_cpu_adagrad.py', 'test_cpu_adam.py', 'test_aio.py', 'test_dequantize.py', 'test_fake_quantization.py', 'test_quantize.py', 'test_sparse_attention.py', 'test_nhwc_bias_add.py', 'test_bias_add.py', 'test_bias_geglu.py', 'test_bias_gelu.py', 'test_bias_relu.py', 'test_layer_norm.py', 'test_moe_res_matmult.py', 'test_residual_add.py', 'test_pipe_module.py', 'test_flops_profiler.py', 'test_activation_checkpointing.py', 'test_coalesced_collectives.py', 'test_onebit.py', 'test_bf16.py', 'test_dynamic_loss_scale.py', 'test_fp16.py', 'test_pipe.py', 'test_pipe_schedule.py', 'test_topology.py', 'test_averaging_sparse_gradients.py', 'test_sparse_grads.py', 'test_autocast.py', 'test_data.py', 'test_data_efficiency.py', 'test_ds_config_dict.py', 'test_ds_config_model.py', 'test_ds_initialize.py', 'test_lr_schedulers.py', 'test_multi_output_model.py', 'test_pld.py', 'test_runtime_utils.py', 'test_partition.py', 'test_ignore_unused_parameters.py', 'test_zero.py', 'test_zero_config.py', 'test_zero_context.py', 'test_zero_context_ancestry.py', 'test_zero_context_return.py', 'test_zero_tensor_fragment.py', 'test_zero_tiled.py', 'simple_model.py', 'util.py', 'test_groups.py', 'test_init_on_device.py']"
9726bd46500ddabff5a2807be27a86466cacef6d,"Fix comms benchmark import issues and support MPI/slurm launching (#2932)

* Fix benchmark import issues and support MPI launching with pure torch.dist

* Formatting

* Update comms benchmark README

* Formatting

* Added better error handling and support MPI torch.dist backend

* Update formatting versions

* Formatting again

* Trigger CI

---------

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['91d63e0228c29d0f8b25ea3bc7173acdcdddd136'],False,"['README.md', 'all_gather.py', 'all_reduce.py', 'all_to_all.py', 'broadcast.py', 'constants.py', 'pt2pt.py', 'run_all.py', 'utils.py']"
a6317eb509cf6a7a3929787530174f7c575cf475,"♻️ replace deprecated functions for communication (#2995)

* :hankey: drop dead code

* :recycle: replace has_all_gather_base with has_all_gather_into_tensor

* :recycle: remove deprecated _all_gather_base

* :recycle: remove deprecated _reduce_scatter_base

* :art: reformat files

* :wrench: fix _six

* Trigger CI

* Trigger CI

* Trigger CI

* :art: formatting

* incorporate suggestion

* incorporate suggestion

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['4b6d7c159ca8664141d06dc29fb7a9e2cefc407c'],False,"['all_gather.py', 'comm.py', 'torch.py', 'utils.py', 'utils.py', 'partition_parameters.py', 'comms_logging.py', 'comms-logging.md']"
261d637086feb8fec1c6507eafc8f8cae4b49a41,"Make fp32 default communication data type (#2970)

* Make fp32 default communication data type

* Fix asserts",['a6317eb509cf6a7a3929787530174f7c575cf475'],False,"['engine.py', 'stage3.py', 'stage_1_and_2.py']"
b361c72761d97f5a1714a3e91d1f7c36fd3cfdd8,"Update DeepSpeed copyright license to Apache 2.0 (#3111)

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['261d637086feb8fec1c6507eafc8f8cae4b49a41'],False,"['.pre-commit-config.yaml', 'LICENSE', '__init__.py', 'abstract_accelerator.py', 'cuda_accelerator.py', 'real_accelerator.py', '__init__.py', '__init__.py', 'all_gather.py', 'all_reduce.py', 'all_to_all.py', 'broadcast.py', 'constants.py', 'pt2pt.py', 'run_all.py', 'utils.py', 'bert-bench.py', 'collect_results.py', 'gpt-bench.py', 'cpu_adagrad.cpp', 'cpu_adam.cpp', 'fused_adam_frontend.cpp', 'multi_tensor_adam.cu', 'multi_tensor_apply.cuh', 'deepspeed_aio_common.cpp', 'deepspeed_aio_common.h', 'deepspeed_aio_types.cpp', 'deepspeed_aio_types.h', 'deepspeed_aio_utils.cpp', 'deepspeed_aio_utils.h', 'deepspeed_aio_thread.cpp', 'deepspeed_aio_thread.h', 'deepspeed_pin_tensor.cpp', 'deepspeed_pin_tensor.h', 'deepspeed_py_aio.cpp', 'deepspeed_py_aio.h', 'deepspeed_py_aio_handle.cpp', 'deepspeed_py_aio_handle.h', 'deepspeed_py_copy.cpp', 'deepspeed_py_copy.h', 'py_ds_aio.cpp', 'aio_bench_generate_param.py', 'aio_bench_perf_sweep.py', 'ds_aio_basic.py', 'ds_aio_handle.py', 'parse_aio_stats.py', 'perf_sweep_utils.py', 'test_ds_aio.py', 'test_ds_aio_utils.py', 'validate_async_io.py', 'custom_cuda_kernel.cu', 'StopWatch.h', 'Timer.h', 'compat.h', 'context.h', 'conversion_utils.h', 'cpu_adagrad.h', 'cpu_adam.h', 'cublas_wrappers.h', 'custom_cuda_layers.h', 'dequantization_utils.h', 'dropout.h', 'ds_kernel_utils.h', 'ds_transformer_cuda.h', 'feed_forward.h', 'gelu.h', 'gemm_test.h', 'general_kernels.h', 'memory_access_utils.h', 'normalize_layer.h', 'quantization.h', 'quantization_utils.h', 'quantizer.h', 'reduction_utils.h', 'simd.h', 'softmax.h', 'strided_batch_gemm.h', 'type_shim.h', 'fused_lamb_cuda.cpp', 'fused_lamb_cuda_kernel.cu', 'dequantize.cu', 'fake_quantizer.cu', 'pt_binding.cpp', 'quantize.cu', 'gather_scatter.cu', 'pt_binding.cpp', 'slice_attn_masks.cu', 'token_sort.cu', 'utils.cpp', 'opt_bias_add.cu', 'pt_binding.cpp', 'spatial_cuda_layers.h', 'cublas_wrappers.cu', 'dropout_kernels.cu', 'ds_transformer_cuda.cpp', 'gelu_kernels.cu', 'general_kernels.cu', 'apply_rotary_pos_emb.cu', 'dequantize.cu', 'gelu.cu', 'layer_norm.cu', 'pt_binding.cpp', 'relu.cu', 'softmax.cu', 'transform.cu', 'inference_context.h', 'inference_cublas_wrappers.h', 'inference_cuda_layers.h', 'normalize_kernels.cu', 'softmax_kernels.cu', 'transform_kernels.cu', 'flatten_unflatten.cpp', '__init__.py', '__init__.py', 'autotuner.py', 'config.py', 'constants.py', 'scheduler.py', '__init__.py', 'base_tuner.py', 'cost_model.py', 'index_based_tuner.py', 'model_based_tuner.py', 'utils.py', 'utils.py', '__init__.py', 'constants.py', 'deepspeed_checkpoint.py', 'reshape_3d_utils.py', 'reshape_meg_2d.py', 'reshape_utils.py', 'universal_checkpoint.py', 'utils.py', 'zero_checkpoint.py', '__init__.py', 'backend.py', 'comm.py', 'config.py', 'constants.py', 'torch.py', 'utils.py', '__init__.py', 'basic_layer.py', 'compress.py', 'config.py', 'constants.py', 'helper.py', 'scheduler.py', 'utils.py', 'constants.py', '__init__.py', 'config.py', 'constants.py', 'elastic_agent.py', 'elasticity.py', 'utils.py', 'env_report.py', 'git_version_info.py', '__init__.py', 'config.py', 'engine.py', '__init__.py', 'constants.py', 'launch.py', 'multinode_runner.py', 'runner.py', '__init__.py', '__init__.py', 'unet.py', 'vae.py', '__init__.py', 'cuda_graph.py', '__init__.py', 'clip_encoder.py', 'ds_base.py', 'ds_bert.py', 'ds_bloom.py', 'ds_gpt.py', 'ds_megatron_gpt.py', 'ds_opt.py', 'ds_transformer.py', '__init__.py', 'auto_tp.py', '__init__.py', 'base.py', 'base_moe.py', 'bert.py', 'bloom.py', 'clip.py', 'distil_bert.py', '__init__.py', 'megatron.py', 'meta_tensor.py', 'gpt2.py', 'gptj.py', 'gptneo.py', 'gptneox.py', 'megatron_gpt.py', 'megatron_gpt_moe.py', 'opt.py', 'unet.py', 'vae.py', 'inject.py', 'layers.py', 'load_checkpoint.py', 'module_quantize.py', 'policy.py', 'replace_module.py', 'replace_policy.py', 'utils.py', '__init__.py', 'experts.py', 'layer.py', 'mappings.py', 'sharded_moe.py', 'utils.py', '__init__.py', 'config.py', 'csv_monitor.py', 'monitor.py', 'tensorboard.py', 'utils.py', 'wandb.py', '__init__.py', 'config.py', 'constants.py', '__init__.py', '__init__.py', 'cpu_adagrad.py', '__init__.py', 'cpu_adam.py', 'fused_adam.py', 'multi_tensor_apply.py', '__init__.py', '__init__.py', 'fused_lamb.py', '__init__.py', 'quantizer.py', '__init__.py', 'dropping_utils.py', '__init__.py', 'bert_sparse_self_attention.py', 'matmul.py', 'softmax.py', 'sparse_attention_utils.py', 'sparse_self_attention.py', 'sparsity_config.py', '__init__.py', 'matmul.tr', 'softmax_bwd.tr', 'softmax_fwd.tr', '__init__.py', '__init__.py', 'bias_add.py', 'config.py', 'diffusers_2d_transformer.py', 'diffusers_attention.py', 'diffusers_transformer_block.py', 'ds_attention.py', 'ds_mlp.py', 'moe_inference.py', '__init__.py', 'base.py', 'gelu_gemm.py', 'linear.py', 'mlp_gemm.py', 'qkv_gemm.py', 'residual_add.py', 'softmax.py', 'softmax_context.py', 'vector_matmul.py', 'triton_ops.py', 'transformer.py', '__init__.py', '__init__.py', 'config.py', 'constants.py', '__init__.py', 'profiler.py', '__init__.py', '__init__.py', 'checkpointing.py', 'config.py', 'bf16_optimizer.py', '__init__.py', 'checkpoint_engine.py', 'nebula_checkpoint_engine.py', 'torch_checkpoint_engine.py', '__init__.py', 'coalesced_collectives.py', 'mpi.py', 'nccl.py', '__init__.py', 'cupy.py', 'config.py', 'config_utils.py', 'constants.py', '__init__.py', 'config.py', 'constants.py', 'curriculum_scheduler.py', '__init__.py', 'basic_layer.py', 'helper.py', 'scheduler.py', 'utils.py', '__init__.py', 'data_analyzer.py', 'data_sampler.py', 'indexed_dataset.py', 'utils.py', 'dataloader.py', 'eigenvalue.py', 'engine.py', '__init__.py', 'fused_optimizer.py', 'loss_scaler.py', '__init__.py', 'adam.py', 'lamb.py', 'zoadam.py', 'unfused_optimizer.py', 'lr_schedules.py', '__init__.py', 'engine.py', 'module.py', 'p2p.py', 'schedule.py', 'topology.py', 'progressive_layer_drop.py', 'quantize.py', 'sparse_tensor.py', 'state_dict_factory.py', '__init__.py', 'aio_config.py', 'async_swapper.py', 'constants.py', 'optimizer_utils.py', 'partitioned_optimizer_swapper.py', 'partitioned_param_swapper.py', 'pipelined_optimizer_swapper.py', 'utils.py', 'utils.py', 'weight_quantizer.py', '__init__.py', 'config.py', 'contiguous_memory_allocator.py', 'linear.py', 'offload_config.py', 'parameter_offload.py', 'partition_parameters.py', 'partitioned_param_coordinator.py', 'stage3.py', 'stage_1_and_2.py', 'test.py', 'tiling.py', 'utils.py', '__init__.py', 'comms_logging.py', 'debug.py', 'exceptions.py', 'groups.py', 'init_on_device.py', 'logging.py', 'mixed_precision_linkage.py', 'nvtx.py', 'tensor_fragment.py', 'timer.py', 'types.py', 'zero_to_fp32.py', 'conf.py', '__init__.py', 'all_ops.py', 'async_io.py', 'builder.py', 'cpu_adagrad.py', 'cpu_adam.py', 'fused_adam.py', 'fused_lamb.py', 'quantizer.py', 'random_ltd.py', 'sparse_attn.py', 'spatial_inference.py', 'stochastic_transformer.py', 'transformer.py', 'transformer_inference.py', 'utils.py', 'bump_patch_version.py', 'check-license.py', 'check-torchcuda.py', 'check-torchdist.py', 'setup.py', 'test_ds_init.py', 'flatten_bench.py', 'unflatten_bench.py', 'conftest.py', 'test_simple.py', 'BingBertSquad_run_func_test.py', 'BingBertSquad_test_common.py', '__init__.py', 'test_e2e_squad.py', '__init__.py', 'run_checkpoint_test.py', 'run_func_test.py', 'run_perf_baseline.py', 'run_perf_test.py', 'test_common.py', 'run_sanity_check.py', 'test_mpi_backend.py', 'test_mpi_perf.py', 'test_nccl_backend.py', 'test_nccl_perf.py', 'adagrad_test.py', 'adam_test.py', 'adam_test1.py', 'stage3_test.py', 'test.py', 'test_model.py', '__init__.py', 'alexnet_model.py', 'test_autotuning.py', 'common.py', 'test_latest_checkpoint.py', 'test_lr_scheduler.py', 'test_moe_checkpoint.py', 'test_other_optimizer.py', 'test_pipeline.py', 'test_reshape_checkpoint.py', 'test_sparse.py', 'test_tag_validation.py', 'test_zero_optimizer.py', 'test_dist.py', 'common.py', 'test_compression.py', 'test_elastic.py', 'test_checkpoint_sharding.py', 'test_inference.py', 'test_inference_config.py', 'test_model_profiling.py', 'test_ds_arguments.py', 'test_multinode_runner.py', 'test_run.py', 'megatron_model.py', 'test_configurable_parallel_mp.py', 'test_configurable_parallel_pp.py', 'modeling.py', 'modelingpreln.py', 'test_moe.py', 'test_moe_tp.py', 'test_monitor.py', 'multi_output_model.py', 'test_accelerator_backward.py', 'test_accelerator_forward.py', 'test_cpu_adagrad.py', 'test_adamw.py', 'test_cpu_adam.py', 'test_aio.py', 'test_dequantize.py', 'test_fake_quantization.py', 'test_quantize.py', 'test_sparse_attention.py', 'test_nhwc_bias_add.py', 'test_bias_add.py', 'test_bias_geglu.py', 'test_bias_gelu.py', 'test_bias_relu.py', 'test_layer_norm.py', 'test_moe_res_matmult.py', 'test_residual_add.py', 'test_pipe_module.py', 'test_flops_profiler.py', 'test_activation_checkpointing.py', 'test_coalesced_collectives.py', 'test_onebit.py', 'test_bf16.py', 'test_dynamic_loss_scale.py', 'test_fp16.py', 'test_pipe.py', 'test_pipe_schedule.py', 'test_topology.py', 'test_averaging_sparse_gradients.py', 'test_csr.py', 'test_sparse_grads.py', 'test_autocast.py', 'test_data.py', 'test_data_efficiency.py', 'test_ds_config_dict.py', 'test_ds_config_model.py', 'test_ds_initialize.py', 'test_lr_schedulers.py', 'test_multi_output_model.py', 'test_pld.py', 'test_runtime_utils.py', 'test_partition.py', 'test_ignore_unused_parameters.py', 'test_zero.py', 'test_zero_config.py', 'test_zero_context.py', 'test_zero_context_ancestry.py', 'test_zero_context_return.py', 'test_zero_tensor_fragment.py', 'test_zero_tiled.py', 'utils.py', 'simple_model.py', 'util.py', 'test_get_optim_files.py', 'test_groups.py', 'test_init_on_device.py']"
62db3c0d7f6f6a4056b311848dc198b286f6d7a8,[docs] fix blog links,['1242d8b4ebfb8619a6bb0a1ca069f8de1ff809d8'],False,"['README.md', 'index.md']"
fcb868e27c3993aab8aa151ca579456cb32dcee5,Fix launch issue (#3137),['fcc0d9c0aac0dcb9286b551d97cbea0d2bafe452'],False,['zero_to_fp32.py']
f2c9a827993d5e6e951a8ce920d55de810b2aab2,fix CI badges (#3138),['fcb868e27c3993aab8aa151ca579456cb32dcee5'],False,"['amd.yml', 'formatting.yml', 'nv-accelerate-v100.yml', 'nv-inference.yml', 'nv-lightning-v100.yml', 'nv-megatron.yml', 'nv-mii.yml', 'nv-torch-latest-cpu.yml', 'nv-torch-latest-v100.yml', 'nv-torch19-p40.yml', 'nv-transformers-v100.yml', 'python.yml', 'README.md']"
e73de8cee89ca1b2296c8eead2ae3d904e24271c,"Optimize Softmax Kernel (#3112)

* Simplify kernel

* Coalesce memory attempt 1. Logits divergence.

* Logits fix?

* sync after every global mem access

* template on iterations. Down to 8.3% cuda time for 8k tokens

* Up to 64 iterations

* Add alibi/mask check

* fp32

* Revert builder.py

* naming. precommit

* Revert ""naming. precommit""

This reverts commit 150eb7d96b6084190265b440739317216992bd82.

* naming. spacing

* Spacing. simplify checks

* remove bsyncs

* missed bsyncs

* precommit",['f2c9a827993d5e6e951a8ce920d55de810b2aab2'],False,['softmax.cu']
ab1f32de326d84b3a749ea017f140f97dc315714,"Update skip on torch in tests (#3136)

* Replace old torch version checks with existing function

* Clean up formatting",['20ed15be04530e755858b8256eaa29e6d230ea09'],False,"['nccl.py', 'adam.py', 'lamb.py', 'zoadam.py', 'test_compression.py', 'test_configurable_parallel_mp.py', 'test_configurable_parallel_pp.py', 'test_flops_profiler.py', 'test_onebit.py']"
951d4df74731dfb8553c9b8d87a57e5025d1ee31,"fix example symlink about DeepSpeed+AzureML (#3127)

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['ab1f32de326d84b3a749ea017f140f97dc315714'],False,['README.md']
1ec34e54de6bf07f85802c1ae104a876f6e65cf9,Update megatron GPT2Model,['8630f73e09084cc29a5516656c882dc5174e108c'],False,"['ds_gpt2_test.sh', 'megatron_model.py']"
4b35833379bd9eec6fc148a3fa62bbbac2ba1af7,"Revert ""Update megatron GPT2Model""

This reverts commit 1ec34e54de6bf07f85802c1ae104a876f6e65cf9.",['1ec34e54de6bf07f85802c1ae104a876f6e65cf9'],False,"['ds_gpt2_test.sh', 'megatron_model.py']"
30d97705499779b3be4cae2ca80d919833c6e1f3,"Recover shared parameters (#3033)

* submit changes

* update format

* fix fomrat

* revert

* test

* add top

* treat z1 as z2

* revert

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['4b35833379bd9eec6fc148a3fa62bbbac2ba1af7'],False,['zero_to_fp32.py']
c48c97991bff44917db4b40d4f6b22447a3459cb,"Fix for Diffusers 0.14.0 (#3142)

cross attention kwargs and vae config for diffusers 0.14.0",['30d97705499779b3be4cae2ca80d919833c6e1f3'],False,"['unet.py', 'vae.py']"
1f85569e1c65b6a9da36a3ebfde099deeee8273a,"Fix copyright check, add copyright replace script (#3141)

* fix copyright script and add replace-copyright script",['c48c97991bff44917db4b40d4f6b22447a3459cb'],False,"['check-license.py', 'replace_copyright.py']"
0cd64bd4c965fd6fa5a670ce82f2a900506f9df1,"fixing a bug in CPU Adam and Adagrad (#3109)

Co-authored-by: Bing Xie <bingxie@BINGHYPC014.redmond.corp.microsoft.com>
Co-authored-by: Shaden Smith <Shaden.Smith@microsoft.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['baa95c6256c1064f602049918cb64db3859cc00b'],False,"['cpu_adagrad.cpp', 'cpu_adam.cpp']"
4d27225f3e22c4dd14074152b203659e3c627788,"zero.Init() should pin params in GPU memory as requested (#2953)

* Persist params in zero.Init

* Disable debug prints

* Formatting

* Avoid offloading persisted params

* Simplify world_size=1

* Formatting

* Remove pdb

* Restructure

* Formating

* Formatting

* Apply persistence only if ds_config available

* Fix typo

* add util function for getting pydantic config default values

---------

Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>",['1ab42fe829475ed315b4718ea8906783ebbb2ba1'],False,"['config_utils.py', 'partition_parameters.py']"
47f9f13bd3236d426d558d61ef358b36191ef026,"DeepSpeed Chat (#3186)

Co-authored-by: Reza Yazdani <44502768+RezaYazdaniAminabadi@users.noreply.github.com>
Co-authored-by: yaozhewei <zheweiy@berkeley.edu>
Co-authored-by: Ammar Ahmad Awan <ammar.awan@microsoft.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>
Co-authored-by: Connor Holmes <connorholmes@microsoft.com>
Co-authored-by: Lok Chand Koppaka <lokoppak@microsoft.com>
Co-authored-by: Masahiro Tanaka <81312776+tohtana@users.noreply.github.com>
Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>",['cc9dfffbaf2c0cc2affed995e444d628ff58653c'],False,"['cuda_accelerator.py', 'context.h', 'cpu_adagrad.h', 'cpu_adam.h', 'fake_quantizer.cu', 'dropout_kernels.cu', 'ds_transformer_cuda.cpp', 'pt_binding.cpp', 'inference_context.h', '__init__.py', 'config.py', 'ds_transformer.py', 'base.py', 'bert.py', 'bloom.py', 'clip.py', 'distil_bert.py', 'meta_tensor.py', 'gpt2.py', 'gptj.py', 'gptneo.py', 'gptneox.py', 'megatron_gpt.py', 'opt.py', 'layers.py', 'policy.py', 'replace_module.py', 'config.py', 'ds_attention.py', 'ds_mlp.py', 'gelu_gemm.py', 'linear.py', 'mlp_gemm.py', 'qkv_gemm.py', 'vector_matmul.py', 'config.py', 'engine.py', 'loss_scaler.py', 'hybrid_engine.py', 'utils.py', 'config.py', 'linear.py', 'parameter_offload.py', 'partition_parameters.py', 'partitioned_param_coordinator.py', 'stage3.py', 'hybrid_engine_config.json', 'hybrid_engine_test.py', 'version.txt']"
cfdd801302840c328412485516dfc77343cb11ce,"fix references to figures (#3189)

Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>",['47f9f13bd3236d426d558d61ef358b36191ef026'],False,['README.md']
79b411b912d536b57a940232565172e3036dbe63,fix typo (#3183),['cfdd801302840c328412485516dfc77343cb11ce'],False,['bert-pretraining.md']
c0add889d92a62cc93dd28f426e2402dee6bc9d5,"Fix typo (#3164)

Fix microsoft/DeepSpeed#3163",['79b411b912d536b57a940232565172e3036dbe63'],False,['config-json.md']
970d827ff6b2a5bb80aca4f7115a02942f389aa5,fix figure (#3199),['02dec1f543b3120a59d4b446a371c48f944e2f5a'],False,['hero-figure-black-ja.png']
29aea09a81332ab42e4c5cdb37323bd061f291fa,"feat: Add support for `NamedTuple` when sharding parameters [#3029] (#3037)

* feat: Add support for `NamedTuple` when sharding parameters [#3029]

* Formatting

---------

Co-authored-by: Alexander van Eck <alexander.vaneck@paige.ai>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['970d827ff6b2a5bb80aca4f7115a02942f389aa5'],False,"['parameter_offload.py', 'test_zero.py']"
a725c5d7eb207b5260476cbc3b0d6757f9717d44,fix license badge (#3200),['29aea09a81332ab42e4c5cdb37323bd061f291fa'],False,['README.md']
9408a8666c4fdb212bdfad76b544b86015284908,"Update AMD workflows (#3179)

* Update AMD workflows

* Update MI200 test flow to use torch latest

* Update tolerances to values that pass (will fix before completing PR)

* Revert chyanges to atol

* Rename workflows

* Fix CI badges",['a725c5d7eb207b5260476cbc3b0d6757f9717d44'],False,"['amd-mi100.yml', 'amd-mi200.yml', 'README.md']"
0b5252bbd3347a25e1a541119129be269a6899cc,"[CPU support] Optionally bind each rank to different cores on host (#2881)

* add fallback path for kernels used in megatron

* temporary numactl WA for SPR 56core

* adapt core allocation according to number of ranks

* add switch to turn on numactl

* detect number of cores on the system

* allow select a subset of the cores on the system to bind

* remove unneeded changes

* use current_env to set OMP_NUM_THREADS in subprocess

* add test for ds_arguments

* change --bind_cores_to_rank option to store_true

* add test for parse_range_list

* add comment for parse range list

* add test for parse range list, rewrite parse_range_list

* fix format error

* fix format

* add -m parameter to numactl when necessary

* Check KMP_AFFINITY to avoid conflict with numactl

* fix format

* negative case for parse_range_list

* detect whether numactl is installed before use numactl to bind cores

* check numactl with package manager of distro

---------

Co-authored-by: sdp <sdp@aia-sdp-spr-108864.jf.intel.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['9408a8666c4fdb212bdfad76b544b86015284908'],False,"['launch.py', 'runner.py', 'test_ds_arguments.py']"
a8f999e3c4c7ad66ba6be1caa5a054a50cf57852,"Update DeepSpeed-Chat docs with latest changes to scripts (#3219)

* update docs to reflect changes in deepspeed-chat training script

* add blogs to ignored changes in unit tests",['ad97ca3417d4cb5eec686dee0750b74e783c3848'],False,"['amd-mi100.yml', 'amd-mi200.yml', 'nv-accelerate-v100.yml', 'nv-inference.yml', 'nv-lightning-v100.yml', 'nv-megatron.yml', 'nv-mii.yml', 'nv-torch-latest-cpu.yml', 'nv-torch-latest-v100.yml', 'nv-torch19-p40.yml', 'nv-torch19-v100.yml', 'nv-transformers-v100.yml', 'README.md', 'README.md', 'README.md']"
717c30203e48cb4bd195d4f12df78f6b373662ad,"Nested zero.Init() and dynamically defined model class (#2989)

* support nesting zero.Init() and dynamically defined module

* throw an error if model class defined in zero.Init is not wrapped

* fix check on new classes that are not wrapped in zero.Init()

* add tests of nesting zero.Init() and dynamically defined classes

* fix tests for zero.Init

* fix style

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['a8f999e3c4c7ad66ba6be1caa5a054a50cf57852'],False,"['partition_parameters.py', 'test_zero_dynamic_class.py', 'test_zero_nesting_init.py']"
94fbd9479cd350adc64ab5a99f7637c2707ca974,"Update torch version check in building sparse_attn (#3152)

* Update torch version check in building sparse_attn

* Update triton check

* Fix error message

* Format fixes

* Test with triton 2

* Change requirements back

* fix to just prevent 2.0.0

* Fix formatting

---------

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['717c30203e48cb4bd195d4f12df78f6b373662ad'],False,['sparse_attn.py']
fef5aa6e6d88e963ebab117724cd9c4925fcd612,fix for SD afte DeepSpeed-chat merge (#3218),['6fc8e33c12d89d641d55e7e7decbd29fb81b2ba2'],False,['diffusers_attention.py']
050aee287d70157e29f242b3a629a4cb97b4e4e7,"fix typo in autotuner.py (#3269)

resutls -> results

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['6dbff77117e7edfcab7e103ab3b082bae870bbf0'],False,['autotuner.py']
48297c4841374776a3c4d9319a9b57378f598d65,"improving int4 asymmetric quantization accuracy (#3190)

* Fixes for asymmetric quantization

* addtional offset to further improve accuracy

* put the 0.5 into offset rather than applying it later

* update unit test for quantization

* fix format

* attempt to fix format

---------

Co-authored-by: Connor Holmes <connorholmes@microsoft.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['050aee287d70157e29f242b3a629a4cb97b4e4e7'],False,"['quantization_utils.h', 'test_dequantize.py', 'test_quantize.py']"
bcccee4d85557994d7a1ec7f068bb7ae369b2173,"Fix cupy install version detection (#3276)

* updated cupy install

* do non-isolated pip install

* Update action.yml",['077e42e68a766a5ec1a1b231a62af0b6125defdd'],False,"['nv-torch19-p40.yml', 'nv-torch19-v100.yml', 'action.yml', 'setup.py']"
f7bfe5e7efbb9d04b1297f404de6fd76a6a07f4c,"[ROCm] temporary workaround till __double2half support enabled in HIP (#3236)

* temporary WAR workaround till __double2half support enabled in HIP

* workaround only for hipcc

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['bcccee4d85557994d7a1ec7f068bb7ae369b2173'],False,['conversion_utils.h']
089056236cceb119d62c3a10b122592b61144a41,Fix pydantic and autodoc_pydantic (#3290),['f7bfe5e7efbb9d04b1297f404de6fd76a6a07f4c'],False,"['requirements-readthedocs.txt', 'requirements.txt']"
ab1d2f826b5e1365b9e68b59e4d13e27c6f1065e,"Update Dockerfile (#3298)

line 98 should be
curl -O https://bootstrap.pypa.io/pip/3.6/get-pip.py && \
to avoid
#16 106.9 ERROR: This script does not work on Python 3.6 The minimum supported Python version is 3.7. Please use https://bootstrap.pypa.io/pip/3.6/get-pip.py instead.

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['dd902a3c0fc3ff937debf8892671253d50764169'],False,['Dockerfile']
dd8df20fe0cc773d580689081d03e14b98e81416,"zero3 checkpoint frozen params (#3205)

* zero3 checkpoint frozen params

* Remove debug prints

* Move to cpu

* WIP

* WIP

* WIP

* Cleanup

* Cleanup

* Extend unit test for frozen params

* API fix",['ab1d2f826b5e1365b9e68b59e4d13e27c6f1065e'],False,"['constants.py', 'bf16_optimizer.py', 'engine.py', 'engine.py', 'zero_to_fp32.py', 'common.py', 'test_zero_optimizer.py', 'test_zero.py', 'simple_model.py']"
ad168a695425f2617976cf0ab2e281854b97aa26,"Fix for dist not being initialized when constructing main config (#3324)

* move dist init out of Engine",['dd8df20fe0cc773d580689081d03e14b98e81416'],False,"['__init__.py', '__init__.py', 'torch.py', 'utils.py', 'engine.py']"
145c3a75916cc39de9048e6c5b415fac6d634896,"Fix missing scale attributes for GPTJ (#3256)

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>",['ad168a695425f2617976cf0ab2e281854b97aa26'],False,"['pt_binding.cpp', 'gelu_gemm.py']"
4a3ca4e26d4f29e176ea80bf5ec303240107b49a,"Fix formatting (#3343)

* formatting

* fixing clang-format version

* update pre-commit URL",['78afab048a8daebfd89a6cf4e862d169b5ebad40'],False,"['.pre-commit-config.yaml', 'general_kernels.cu', 'requirements-dev.txt']"
496a9a3a62ba4ce971403f053c059b9a9ff9cb99,"Diffusers 0.15.0 bug fix (#3345)

* diffusers 0.15.0 cross attention class check

* revert diffusers_attention.py",['6e1cbebe52f8a66872929689419c3f159cb43bcb'],False,['replace_module.py']
a748bfc6d0f83fb61738f06cfdf9943b8bd21299,"fix mpich launcher issue in multi-node (#3078)

Signed-off-by: Wang, Yi A <yi.a.wang@intel.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['c9196b615ed0e6b31c4e6490109d7a1bb282509f'],False,['multinode_runner.py']
3031eec44efc6ea15b7d5cda94da94227a248334,"Update DS-Chat issue template (#3368)

* request log output

* add more details",['a748bfc6d0f83fb61738f06cfdf9943b8bd21299'],False,['deepspeed_chat_bug_report.md']
085981bf1caf5d7d0b26d05f7c7e9487e1b35190,"add deepspeed chat blog links, add tags (#3369)",['3031eec44efc6ea15b7d5cda94da94227a248334'],False,"['2020-02-13-release.md', '2020-02-13-turing-nlg.md', '2020-03-17-reduce-scatter.md', '2020-05-19-bert-record.md', '2020-05-19-press-release.md', '2020-05-19-zero-stage2.md', '2020-05-28-fastest-bert-training.md', '2020-07-24-deepspeed-webinar.md', '2020-08-07-webinar-on-demand.md', '2020-09-08-sparse-attention-news.md', '2020-09-09-ZeRO-Offload.md', '2020-09-09-onebit-adam-blog-post.md', '2020-09-09-onebit-adam-news.md', '2020-09-09-pipeline-parallelism.md', '2020-09-09-sparse-attention.md', '2020-10-28-progressive-layer-dropping-news.md', '2021-03-08-zero3-offload.md', '2021-05-05-MoQ.md', '2021-05-05-inference-kernel-optimization.md', '2021-05-14-inference-release.md', '2021-08-18-deepspeed-moe.md', '2021-11-15-autotuning.md', '2021-12-09-deepspeed-moe-nlg.md', '2022-01-19-moe-inference.md', '2022-03-21-amd-support.md', '2022-07-26-deepspeed-azure.md', '2022-09-10-zero-inference.md', '2022-10-11-mii.md', '2022-12-12-data-efficiency.md', '2023-03-31-multi-modal.md', '2023-04-24-deepspeed-chat-chinese.md', '2023-04-24-deepspeed-chat-japanese.md', '2023-04-24-deepspeed-chat.md']"
39825a9092b997a2fdcf129e91224183dda93cbc,"Fix redundant shared_params in zero_to_fp32.py (#3149)

* submit changes

* update format

* fix fomrat

* revert

* test

* add top

* treat z1 as z2

* fix shared

* remove old changes

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['085981bf1caf5d7d0b26d05f7c7e9487e1b35190'],False,['zero_to_fp32.py']
d56268f3752433d63b1025435d8c597174bf7033,"fixing default communication_data_type for bfloat16_enabled and docs (#3370)

Co-authored-by: Alexander Jipa <azzhipa@amazon.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['39825a9092b997a2fdcf129e91224183dda93cbc'],False,"['engine.py', 'config-json.md', 'test_bf16.py']"
2e99f6edf6df018d33be71f2cfe64c12bae3b662,"[DRAFT] Tentative implementation of MiCS (#2964)

* include mics config and optimizer

* change private vars to public vars

so the child class can initialize these vars

* Port the init function from stage3

* adding a model test file for mics

* adopt to get_acceleartor api and fp16 group defrag

* WIP: porting mics modification to ms master

* WIP: included gradient all-reduce among replication groups

* WIP: ported hierarchical all gather part

did basic loss test on a simple MLP model

* [Bug fix] using the comm group attached on the param

* torch2.0 support

* remove print

* delegate wait op

* [Bug] fix naming

* adding doc string

* resolving recursive import

* fix formating, typo and license

* fix license and unit test error

---------

Co-authored-by: Ubuntu <ubuntu@ip-172-31-14-191.us-west-2.compute.internal>
Co-authored-by: Ubuntu <ubuntu@ip-172-31-7-70.us-west-2.compute.internal>
Co-authored-by: Zhen Zhang <zhzhn@amazon.com>
Co-authored-by: zhzhn <zhzhn@ip-10-2-57-114.us-west-2.compute.internal>",['d92539509b1e9a6178cfdb921d5080e76f690bce'],False,"['comm.py', 'torch.py', 'config.py', 'engine.py', '__init__.py', 'config.py', 'mics.py', 'mics_utils.py', 'partition_parameters.py', 'stage3.py', 'zero3.rst', 'test_mics_config.py', 'test_model.py']"
01d17492c360753670662ae6cac65e0c35e70734,"Fix memory leak in zero2 contiguous gradients (#3306)

No usage of extra_large_param_to_reduce if contiguous_gradients is False.
It keeps reference of the param for the lifetime of the application.

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['0e357666a10e71841b211f29ede7e7b21ce30b7c'],False,['stage_1_and_2.py']
a094c9763de8d42107cbffd0bb9abb8056aa3c60,"remove megatron-lm, no longer pip installable (#3389)

* remove megatron-lm, no longer pip installable

* Add skips to tests that require megatron-lm and can't be run currently.

* formatting

* Formatting

---------

Co-authored-by: Logan Adams <loadams@microsoft.com>",['f7d71ec15376bc8188e5e8eca9c79177536c6e5d'],False,"['requirements-dev.txt', 'test_compression.py', 'test_configurable_parallel_mp.py', 'test_configurable_parallel_pp.py']"
7ddc3b01dd0514233a4b7f75040bc902b833a5f5,"Fix pipeline module evaluation when contiguous activation checkpointing is enabled (#3005)

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['a094c9763de8d42107cbffd0bb9abb8056aa3c60'],False,"['engine.py', 'test_pipe_module.py']"
b0d9c4d052796bdd16596c43053eb8b798696141,"Fix `PipelineEngine.eval_batch` result (#3316)

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['b2ac7f6333e1c621a0e5a03e7b214f00bcda441a'],False,['engine.py']
77ebf760f38b8fd9554edc3e6e350a78473018b1,"[zero_to_fp32] fix shared param recovery (#3407)

* [zero_to_fp32] fix shared param recovery

* cleanup

* cleanup

* better naming

* not all params have ds_id it seems

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['dcb4a7d66439f2100c6d0e67723eb94bfe3f5ced'],False,"['engine.py', 'zero_to_fp32.py']"
0a61d5d66462d51c6d867ab58479adf94f59de97,"Hybrid Engine Refactor and Llama Inference Support (#3425)

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['d2e6110e23bc9169e9238a467492f2b66185523c'],False,"['nv-accelerate-v100.yml', 'nv-inference.yml', 'nv-lightning-v100.yml', 'nv-megatron.yml', 'nv-mii.yml', 'nv-torch19-p40.yml', 'nv-torch19-v100.yml', 'nv-transformers-v100.yml', 'ds_kernel_utils.h', 'apply_rotary_pos_emb.cu', 'dequantize.cu', 'gelu.cu', 'layer_norm.cu', 'pointwise_ops.cu', 'pt_binding.cpp', 'relu.cu', 'rms_norm.cu', 'softmax.cu', 'transform.cu', 'inference_context.h', 'inference_cublas_wrappers.h', 'inference_cuda_layers.h', 'config.py', 'engine.py', 'ds_transformer.py', '__init__.py', 'base.py', 'bert.py', 'bloom.py', 'clip.py', 'distil_bert.py', '__init__.py', 'gated_mlp.py', 'hybrid_engine.py', 'hybrid_megatron.py', 'megatron.py', 'meta_tensor.py', 'split_qkv.py', 'gpt2.py', 'gptj.py', 'gptneo.py', 'gptneox.py', 'llama.py', 'megatron_gpt.py', 'megatron_gpt_moe.py', 'opt.py', 'policy.py', 'replace_module.py', 'replace_policy.py', 'utils.py', 'config.py', 'diffusers_attention.py', 'diffusers_transformer_block.py', 'ds_attention.py', 'ds_mlp.py', 'moe_inference.py', 'gelu_gemm.py', 'linear.py', 'mlp_gemm.py', 'qkv_gemm.py', 'residual_add.py', 'softmax.py', 'softmax_context.py', 'vector_matmul.py', 'hybrid_engine.py', 'types.py', 'builder.py', 'transformer_inference.py', 'test_hybrid_engine.py', '__init__.py', 'inference_test_utils.py', 'test_bias_add.py', 'test_bias_geglu.py', 'test_bias_gelu.py', 'test_bias_relu.py', 'test_layer_norm.py', 'test_moe_res_matmult.py', 'test_residual_add.py', 'test_rms_norm.py']"
d10b8ca011b18eba3a6ca56f4208a732d7fbb744,"add sharded checkpoint loading for AutoTP path to reduce the peak mem… (#3102)

* add sharded checkpoint loading for AutoTP path to reduce the peak memory in initialization stage

Signed-off-by: Wang, Yi A <yi.a.wang@intel.com>

* fix gptj sharded checkpoint loading problem

Signed-off-by: Wang, Yi A <yi.a.wang@intel.com>

---------

Signed-off-by: Wang, Yi A <yi.a.wang@intel.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['0a61d5d66462d51c6d867ab58479adf94f59de97'],False,"['engine.py', 'replace_module.py']"
87edbc8dcee81db66138bca1c3cc0055f7106d68,"Fix spelling errors in comments and documents  (#3486)

* fix some spelling error under doc/

* fix spelling error deepspeed/

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['4132118054b3c0458baee150e24d55322cf3c2f2'],False,"['autotuner.py', 'constants.py', 'base_tuner.py', 'utils.py', 'basic_layer.py', 'compress.py', 'constants.py', 'training.md', 'MoQ-tutorial.md', 'large-models-w-deepspeed.md', 'mixture-of-experts-inference.md', 'mixture-of-experts-nlg.md']"
f1fab902c8248d7838d9206ce47cdaf4ae1e37ec,"fix spelling error (#3482)

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['87edbc8dcee81db66138bca1c3cc0055f7106d68'],False,"['pt_binding.cpp', 'inference_context.h']"
195563a2c5dd865c2f56df133e14249c1387171c,"fix a small type error on bf16+pp (#3441)

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['f1fab902c8248d7838d9206ce47cdaf4ae1e37ec'],False,['engine.py']
198166423d9f2bd8c03a62bc4d8b1c0e74e31b19,fix spelling error with deepspeed/ (#3494),['195563a2c5dd865c2f56df133e14249c1387171c'],False,"['autotuner.py', 'deepspeed_checkpoint.py', 'comm.py', 'compress.py', 'helper.py', 'utils.py', 'config.py', 'constants.py', 'config.py']"
58c4d230920f10b9a0c33891b6cb88afc1a6a5f4,"fix spelling error with  docs/index.md (#3443)

* Update index.md

fix spelling error

* Update training.md

fix spelling error

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['198166423d9f2bd8c03a62bc4d8b1c0e74e31b19'],False,['index.md']
b31b46c0d100f93295eb33ce54c03d093c053b62,"fix regression in shard checkpoint loading in AutoTP Path caused by qkv_copy() is deleted and add UT case for shard checkpoint loading in AutoTP (#3457)

* add UT case for shard checkpoint loading in AutoTP

Signed-off-by: Wang, Yi A <yi.a.wang@intel.com>

* autoTP path also support shard loading

Signed-off-by: Wang, Yi A <yi.a.wang@intel.com>

---------

Signed-off-by: Wang, Yi A <yi.a.wang@intel.com>",['db26f8b41325be2a7f7af8b386b4e8951a5a76c9'],False,"['engine.py', 'replace_module.py', 'test_checkpoint_sharding.py']"
194053bd58947ac6a45363ba780c9dfb127d3064,"Hybrid Engine Fix Llama (#3505)

Co-authored-by: Connor Holmes <connorholmes@microsoft.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>
Co-authored-by: Ammar Ahmad Awan <ammar.awan@microsoft.com>",['b303fa8b5b27bb7f27d8d9f2ceab8da3f41ba8e6'],False,"['gated_mlp.py', 'hybrid_engine.py', 'split_qkv.py', 'llama.py', 'layers.py', 'load_checkpoint.py', 'hybrid_engine.py', 'test_he_all.py', 'test_he_llama.py']"
254663a28cffcd83ecfa265f9accb474e7ae0506,"fix spelling error with deepspeed/runtime/ (#3509)

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['194053bd58947ac6a45363ba780c9dfb127d3064'],False,"['launch.py', 'base.py', 'nebula_checkpoint_engine.py', 'engine.py', 'hybrid_engine.py', 'utils.py', 'stage3.py', 'stage_1_and_2.py']"
b3956dc9e3c7af8d802a48f145a5e99b0b73e772,change actions/checkout@v2 to v3 (#3526),['4d269c6e4db390c2b427decc863e0a29df2fa99f'],False,"['amd-mi100.yml', 'amd-mi200.yml', 'auto-sync.yml', 'formatting.yml', 'nv-accelerate-v100.yml', 'nv-inference.yml', 'nv-lightning-v100.yml', 'nv-megatron.yml', 'nv-mii.yml', 'nv-nightly.yml', 'nv-pre-compile-ops.yml', 'nv-torch-latest-cpu.yml', 'nv-torch-latest-v100.yml', 'nv-torch-nightly-v100.yml', 'nv-torch19-p40.yml', 'nv-torch19-v100.yml', 'nv-transformers-v100.yml', 'python.yml']"
d73f99d8550831311c0c118c169ad480c11d8852,"fix typo with docs/ (#3523)

* fix spelling error with deepspeed/runtime/

* fix typo docs/

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['b3956dc9e3c7af8d802a48f145a5e99b0b73e772'],False,"['autotuner.py', 'inference-tutorial.md', 'large-models-w-deepspeed.md', 'mixture-of-experts-inference.md', 'model-compression.md', 'pipeline.md', 'sparse-attention.md']"
1824cb5570b9529567d28904e95ac6dc1d6c0a71,fix if statement for self.mpu (#3497),['82991df05419828d65cbb658efc01b50c135ea00'],False,['hybrid_engine.py']
5c6da1f001f936234a31a238e71ca386e34eb51a,"Fix passing offload_optimizer_config instead of offload_param_config to (#3420)

DeepSpeedZeRoOffload

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['1824cb5570b9529567d28904e95ac6dc1d6c0a71'],False,['stage3.py']
9f4a876360296f24577dd07796625f793a38b0c1,"Fix broadcast error on multi-node training with ZeroStage3 and TensorParallel=2 (#2999)

* * try to fix broadcast error on multi-node training with ZeroStage3 and TensorParallel=2

* * fix format error

* * fix format issue

* * add TODO for integrated testing of TP and ZeRO 1/2/3

* fix default pg error

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['5c6da1f001f936234a31a238e71ca386e34eb51a'],False,"['partition_parameters.py', 'test_configurable_parallel_mp.py']"
4716b0f7691f6e58842fc522b48a54090b784ca8,"share inflight registry between PartitionedParameterCoordinators (#3462)

* share inflight registry between PartitionedParameterCoordinator

* bound registry to model

* make InflightParamRegistry standalone

* fix format

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['9f4a876360296f24577dd07796625f793a38b0c1'],False,"['parameter_offload.py', 'partitioned_param_coordinator.py']"
c8d3f5eb196423083f75dc23b5d4b4bcdd222e55,"fix typo in comments with deepspeed/  (#3537)

* fix spelling error with deepspeed/runtime/

* fix typo docs/

* fix typo in comments with deepspeed/

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['9685eb92ab98ea4534fbfce21e303f396575f7e4'],False,"['basic_layer.py', 'compress.py', 'constants.py', 'cpu_adam.py', 'fused_adam.py', 'dropping_utils.py', 'sparsity_config.py', 'replace_copyright.py', 'test_inference.py']"
5147b90aa4477c4a3914c4ca10e701f30753837d,"[ROCm] Hip headers fix (#3532)

* Add cg headers hipification

* Exclude including cuda_bf16.h on ROCm

* Merge

* Retricting including cuda_bf16.h with BF16_AVAILABLE var

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['c8d3f5eb196423083f75dc23b5d4b4bcdd222e55'],False,"['apply_rotary_pos_emb.cu', 'inference_cublas_wrappers.h']"
1f72082fc0ea159d0de46886d8e713dda7df9ce2,"[CPU] Support Intel CPU inference (#3041)

* add fallback path for kernels used in megatron

* temporary numactl WA for SPR 56core

* adapt core allocation according to number of ranks

* add switch to turn on numactl

* detect number of cores on the system

* allow select a subset of the cores on the system to bind

* remove unneeded changes

* add ccl backend

* change nccl to ccl

* remove unused code

* add comm/ccl to ops

* initial ccl comm support

* first broadcast case passed

* add CCL_Backend to DeepSpeed

* support comm timer for CPU

* support barrier for comm backend

* support specify master address from deepspeed command line

* support pytorch 2.0

* remove 'block' from api

* Tweak for debug

Signed-off-by: Cao, Zhong Z <zhong.z.cao@intel.com>

* Remove unecessary directory

Signed-off-by: Cao, Zhong Z <zhong.z.cao@intel.com>

* Add bf16 kernel support for inference

* Add temporary torch implement for cpu inference

* Add softmax ops cpu fallback for inference

* bind cores to numa domain as well

* merge latest change in gma/numactl

* initial bf16 kernel support with fallback path

* initial fallback path for bloom kernel injection

* fix softmax attn mask

* check KMP_AFFINITY to avoid conflict with numactl

* New CCLBackend which utilize TorchBackend for initialization

* rollback last change because there is result error

* fix bloom injection policy TP could not work issue.

injection_policy={BloomBlock: (""self_attention.dense"", ""mlp.dense_4h_to_h"")}

* Use TorchBackend to initialize CCLBackend, make behavior consistent

* remove comm under deepspeed/ops

* add license header

* code clean up

* fix format issue

* remove magic number in main address

* add caching support but not turn on by default

* change name of inference_cuda_module to inference_module

* Check for is_synchronized_device in accelerator before get Event

* fix typo

* Fix fallback path of softmax kernel on CUDA device for BF16 data type, because CUDA tril does not support BF16 datatype, enforce fp32 data type

* add cpu backend files

* change CPU_Accelerator op_builder_dir

* remove cpu_kernel_path

* using CPU_Accelerator on non-cuda device

* fix deepspeed.op_builder => deepspeed.ops.op_builder

* add alias for num_gpus: num_accelerators

* allow loading cpu_builder in build stage

* Assume cuda available if torch not installed

* add oneccl_binding_pt to requirements

* move oneccl-binding-pt to seperate requiremetns-cpu.txt

* add missing file

* use dependency_links in setuptools.setup() call for additional dependency links

* install oneccl_bind_pt in workflows

* change oneccl_bind_pt's version from 1.13 to 2.0

* use intel_exention_for_pytorch as indicator that CPU_Accelerator should be used

* Add indicator for Accelerator used

* change foo.c to foo.cpp

* exclude 'cpu' directory in CUDA op builder reflection

* add a cpu-inference workflow

* run cpu-inference workflow on self-hosted instance

* change cpu runs-on node to v100 node

* print out python version in workflow

* add verbose in pip command to understand oneccl_bind_pt install issue

* update cpu-inference workflow

* add a stage to detect instance instruction sets

* add back bf16 support for CPU inference

* enable autoTP for bloom

Signed-off-by: Wang, Yi A <yi.a.wang@intel.com>

* update workflow to detect cpu instruction sets

* temporary WA for Intel Extension for PyTorch AVX2 instructioon set detection

* change cpu-inference workflow machine to ubuntu-20.04

* add sharded checkpoint loading for AutoTP path to reduce the peak memory in initialization stage

Signed-off-by: Wang, Yi A <yi.a.wang@intel.com>

* enable policy for llama

* use a special build ipex to test avx2 detection fix

* fix format

* fix test fail issue

Signed-off-by: Wang, Yi A <yi.a.wang@intel.com>

* fix gptj sharded checkpoint loading problem

Signed-off-by: Wang, Yi A <yi.a.wang@intel.com>

* return a not implemented build in get_op_builder in cpu_backend

* support cpu device in tests

* use cpuinfo to extract number of CPUs

* use ~/tmp as transfomer cache rather than /blob/

* Add support for mpich launcher with prefer_deepspeed_comm

* add missing modification in accelerator

* enable IMPI launcher

* remove unused file and fix formatting

* clean up ccl.cpp

* Less confusing error message when certin op builder are not implemented

* Fix license header

* Add license header

* add license headers

* add license header

* fix cuda specific code in test

* update CPU workflow

* use numactl to bind to core

* allow bind_cores_to_rank in multi-node impi runner

* fix format error

* Remove InferenceBuilder

* fix format error in numa.py

* check whether op is in installed ops in ds_report.py

* allow override accelerator with DS_ACCELERATOR='cuda','cpu' or 'xpu'

* lazy init class_dict in CUDA_Accelerator to avoid cyclic initialization of CUDA_Accelerator

* put short path in the beginning in real_accelerator.py

* device_count return number of NUMA nodes

* fix typo

* install numactl in cpu workflow

* Follow comments

* Better implementation of device_count() and current_device()

* remove dependency_link for Intel Extension for DeepSpeed

* use check is_synchronized_device in timer only once

* remove env mapping WA in cpu_accelerator

* fix duplicate definition

* fix format error

* refine ccl backend selection

* move comments to the right place

* remove prefer_deepspeed_comm, use CCLBackend by default

* refractor fallback path

* Fix execution failure in kernel injection path

* do not refractory kernel injection fallback path in  residual_add because it contains function call with side-effect

* guard residual_add fallback path with environ DS_KI_FALLBACK=True

* fix format error

* add test for allreduce on CPU workflow

* fix format error

* Fallback to TorchBackend if CCLBackend kernel are not implemented

* Update Intel Extension for Pytorch installation link

* Don't specify version number of Intel Extension for PyTorch

* install oneCCL for CCLBackend

* fix link path for CPU comm kernels

* fix source oneCCL environment

* source oneCCL env before run UT

* Give more specific instruction when CCL_ROOT not defined

---------

Signed-off-by: Cao, Zhong Z <zhong.z.cao@intel.com>
Signed-off-by: Wang, Yi A <yi.a.wang@intel.com>
Co-authored-by: sdp <sdp@aia-sdp-spr-108864.jf.intel.com>
Co-authored-by: Cao, Zhong Z <zhong.z.cao@intel.com>
Co-authored-by: Zhenhuan Chen <zhenhuan.chen@intel.com>
Co-authored-by: baodii <di.bao@intel.com>
Co-authored-by: Wang, Yi A <yi.a.wang@intel.com>
Co-authored-by: jianan-gu <jianan.gu@intel.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['5147b90aa4477c4a3914c4ca10e701f30753837d'],False,"['cpu-inference.yml', 'abstract_accelerator.py', 'cpu_accelerator.py', 'cuda_accelerator.py', 'real_accelerator.py', 'ccl.cpp', 'ccl.py', 'comm.py', 'constants.py', 'reduce_op.py', 'env_report.py', 'engine.py', 'constants.py', 'launch.py', 'multinode_runner.py', 'runner.py', 'ds_transformer.py', 'replace_module.py', 'config.py', 'diffusers_attention.py', 'moe_inference.py', 'base.py', 'gelu_gemm.py', 'linear.py', 'mlp_gemm.py', 'qkv_gemm.py', 'residual_add.py', 'softmax.py', 'softmax_context.py', 'vector_matmul.py', '__init__.py', 'numa.py', 'timer.py', 'builder.py', '__init__.py', 'builder.py', 'comm.py', 'no_impl.py', 'requirements-cpu.txt', 'test_dist.py', 'common.py', 'test_model_profiling.py', 'test_ds_arguments.py']"
5c3ebd7ede063dd5141711c0b2567924b3ec2908,"Clone tensors to avoid torch.save bloat (#3348)

* Clone tensors to avoid torch.save bloat

* Adddocs

* Fix clang-formatting

* Update docs/code-docs/source/model-checkpointing.rst

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>

* Update deepspeed/checkpoint/utils.py

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>

* Update deepspeed/checkpoint/utils.py

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>

* Fix url

* url fix

* Tweak docs

---------

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['1f72082fc0ea159d0de46886d8e713dda7df9ce2'],False,"['utils.py', 'model-checkpointing.rst', 'test_zero_optimizer.py']"
2fc56841e8525dfdfe9f14cda51505af2d71cd8a,"Fix (#3527)

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['5c3ebd7ede063dd5141711c0b2567924b3ec2908'],False,['builder.py']
49d399cd284535129f82cc5fd25a580f35c1c6e1,"Fixing bf16 test  (#3551)

* Fixing bf16 test that was missing a config.

* Chaning train_batch_size to train_micro_batch_size_per_gpu

* Chaning all train_batch_size to train_micro_batch_size_per_gpu",['d1c3c0df5318cbcb2b221d38d4127dd566f4984d'],False,['test_bf16.py']
766798849144fdb51e81204370d4e9411a34cafc,"Fix Hybrid Engine for BLOOM (#3580)

This PR fixes Hybrid Engine (HE) support for the BLOOM model, which was accidentally broken during the HE refactor in GH-3425.

The BLOOM container now inherits the HybridEngineContainer feature and defines a set_lora_params() function necessary for the feature to work. get_lora_params() is correspondingly removed from the BLOOM policy class as well.

GPT-NeoX was also cleaned up by removing a get_lora_params() function from its policy due to it no longer being used.",['49d399cd284535129f82cc5fd25a580f35c1c6e1'],False,"['bloom.py', 'gptneox.py']"
6622776cecb15831dfec96f892921447750e5b69,Fix op_builder against PyTorch nightly (#3596),['766798849144fdb51e81204370d4e9411a34cafc'],False,['builder.py']
736bf1853b76d9fb8a3f04da83a7c6810ddf58d2,bug fix (#3609),['6622776cecb15831dfec96f892921447750e5b69'],False,['data_analyzer.py']
d755b9d616bac5d830e6025dc7d0ca10d45d4ba8,"Align InferenceEngine to store ms in _model_times (#3501)

* Align InferenceEngine to store ms in _model_times

   When using cuda_events, the measured model time is stored in ms.
   When not using cuda_events, the measured model time was stored in seconds.
   This commit fixes the units and aligns them to store ms, the same as elapsed() function.
   This was observed when running the following pytest:
   unit/inference/test_model_profiling.py::TestModelProfiling::test[False-True-roberta-base-fill-mask]

   Returned values were:
     count=0 e2e_t=895.174312 model_t=0.8529715538024902
     count=1 e2e_t=7.500252 model_t=0.0041310787200927734
     count=2 e2e_t=3.887346 model_t=0.0018568038940429688
     count=3 e2e_t=3.577845 model_t=0.0016334056854248047
     count=4 e2e_t=3.43976 model_t=0.0016703605651855469
     count=5 e2e_t=3.310903 model_t=0.0016107559204101562
     count=6 e2e_t=3.299556 model_t=0.001603841781616211
     count=7 e2e_t=3.605722 model_t=0.0015969276428222656
     count=8 e2e_t=3.273741 model_t=0.0015516281127929688
     count=9 e2e_t=3.46306 model_t=0.0016617774963378906

   The units difference is observed here, when model_t is in ther order of 10e-3 comparing to e2e_t

* Update engine.py

---------

Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>",['0411a9f871dafc44ac498e0028d3036da86d53bb'],False,['engine.py']
49a73549b91cc0a589d3a44e59dd4687e774d296,"AISC launcher fixes (#3637)

* tmp remove launcher args

* add exclude list for env variables on aisc

* add comment",['d755b9d616bac5d830e6025dc7d0ca10d45d4ba8'],False,['runner.py']
8f459c50e79528eb2a2612f3a1ea280af6afe883,"Update megatron.md (#3641)

grammar fix.

Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>",['8b8c7031fb9b3530fea0ede2c869e9099864e49a'],False,['megatron.md']
3b299997610e4a93a506bc617584f2845a2051ea,"deepspeed/comm/comm.py: fix typo of warning message (#3636)

Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>",['e02b8d0bd06fd5e3d65c4bc410708ee2443334c7'],False,['comm.py']
f5dde36c1af9fe6b166ed05d381f546f53dcebce,"Fix RuntimeError when using ZeRO Stage3 with mpu: #3564 (#3565)

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['3b299997610e4a93a506bc617584f2845a2051ea'],False,['stage3.py']
cd4e473ee613a3c5d0b70769c5ec19a7fc8a453b,"fix typo with deepspeed/  (#3547)

* fix spelling error with deepspeed/runtime/

* fix typo docs/

* fix typo in comments with deepspeed/

* fix typo deepspeed/

* Update constants.py

Remove the space after nebula

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>",['da8f4e01b55acc71e3da21d8c9aadae3b3f950bb'],False,"['autotuner.py', 'compress.py', 'constants.py', 'README.md', 'checkpoint_engine.py', 'engine.py']"
5d14afd26ce89be4d4e6ebdf8fb6a943aebd4aa3,"fix typo deepspeed/runtime (#3663)

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['460bec4679fb460ccc24f915ea1674e89baf698d'],False,"['partition_parameters.py', 'stage3.py', 'stage_1_and_2.py']"
c88af2143248e4655d401f9231317f3c76018057,"[MiCS] [Fix] saving and loading model checkpoint logic for MiCS sharding (#3440)

* fix mics save checkpoint hanging

* MiCS load_checkpoint

* copyright

* fix for torch-1.9.0

all_reduce_coalesced api does not support nccl backend

* Naming alignment

* adding more test conditions for mics shard size

* test with different shard sizes

* adding assertion for better error msg

---------

Co-authored-by: Zhen Zhang <zhzhn@amazon.com>",['f483c0348bc141e40f2db44346d32f28da54ad4b'],False,"['torch.py', 'engine.py', 'mics.py', 'test_mics.sh', 'test_mics_config.py', 'test_mics_optimizer.py']"
3fb3cfdced941d8c7856c020812054141f8eab2c,"fix some typo (#3675)

* fix typo deepspeed/runtime

* fix some typo

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['c88af2143248e4655d401f9231317f3c76018057'],False,"['real_accelerator.py', 'README.md', 'model_based_tuner.py']"
e5fe5f65e8be68364210f0173f1d1c4ab58a4ab7,"Use logger in accelerator (#3682)

* Use logger in accelerator

* Handle pre-build cases

* Explain possible import failure",['3fb3cfdced941d8c7856c020812054141f8eab2c'],False,['real_accelerator.py']
4cd0a003f5b6744a3455c34ad0d20364a8627b30,"non-JIT build fix on ROCm (#3638)

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['2d737eddcc44c838a8bc7f934d7dd9cc31bd2b9c'],False,['builder.py']
b7f463ddeb00f8a4119520383759b201bc572b07,"Fix local rank mismatch for heterogeneous nodes (#3409)

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['4cd0a003f5b6744a3455c34ad0d20364a8627b30'],False,['launch.py']
d8aaa581223f00f84645629d4c0e31879827937f,"Fix incorrectly formatted f string (#3698)

Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>",['c17313fb241ddf4ccc8c2b8b135cbcc5e9e89ce5'],False,['runner.py']
f2f5f21b52a82de75c85abd190ecd328ce762c5a,"fix typo name (#3689)

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>",['d8aaa581223f00f84645629d4c0e31879827937f'],False,['split_qkv.py']
7e59ef12300032fc50683fae534cbd41ec1475b0,"Revert ""fix typo name (#3689)"" (#3702)

This reverts commit f2f5f21b52a82de75c85abd190ecd328ce762c5a.",['f2f5f21b52a82de75c85abd190ecd328ce762c5a'],False,['split_qkv.py']
34a9fbf1a3f1c5b992bcea92b1b1aa8ba9ea5cc0,"Fix gpt-j inference issue (#3639)

* fix gpt-j inference issue for mlp_gemm_func call

* bring back the gpt-j inference-test

* fix formatting

* fix the neox and pythia injection issue",['7e59ef12300032fc50683fae534cbd41ec1475b0'],False,"['base.py', 'test_inference.py']"
fb2b4ab11a5a2bbeb998b82aba1205d2bdd06152,"Fix unit test typo in tests/unit/ops/transformer/inference (#3697)

* mix typo and missing epsilon value

* Touch file to re-build

* revert changes

* Touch file to re-build

* Format

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Logan Adams <loadams@microsoft.com>",['c5edc91ecb5a080d8110f2b4dfbc323f8f767116'],False,"['test_layer_norm.py', 'test_rms_norm.py']"
0977106ac902f194e25d839bc2c0bdfbe6e33549,"zero3 performance optimizations (#3622)

* Remove dead code

params_already_reduced is not used

* Prevent evaluation of debug strings

Debug strings are evaluated even when logging is disabled

* Use contiguous gradients tensor reduce scatter between ranks

Use allreduce instead of reduce scatter. lower cpu overhead.

* move overflow tracker to optimizer.step

Don't check overflow in gradients for every bucket.
Do overflow chack once on grad flat buffer just before optimizer step

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['df425097869214c3371d74a4cbd7c506bea3cef7'],False,"['partition_parameters.py', 'partitioned_param_coordinator.py', 'stage3.py', 'test_zero.py']"
fc8e5c8858206802ab1b454d67b8c937e3e9ce9b,"Fix typo in name of hybrid engine function (#3704)

* Fix typo in name of hybrid engine function

* Fix",['0977106ac902f194e25d839bc2c0bdfbe6e33549'],False,['split_qkv.py']
6f4fc30b5823f0a6222617a99093e6ab7f52c1a4,"FP8 unittest for H100 (#3731)

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['5289d691d36e5f7a22e99954c82038e0dbc64c52'],False,['test_fp8.py']
1b40182312a038342c713df4c517359d8b2ee48a,"Fix apex install bugs (#3741)

* Fix apex installation

* Switch install flag from build-opt to global-opt to fix missing cpp_ext

* Try installing with support for newer pip

* Add build packaging

* Update to latest

* Pin to specific commit while pyproject.toml is fixed",['6f4fc30b5823f0a6222617a99093e6ab7f52c1a4'],False,['nv-megatron.yml']
09332dbf9f902359c1d5111090e3618a97331595,"Fix autotuner get_gas_from_user_config (#3664)

Co-authored-by: Feng Zhoutian <fengzhoutian@meituan.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['1b40182312a038342c713df4c517359d8b2ee48a'],False,['autotuner.py']
46bb08c2df725fce1d49cf27341dec0d42a0e0d5,"Include cublas error details when getting cublas handle fails (#3695)

* include cublas error details when getting cublas handle fails

* run clang-format

* just use raw enum value to avoid depending on minimum cuda version

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['09332dbf9f902359c1d5111090e3618a97331595'],False,"['context.h', 'inference_context.h']"
45466afa34aa25da004cc0032952576efb6d0538,"fix hybrid engine mlp module (#3736)

* fixgated_mlp.py

* fix hybrid_engine.py

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['46bb08c2df725fce1d49cf27341dec0d42a0e0d5'],False,"['gated_mlp.py', 'hybrid_engine.py']"
cd911f9ab2213edb0c8781bd5fd604c37c020dfb,Fix output transpose dimension bugs (#3747),['45466afa34aa25da004cc0032952576efb6d0538'],False,['pt_binding.cpp']
3f5e4931098bf533f8217afb6d986c90f81aed80,"fix ccl_backend and residual_add problems (#3642)

* fix ccl_backend path when it should fallback

* fix residual_add fallback when only one kernel is ready

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['9a8f6a1d3c603f352c2ec923c7b575cd1595dfaf'],False,"['ccl.py', 'comm.py', 'residual_add.py']"
044dd0e2c3bf44b98163f15f42343a72b5541078,Fix url in getting-started guide (docs) (#3768),['3f5e4931098bf533f8217afb6d986c90f81aed80'],False,['getting-started.md']
062408683c23f31b3e8f8dd1e1cff8004c0aa45a,"[Fix] _conv_flops_compute when padding is a str and stride=1 (#3169)

* fix conv_flops_compute when padding is a str when stride=1

* fix error

* change type of paddings to tuple

* fix padding calculation

* apply formatting check

---------

Co-authored-by: Cheng Li <pistasable@gmail.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['d2bf38d6f2b344fdb1e4111d57c411f8bc17577c'],False,['profiler.py']
a76cced3facbb3ad7f2986f608b5015a48101c85,fix interpolate flops compute (#3782),['062408683c23f31b3e8f8dd1e1cff8004c0aa45a'],False,['profiler.py']
b3320940153f638def99959468b5b88fa8cfd76b,"ZeRO++ chinese blog (#3793)

* zeropp chinese blog

* try better quality images

* make title larger

* even larger...

* various fix

* center captions

* more fixes

* fix format",['6e4faf869dae7e53b55393b3a4a073f0a2ee7dad'],False,"['eval1.png', 'eval2.png', 'eval3.png', 'hpz.png', 'overview.png', 'qgz.gif', 'qwz.png', 'rlhf-eval.png', 'zero-overview.gif', 'README.md']"
52c6baa933649a313e0083f5dff8d7f6865d4b3d,remove staging trigger (#3792),['b3320940153f638def99959468b5b88fa8cfd76b'],False,"['amd-mi100.yml', 'amd-mi200.yml', 'cpu-inference.yml', 'formatting.yml', 'nv-accelerate-v100.yml', 'nv-inference.yml', 'nv-lightning-v100.yml', 'nv-megatron.yml', 'nv-mii.yml', 'nv-pre-compile-ops.yml', 'nv-torch-latest-cpu.yml', 'nv-torch-latest-v100.yml', 'nv-torch19-p40.yml', 'nv-torch19-v100.yml', 'nv-transformers-v100.yml', 'python.yml']"
69d1b9f97879ba2f8cd1518d6ee17f0f1419f5bc,"DeepSpeed-Triton for Inference (#3748)

Co-authored-by: Stephen Youn <styoun@microsoft.com>
Co-authored-by: Arash Bakhtiari <arash@bakhtiari.org>
Co-authored-by: Cheng Li <pistasable@gmail.com>
Co-authored-by: Ethan Doe <yidoe@microsoft.com>
Co-authored-by: yidoe <68296935+yidoe@users.noreply.github.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['52c6baa933649a313e0083f5dff8d7f6865d4b3d'],False,"['formatting.yml', 'triton-bert-base-latency.png', 'triton-bert-large-latency.png', 'README.md', '__init__.py', 'config.py', 'engine.py', 'ds_transformer.py', 'base.py', 'bert.py', 'distil_bert.py', 'config.py', 'gelu_gemm.py', 'linear.py', 'mlp_gemm.py', 'qkv_gemm.py', 'vector_matmul.py', '__init__.py', 'attention.py', 'gelu.py', 'layer_norm.py', 'matmul_ext.py', 'mlp.py', 'ops.py', 'residual_add.py', 'softmax.py', 'triton_matmul_kernel.py', 'requirements-dev.txt', 'requirements-inf.txt', 'requirements-triton.txt', 'setup.py', 'test_inference.py', 'test_attention.py', 'test_gelu.py', 'test_layer_norm.py', 'test_matmul.py', 'test_residual_add.py', 'test_softmax.py']"
d18aa2c79c4c3782ccc20781efb3630108a7f749,"ZeRO++ (#3784)

Co-authored-by: Sam Abe Jacobs <samjacobs@microsoft.com>
Co-authored-by: GuanhuaWang <alexwgh333@gmail.com>
Co-authored-by: cmikeh2 <connorholmes@microsoft.com>
Co-authored-by: Samyam Rajbhandari <samyamr@microsoft.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Ammar Ahmad Awan <ammar.awan@microsoft.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>
Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>",['69d1b9f97879ba2f8cd1518d6ee17f0f1419f5bc'],False,"['README.md', 'quantization.h', 'pt_binding.cpp', 'quant_reduce.cu', 'swizzled_quantize.cu', 'comm.py', 'torch.py', 'coalesced_collectives.py', 'engine.py', 'config.py', 'mics.py', 'parameter_offload.py', 'partition_parameters.py', 'partitioned_param_coordinator.py', 'partitioned_param_profiler.py', 'stage3.py', 'groups.py', 'zeropp.md', 'ZeRO-baseline.png', 'ZeROpp.png', 'index.md', 'quantizer.py', 'test_model.py', 'test_hpzero.py', 'test_qgzero.py', 'test_qwzero.py']"
b0752b2ef6b5b74ab78c5378ae041d1ef3a03f78,"Add ZeRO++ Japanese blog (#3797)

* zeropp chinese blog

* try better quality images

* make title larger

* even larger...

* various fix

* center captions

* more fixes

* fix format

* add ZeRO++ Japanese blog

* add links

---------

Co-authored-by: HeyangQin <heyangqin@microsoft.com>
Co-authored-by: Conglong Li <conglong.li@gmail.com>",['94479e2b752e73e4bf5429f65a998a202618515d'],False,"['README.md', 'README.md', '2023-06-22-zeropp-chinese.md', '2023-06-22-zeropp-japanese.md', '2023-06-22-zeropp.md', 'index.md']"
c80855b5431e46b1d2bb5a0790c893152895d95e,"Bug Fixes for autotuner and flops profiler (#1880)

* fix autotuner when backward is not called

* fix format

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['b0752b2ef6b5b74ab78c5378ae041d1ef3a03f78'],False,"['README.md', 'autotuner.py', 'engine.py']"
aebdfb3b9257ec7b1cf3f654a07d406aa9d8a12d,"Fix Bug in transform.cu (#3534)

* Bug fix

* Fixed formatting error

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['d33f1f851f67755d17f750d1fe4e7293664fe55f'],False,['transform.cu']
bafaf3c0bb71163d775f47741e48a61d8ce62ef1,"bug fix: triton importing error (#3799)

Co-authored-by: Stephen Youn <styoun@microsoft.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['aebdfb3b9257ec7b1cf3f654a07d406aa9d8a12d'],False,['matmul_ext.py']
2b2be85f43de41986f02ecc76012b359c821bb24,Prevent hangs in CI during parallel run compilation (#2844),['1b888399dcb6dd2fb4d28073c30c690cdb6ce346'],False,"['nv-torch-latest-cpu.yml', 'conftest.py']"
6102d128f2821258eca0ebcd7355c28815197fbc,"Revert ""Prevent hangs in CI during parallel run compilation (#2844)"" (#3817)

This reverts commit 2b2be85f43de41986f02ecc76012b359c821bb24.",['2b2be85f43de41986f02ecc76012b359c821bb24'],False,"['nv-torch-latest-cpu.yml', 'conftest.py']"
203ac9d7ac32f4ed2c2da6d54ccc8ed8346a1699,"support model declaration in zero.Init context (#3592)

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['a63f9152b9cfd01a8080b2d1a13f2ac60af35e20'],False,"['__init__.py', 'partition_parameters.py', 'test_zero_dynamic_class.py', 'test_zero_nesting_init.py']"
78b769359152e3c935c1d2ccd0ce5b3cc92e237e,Re-enable GPT-J unit tests and refactor inference tests (#3618),['7726fc8d54afd638b8a1ae00397dd6965d898741'],False,"['.flake8', '.pre-commit-config.yaml', 'pytest.ini', 'test_inference.py', 'test_model_profiling.py']"
f8551b439ea4a849ecb37d148e3bacbffd64121f,"Fix racing condition in GatheredParameters (#3819)

* Fix racing condition in GatheredParameters",['78b769359152e3c935c1d2ccd0ce5b3cc92e237e'],False,"['partition_parameters.py', 'partitioned_param_coordinator.py']"
fd1d2c64472c1a3061a05eb3b56a3f882199cfba,"Reduce Unit Test Time (Part 2) (#3838)

* utilize shorter tests for MII

* use cached torch download

* rework zero++ unit tests

* formatting

---------

Co-authored-by: HeyangQin <heyangqin@microsoft.com>",['c973e15711c6bcd447f6574909ccfcc940b1050c'],False,"['amd-mi200.yml', 'nv-accelerate-v100.yml', 'nv-inference.yml', 'nv-lightning-v100.yml', 'nv-megatron.yml', 'nv-mii.yml', 'nv-nightly.yml', 'nv-torch-latest-v100.yml', 'nv-torch19-p40.yml', 'nv-torch19-v100.yml', 'nv-transformers-v100.yml', 'test_qgzero.py', 'test_qwzero.py', 'test_zeropp.py']"
5d1124f2aaff1af3dd1fab8f142ae09cc25cc228,"[profiling]add show_straggler argument to log_summary() (#3579)

* add show_straggler argument to log_summary()

* Show straggler effect logging in seperate table

* fix formatting

* add docs for log_summary with straggler effect

* fix typo

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['fd1d2c64472c1a3061a05eb3b56a3f882199cfba'],False,"['comm.py', 'comms_logging.py', 'comms-logging.md']"
2ded2ff0be6ef5f1e592af7d2f7e05c1151a8d44,"checking process_group before merging bucket ranges (#3521) (#3577)

Co-authored-by: Alexander Jipa <azzhipa@amazon.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['5d1124f2aaff1af3dd1fab8f142ae09cc25cc228'],False,"['stage_1_and_2.py', 'test_moe.py', 'simple_model.py']"
d229ff175ebd036b21fc8b5fe95e597c54ec8528,"Zero3 Fix allreduce optimization for extra large tensor (#3832)

Grad tensors that don't fit in the bucket flat buffer are not added to it, but still added to params_in_ipg_bucket
if such tensors exists use reduce_scatter of params_in_ipg_bucket instead of allreduce. since allreduce assumes all grads are in ipg_bucket_flat_buffer.

Add test for reduce scatter=false
Fix padding to zeros instead of undefined values

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['807d1b5dfc630a13a15d5385e3857b5d5387078c'],False,"['stage3.py', 'test_zero.py']"
691d246e0216a5c209eff10a7a6ee8c78eed521e,"[zero] revert PR #3166, it disabled grad clip for bf16 (#3790)

* zero++ tutorial PR (#3783)

* [Fix] _conv_flops_compute when padding is a str and stride=1 (#3169)

* fix conv_flops_compute when padding is a str when stride=1

* fix error

* change type of paddings to tuple

* fix padding calculation

* apply formatting check

---------

Co-authored-by: Cheng Li <pistasable@gmail.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>

* fix interpolate flops compute (#3782)

* use `Flops Profiler` to test `model.generate()` (#2515)

* Update profiler.py

* pre-commit run --all-files

* Delete .DS_Store

* Delete .DS_Store

* Delete .DS_Store

---------

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>
Co-authored-by: Cheng Li <pistasable@gmail.com>

* revert PR #3166, it disabled grad clip for bf16

* ensure no loss scaling for non-fp16 dtypes

* revert PR #3611 (#3786)

* bump to 0.9.6

* ZeRO++ chinese blog (#3793)

* zeropp chinese blog

* try better quality images

* make title larger

* even larger...

* various fix

* center captions

* more fixes

* fix format

* remove staging trigger (#3792)

* DeepSpeed-Triton for Inference (#3748)

Co-authored-by: Stephen Youn <styoun@microsoft.com>
Co-authored-by: Arash Bakhtiari <arash@bakhtiari.org>
Co-authored-by: Cheng Li <pistasable@gmail.com>
Co-authored-by: Ethan Doe <yidoe@microsoft.com>
Co-authored-by: yidoe <68296935+yidoe@users.noreply.github.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>

* ZeRO++ (#3784)

Co-authored-by: HeyangQin <heyangqin@microsoft.com>
Co-authored-by: GuanhuaWang <alexwgh333@gmail.com>
Co-authored-by: cmikeh2 <connorholmes@microsoft.com>
Co-authored-by: Ammar Ahmad Awan <ammar.awan@microsoft.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>
Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Reza Yazdani <reyazda@microsoft.com>

* adding zero++ to navigation panel of deepspeed.ai (#3796)

* Add ZeRO++ Japanese blog (#3797)

* zeropp chinese blog

* try better quality images

* make title larger

* even larger...

* various fix

* center captions

* more fixes

* fix format

* add ZeRO++ Japanese blog

* add links

---------

Co-authored-by: HeyangQin <heyangqin@microsoft.com>
Co-authored-by: Conglong Li <conglong.li@gmail.com>

* Bug Fixes for autotuner and flops profiler (#1880)

* fix autotuner when backward is not called

* fix format

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>

* Missing strided copy for gated MLP (#3788)

Co-authored-by: Ammar Ahmad Awan <ammar.awan@microsoft.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>

* Requires grad checking. (#3789)

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>

* bump to 0.10.0

* Fix Bug in transform.cu (#3534)

* Bug fix

* Fixed formatting error

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>

* bug fix: triton importing error (#3799)

Co-authored-by: Stephen Youn <styoun@microsoft.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>

---------

Co-authored-by: Heyang Qin <heyangqin@microsoft.com>
Co-authored-by: Bill Luo <50068224+zhiruiluo@users.noreply.github.com>
Co-authored-by: Cheng Li <pistasable@gmail.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Guorun <84232793+CaffreyR@users.noreply.github.com>
Co-authored-by: stephen youn <13525892+stephen-youn@users.noreply.github.com>
Co-authored-by: Stephen Youn <styoun@microsoft.com>
Co-authored-by: Arash Bakhtiari <arash@bakhtiari.org>
Co-authored-by: Ethan Doe <yidoe@microsoft.com>
Co-authored-by: yidoe <68296935+yidoe@users.noreply.github.com>
Co-authored-by: GuanhuaWang <alexwgh333@gmail.com>
Co-authored-by: cmikeh2 <connorholmes@microsoft.com>
Co-authored-by: Ammar Ahmad Awan <ammar.awan@microsoft.com>
Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>
Co-authored-by: Reza Yazdani <reyazda@microsoft.com>
Co-authored-by: Masahiro Tanaka <81312776+tohtana@users.noreply.github.com>
Co-authored-by: Conglong Li <conglong.li@gmail.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Joe Mayer <114769929+jomayeri@users.noreply.github.com>
Co-authored-by: Ramya Ramineni <62723901+rraminen@users.noreply.github.com>",['d229ff175ebd036b21fc8b5fe95e597c54ec8528'],False,['stage_1_and_2.py']
c1c1d2496fa12f8548e2ccd22cbf591c8bccd361,"fix retrieval of out_channels in _conv_trans_flops_compute (#3834)

Co-authored-by: Pinstripe Potoroo <pinstripe-potoroo@users.github.com>
Co-authored-by: Cheng Li <pistasable@gmail.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['691d246e0216a5c209eff10a7a6ee8c78eed521e'],False,['profiler.py']
d81dfdabcc47e9831f1e123ee91367075e6956f7,"Fix LoRA Fuse/Unfuse in Hybrid Engine (#3563)

* fix lora fuse unfuse in hybrid_engine

* fix name

* fix typo

* remove empty lines

* Update gptj.py

* add lora test-case + fix gptneo implementation

* try to fix format

* try to accelerate testcase by reducing max length

* reduce test runtime

* Fix bloom / gpt-neox and add test for bloom

* fix CI + fix issue in engine

---------

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>
Co-authored-by: Ammar Ahmad Awan <ammar.awan@microsoft.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['c1c1d2496fa12f8548e2ccd22cbf591c8bccd361'],False,"['bloom.py', 'hybrid_engine.py', 'gptj.py', 'gptneo.py', 'gptneox.py', 'llama.py', 'opt.py', 'engine.py', 'hybrid_engine.py', 'test_he_lora.py']"
f3c93b056d55d8a52b123115ec853596da368211,"Add FALCON Auto-TP Support (#3640)

* Add FALCON auto-tp support
* added (skipped) unit test, refactored code to be more readable

---------

Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>",['385e89d4a81dd1c80e8658ddf0365377d32d9a2b'],False,"['auto_tp.py', 'replace_module.py', 'test_inference.py']"
db4638d157e6d6fb748f0dc87e15c72e45b58ba3,"Extend HE-Lora test with Z3 support + Fix/add guard in HE for Z3 (#3883)

* extend the test and fix fp16 typo.

* guard reset params with z3 enabled check.

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['59c9b0914f93b90b71dfcd46b9d169d28747deb4'],False,"['hybrid_engine.py', 'test_he_lora.py']"
cc3a7c9cba14dc59ea2618bf756dc98cb2f35c55,"Fix Meta Tensor checkpoint load for BLOOM models (#3885)

This PR fixes Meta Tensor checkpoint loading for BLOOM models where the SD keys start with transformer..",['d6f622176dd52305d1423210f75c7c25b4373888'],False,['load_checkpoint.py']
97e7b8410ccfc5e68c574b2a4d54b3ff77bee206,"fix error :Dictionary expression not allowed in type annotation Pylance (#3708)

* fix error :Dictionary expression not allowed in type annotation Pylance

* formatting

* formatting

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>
Co-authored-by: Michael Wyatt <mrwyattii@gmail.com>",['cc3a7c9cba14dc59ea2618bf756dc98cb2f35c55'],False,['tensor_fragment.py']
3491e32d72746ec3d990108a23e67b2666b3e0e0,"fix rnn flop profiler to compute flops instead of macs (#3833)

Co-authored-by: Guillaume Sautiere <gsautie@qti.qualcomm.com>
Co-authored-by: Cheng Li <pistasable@gmail.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>",['97e7b8410ccfc5e68c574b2a4d54b3ff77bee206'],False,['profiler.py']
52844f495655c6b77971afc9bcd193e640b754be,"Update workflows for merge queue (#3892)

* update workflow triggers for merge queue

* add branch specifier to trigger",['3491e32d72746ec3d990108a23e67b2666b3e0e0'],False,"['cpu-inference.yml', 'formatting.yml', 'nv-accelerate-v100.yml', 'nv-inference.yml', 'nv-lightning-v100.yml', 'nv-megatron.yml', 'nv-mii.yml', 'nv-pre-compile-ops.yml', 'nv-torch-latest-cpu.yml', 'nv-torch-latest-v100.yml', 'nv-transformers-v100.yml', 'python.yml']"
9aeba94a8ee130de7f3e66612dcb81f2b58c83df,"Avoid deprecation warnings in `CHECK_CUDA` (#3854)

The `type()` function is deprecated and `is_cuda()` can be used since about forever.
This avoids MANY warnings when compiling extensions.

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>",['52844f495655c6b77971afc9bcd193e640b754be'],False,"['fused_lamb_cuda.cpp', 'ds_transformer_cuda.cpp']"
b58e0fa92a5fa3012d2c7807357dd97aa2de8bb9,"avoid init for deepspeed backend first (#3893)

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['9aeba94a8ee130de7f3e66612dcb81f2b58c83df'],False,['comm.py']
c5e55d3d1477eeb839cf84362fb41954712118a6,"Fix a typo of global variable in comm.py(#3852) (#3852)

This is a bugfix for #3851, global variable cdb is mistakenly written as
cbd. There's no local variable of the same name in this function, so no
effect on functionality yet.

Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>",['b58e0fa92a5fa3012d2c7807357dd97aa2de8bb9'],False,['comm.py']
d24629f4fdaaa92df068de24f926d341f129112c,"[ROCm] Enable TestCUDABackward::test_backward unit tests (#3849)

* Workaround to pass unit/ops/accelerators/test_accelerator_backward.py unit tests on ROCm

* Rearranged is_rocm_pytorch()

* Introduced is_rocm_pytorch() for ROCm

* Fixed formatting errors

* Function call

* formatting fix

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Logan Adams <loadams@microsoft.com>",['c5e55d3d1477eeb839cf84362fb41954712118a6'],False,"['common.py', 'test_accelerator_backward.py']"
c520d476791e5b99ec4465f8d1843d6eb2609a31,"[profiling][mics]Fix some issues for log_summary(). (#3899)

* print_rank_0 is not defined in this python script

* Enable log_summary for mics feature based on Zero_Stage 3",['d24629f4fdaaa92df068de24f926d341f129112c'],False,"['utils.py', 'comms_logging.py']"
7e8bcc07d6532a380a7a439741609b058f083ed6,"fix ""undefined symbol: curandCreateGenerator"" for quantizer op (#3846)",['c520d476791e5b99ec4465f8d1843d6eb2609a31'],False,['builder.py']
af9a622a748eef2aaf73711c96298c79fdb10781,"fix memory leak with zero-3 (#3903)

* fix memory leak with z3

* __del__ is not good for zero-inference",['7e8bcc07d6532a380a7a439741609b058f083ed6'],False,['stage3.py']
55243f3bc8d4e751734ee2000fe3979bd4b6228c,fix some typo docs/ (#3917),['af9a622a748eef2aaf73711c96298c79fdb10781'],False,"['accelerator-abstraction-interface.md', 'data-efficiency.md', 'zero3.rst']"
ce535945e6dacb6ffbf17a7cbf7543fbd86385dd,fix: change ==NONE to is (#3923),['55243f3bc8d4e751734ee2000fe3979bd4b6228c'],False,"['__init__.py', 'base_tuner.py', 'ccl.py', 'comm.py', 'elasticity.py', 'runner.py', 'replace_module.py', 'diffusers_attention.py', 'diffusers_transformer_block.py', 'moe_inference.py', 'residual_add.py', 'engine.py', 'module.py', 'stage3.py', 'debug.py', 'numa.py']"
e292343d7bf693eb6cc87b7845d41cb6e90a63fa,"Del comment deepspeed.zero.Init() can be used as a decorator (#3894)

Object deepspeed.zero.Init() is not callable, it can't be used
as a decorator. Delete this code comment to avoid misunderstanding.

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['ce535945e6dacb6ffbf17a7cbf7543fbd86385dd'],False,['partition_parameters.py']
aef6c65ce39d191ca31618b2a995599942574fd9,"Reduce Unit Test Times (Part 3) (#3850)

* add coverage report

* define env vars in shared action

* reduce time for longest running tests

* fix broken shared action

* reduce test time

* reducing Pipeline test times

* further reducing test times

* rework Z3 test

* testing new mp.pool and persistent dist envs

* fix import

* reuse distributed environment for tests with lots of param combos

* fix for dist teardown

* fix pickling issue with pool cache

* actually fix pickling problem

* avoid running pool cache stuff on non-distributed tests

* fix issues with nested mp.pool

* fix for nested pools in Pipeline Engine

* re-add params

* update workflows with pytest opts

* implement feedback

* resolve race condition with port selection

* Update tests/unit/common.py

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['e59f69a8ff94bade1d06b868d5cc35df6969e556'],False,"['amd-mi100.yml', 'amd-mi200.yml', 'cpu-inference.yml', 'nv-accelerate-v100.yml', 'nv-h100.yml', 'nv-inference.yml', 'nv-lightning-v100.yml', 'nv-megatron.yml', 'nv-mii.yml', 'nv-nightly.yml', 'nv-torch-latest-cpu.yml', 'nv-torch-latest-v100.yml', 'nv-torch-nightly-v100.yml', 'nv-torch19-p40.yml', 'nv-torch19-v100.yml', 'nv-transformers-v100.yml', 'action.yml', 'requirements-dev.txt', '.coveragerc', 'conftest.py', 'alexnet_model.py', 'common.py', 'test_accelerator_backward.py', 'test_accelerator_forward.py', 'test_cpu_adagrad.py', 'test_adamw.py', 'test_cpu_adam.py', 'test_aio.py', 'test_onebit.py', 'test_fp16.py', 'test_pipe.py', 'test_data.py', 'test_ds_initialize.py', 'test_zero.py', 'test_zero_tensor_fragment.py']"
45cecc05fbcc431324ed1429830c37703fe8a1bc,"fix ""ERROR: failed to solve: nvidia/cuda:11.7.0-devel-ubuntu18.04: docker.io/nvidia/cuda:11.7.0-devel-ubuntu18.04: not found"" (#3930)

Update Nvidia docker version.

Fix ""ERROR: failed to solve: nvidia/cuda:11.7.0-devel-ubuntu18.04: docker.io/nvidia/cuda:11.7.0-devel-ubuntu18.04: not found""

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['aa54dba02532f3f487cc635dfdace8fa95593db0'],False,['Dockerfile']
ed34ddcaa9e539a899accbbf921e7b44711217bb,Fix docs for checkpoints (#3955),['45cecc05fbcc431324ed1429830c37703fe8a1bc'],False,['inference-tutorial.md']
4d9654163f843eb0561e7a9d7b383e8b96685f0b,fix Megatron-DeepSpeed links (#3956),['ed34ddcaa9e539a899accbbf921e7b44711217bb'],False,"['README.md', '2021-12-09-deepspeed-moe-nlg.md', '2022-07-26-deepspeed-azure.md', 'azure.md', 'bert-pretraining.md', 'curriculum-learning.md', 'data-efficiency.md', 'mixture-of-experts-inference.md', 'mixture-of-experts-nlg.md', 'model-compression.md', 'index.md']"
f5c834a6e0eedd440af85b0f577ca9c8f5624630,"fix(cpu_accelerator): :bug: Convert LOCAL_SIZE to integer (#3971)

Signed-off-by: Javier Salmeron Garcia <jsalmeron@vmware.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['31ac29ddb918816fcd52d3c481a18107c7a9ce7d'],False,['cpu_accelerator.py']
04b1f58e08a5c2ab2e2c7591cf0186666c811311,"fix duplicated unit test issue (#3951)

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['a1effc9170669859f74f735fb5f10306789e5557'],False,['test_inference.py']
a655d7d3f871df223182ba0b4572df6336f09596,"Switch to torch.linalg.norm (#3984)

* Switch to torch.linalg.norm

* Fix formatting",['04b1f58e08a5c2ab2e2c7591cf0186666c811311'],False,"['basic_layer.py', 'mpi.py', 'nccl.py', 'lamb.py', 'test_he_lora.py']"
fb9aebbf254d6d4535e42df75cb08b7c1dbd03f2,"Fix checkpoint conversion when model layers share weights (#3825)

* fix

* remove debug line

* remove debug line

* remove debug line

* add test case

* add test case

* use adam

* fix formatting

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['5dadf68771a7c80824519aeb86880d60e07c8677'],False,"['engine.py', 'test_shared_weights.py']"
7290aace9b966c671512559ab9912f6a0f89c361,"[CPU] Skip CPU support unimplemented error (#3633)

* skip cpu support unimplemented error and update cpu inference workflow

* add torch.bfloat16 to cuda_accelerator

* remove UtilsBuilder skip

* fused adam can build

* use cpu adam to implement fused adam

* enable zero stage 1 and 2 for synchronized accelerator (a.k.a. CPU)

* remove unused parameters

* remove skip FusedAdamBuilder; add suported_dtypes

* fix format

* Revert ""fix format""

Revert ""remove skip FusedAdamBuilder; add suported_dtypes""

Revert ""remove unused parameters""

Revert ""enable zero stage 1 and 2 for synchronized accelerator (a.k.a. CPU)""

Revert ""use cpu adam to implement fused adam""

Revert ""fused adam can build""

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Ma, Guokai <guokai.ma@intel.com>",['c79a104cf589b5353f00b02bb06a1352c85148d6'],False,"['cpu-inference.yml', 'abstract_accelerator.py', 'cpu_accelerator.py', 'cuda_accelerator.py', 'setup.py', 'test_latest_checkpoint.py', 'test_dist.py', 'test_elastic.py', 'test_he_all.py', 'test_he_llama.py', 'test_checkpoint_sharding.py', 'test_inference.py', 'test_model_profiling.py', 'test_accelerator_backward.py', 'test_accelerator_forward.py', 'test_adamw.py', 'test_fake_quantization.py', 'test_quantize.py', 'test_nhwc_bias_add.py', 'test_flops_profiler.py']"
8afcda2ac91f4d284cc96e4dbdd946f3a6cd7162,"ZeRO Gradient Accumulation Dtype. (#2847)

* Adding attributes for grad accum dtype.

* accumulating reduction grads in stage 2 mode 2

* missing colon

* tracking reduc grad move

* Correct hooks.

* Name change updates.

* Using grad_accum in cpu offload functions.

* Addressing comments: putting bf opt back, removing hooks

* Fixing missing pointer to grad accum.

* Renaming functions.

* More function renames.

* Adding reduction dtype.

* updating for offload

* Adding functionality for stage 3.

* Adding s3 test support.

* Add to MiCS optimizer.

* zero++ tutorial PR (#3783)

* Removing need to grad_reduc attribute.

* Offload correctness.

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Heyang Qin <heyangqin@microsoft.com>",['7290aace9b966c671512559ab9912f6a0f89c361'],False,"['engine.py', 'mics.py', 'stage3.py', 'stage_1_and_2.py', 'test_ds_initialize.py']"
1bc3b78423be188ea765d0180ea17e7b52adfe31,"[CPU] Use allreduce_low_latency for AutoTP and implement low latency allreduce for CPU backend (single node) (#3919)

* use allreduce_low_latency for AutoTP and implement low latency allreduce for CPU backend (single node)

* add fp32 support for SHM allreduce

* avoid assertion for FP16 data type

* fix format

* change 'allreduce_low_latency' to 'inference_allreduce'

* Fix according to comments

* change inference_allreduce to inference_all_reduce to keep naming consistency

* check whether LOCAL_SIZE is defined in ccl.cpp, also define LOCAL_SIZE in test_distributed

* fix format

* Fix format error

* Update tests/unit/comm/test_dist.py

Fix world_size to 4 in UT

Co-authored-by: Michael Wyatt <mrwyattii@gmail.com>

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Michael Wyatt <mrwyattii@gmail.com>",['8afcda2ac91f4d284cc96e4dbdd946f3a6cd7162'],False,"['ccl.cpp', 'ccl.py', 'comm.py', 'torch.py', 'layers.py', 'test_dist.py', 'common.py']"
7b850d3d040c350152d6b6664d30ebb78d73802b,"Re-enable skipped unit tests (#3939)

* fix skipped tests due to bad version check util function

* revert local changes in EP size

* remove added dummy test file

* refactor torch version check function

* Update util.py

* fix for moe test memory leak

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['1bc3b78423be188ea765d0180ea17e7b52adfe31'],False,"['test_moe_checkpoint.py', 'test_zero_optimizer.py', 'test_compression.py', 'test_configurable_parallel_mp.py', 'test_configurable_parallel_pp.py', 'test_moe.py', 'test_moe_tp.py', 'test_flops_profiler.py', 'test_onebit.py', 'test_fp16.py', 'test_ds_initialize.py', 'test_zero.py', 'util.py']"
0a0819b78554a5130b7c1b31d18779d39421dade,"Option to exclude frozen weights for checkpoint save (#3953)

* Option to exclude frozen weights for checkpoint save

* Extend unit test

* Support PP training",['ceccfa3ef68182384c6db1349fab43b9af3ed7f3'],False,"['engine.py', 'engine.py', 'module.py', 'test_zero_optimizer.py']"
0cc2d6ff256312efe1cc05592081267dbac9037d,"Fix user arg parsing in single node deployment (#4007)

* fix for single node launching with user args

* fix with shlex",['11f8e4a5c81ed0553ec1b371fd9d5eee229bdcf0'],False,['runner.py']
8e808392c89efe244b376700c69b5ae2b3a158cb,"Specify triton 2.0.0 requirement (#4008)

* specify triton 2.0.0 requirement

* fix for setup-venv action

* fix for install error

* fix torch install error",['0cc2d6ff256312efe1cc05592081267dbac9037d'],False,"['nv-inference.yml', 'nv-transformers-v100.yml', 'action.yml', 'requirements-dev.txt', 'requirements-inf.txt', 'requirements-triton.txt']"
6b2365e4faec59ba86f1af3501bddff661d6aff2,"Re-enable elastic training for torch 2+ (#4010)

* Switch elasticity function

* Switch elastic training to support torch 2+

* Add comment back in

* Move function to runtime.utils

* Formatting and removing newly unused torch import

* Typo fixed

* Forgot to replace unit with deepspeed.runtime

* Fixes

* Move more functions away from TORCH_MAJOR

* Remove more torch_major

* Fixes

* Remove additional torch_major/minor",['8e808392c89efe244b376700c69b5ae2b3a158cb'],False,"['utils.py', 'nccl.py', 'fused_optimizer.py', 'adam.py', 'lamb.py', 'zoadam.py', 'unfused_optimizer.py', 'utils.py', 'test_moe_checkpoint.py', 'test_zero_optimizer.py', 'test_compression.py', 'test_configurable_parallel_mp.py', 'test_configurable_parallel_pp.py', 'test_moe.py', 'test_moe_tp.py', 'test_flops_profiler.py', 'test_onebit.py', 'test_fp16.py', 'test_ds_initialize.py']"
23a11a39510e2aefb48236e3d2672a7dcbfc42a3,"Make Ascend NPU available (#3831)

* Make Ascend NPU available

NPU accelerator support is introduced in (#3595).
This commit provides two enhancements:
  1. Add a new accelerator_name 'npu' for choosing, it can be specified
by environment variable or auto detected.
  2. Optimize auto detect code in get_accelerator to avoid too many
layers of exception throwing.

* Use DS_ACCELERATOR_LIST for overriding accelerators

When detecting override accelerators there's an error message to show
all support accelerators, using an accelerator list instead of hard
coding accelerator names in this message.

And fix code format issue(yapf).

* Add HCCL backend

HCCL is the distribute backend of Ascend NPU, it already implemented in
npu plugin for pytorch (https://gitee.com/ascend/pytorch). Add HCCL
backend as a not implemented backend to avoid not supported warning.

* Add NPUNotImplementedBuilder

Ascend NPU does not implement any op yet, leave npu folder empty will
throw NoneType[op_name] when not supported op is called. Add this
NPUNotImplementedBuilder as the default builder.

* Optimize builder search logic

1. cpu and other backend implement their ops in sub dirs under
op_builder, cuda_accelerator should skip these sub dirs.
2. Each backend will have its own NotImplementedBuilder, add device
prefix to this class to distinguish.

* Change the unimplemented builder name to the same for each backend",['19d5c03d0a1f24c85aa2beb2914db5694236580c'],False,"['cuda_accelerator.py', 'npu_accelerator.py', 'real_accelerator.py', 'comm.py', 'constants.py', '__init__.py', 'builder.py', 'no_impl.py']"
f4d18fa2b7184b110963cd3c61451283d678f4c9,"fix gates size retrieval logic in _rnn_flops (#3921)

Co-authored-by: Pinstripe Potoroo <pinstripe-potoroo@users.github.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['23a11a39510e2aefb48236e3d2672a7dcbfc42a3'],False,['profiler.py']
777ae39a85988da3e934c272842f6c65686b8896,fix typo in SECURITY.md (#4019),['f4d18fa2b7184b110963cd3c61451283d678f4c9'],False,['SECURITY.md']
0f5406323cc3d0d1c6314cd39ba4cf27a57c9415,"[CPU] FusedAdam and CPU training support (#3991)

* fused adam can build

* use cpu adam to implement fused adam

* enable zero stage 1 and 2 for synchronized accelerator (a.k.a. CPU)

* remove unused parameters

* fix format error

* Remove adam class

* fix format

* support stage3

* reuse simd.h

* fix format

* make memory_stat return meaningful dict

* fix format

* add cpu_adam

* reuse cpu_adam

* header cleanup

* fix cpu_adam

* fix format, add missing file

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['1cc9caa9c65ee319a8e46abd9a725ff98048d53e'],False,"['cpu_accelerator.py', 'cpu_adam.cpp', 'cpu_adam_impl.cpp', 'fused_adam.cpp', 'ccl.cpp', 'cpu_adam.h', 'utils.py', 'parameter_offload.py', 'partition_parameters.py', 'partitioned_param_coordinator.py', 'stage3.py', 'stage_1_and_2.py', '__init__.py', 'cpu_adam.py', 'fused_adam.py', 'cpu_adam.py']"
09601bb811b28fb0db92b6dcb2b737873e6677e8,fix(pipe): `strict` parameter of method `PipelineModule.load_state_dir` is not work (#4020),['eeab613ab888da19e9a46285c6c87f45e5c4ce63'],False,['module.py']
15f94ae756476579e4e05781a25897a177bdbf93,"Engine side fix for loading llama checkpoint fine-tuned with zero3 (#3981)

* Engine side fix for loading llama checkpoint fine-tuned with zero3

* Fixes to support llama fine-tuning in ds-chat

* Refactored the code to avoid using an except block.

* formatting

* revert permissions change

---------

Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>",['b354c28b760efb46ff04410739fd574583530441'],False,"['llama.py', 'ds_mlp.py']"
389bf6931949ad85ecc0277eefd5bb7f6f5af285,fix: Remove duplicate word the (#4051),['15f94ae756476579e4e05781a25897a177bdbf93'],False,"['basic_layer.py', 'meta_tensor.py', 'contiguous_memory_allocator.py', 'stage_1_and_2.py', 'config-json.md', 'gan.md', 'onebit-adam.md', 'onebit-lamb.md', 'progressive_layer_dropping.md', 'zero-one-adam.md', 'autotuning.rst', 'monitor.rst']"
0b507253e5b424994517cd7f7964c954fc1dadcc,fix comm logging for inference (#4043),['389bf6931949ad85ecc0277eefd5bb7f6f5af285'],False,['comms_logging.py']
76953a37b7a13dd727c0a8ee02fd47dc0364e891,"fix opt-350m shard loading issue in AutoTP (#3600)

Signed-off-by: Wang, Yi A <yi.a.wang@intel.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>
Co-authored-by: Reza Yazdani <44502768+RezaYazdaniAminabadi@users.noreply.github.com>",['0b507253e5b424994517cd7f7964c954fc1dadcc'],False,"['replace_module.py', 'test_checkpoint_sharding.py']"
6b877d2dbc879851dc5f9b7a4fc547eb4a3724b2,"autoTP for fused qkv weight (#3844)

* autoTP for fused qkv weight

* fix format

* clean up

* clean up

* clean up

* update

* make logic flow to util and move to file

* fix formatting

* remove empty line

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Reza Yazdani <44502768+RezaYazdaniAminabadi@users.noreply.github.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['0bafeac491da935ae192bc07ab0e946bb92ac5be'],False,"['auto_tp.py', 'fusedqkv_utils.py', 'replace_module.py']"
7f90ef4bddde0604f80857cf808d4309a1db672e,"Multiple zero stage 3 related fixes (#3886)

* Option to override module apply

* Removing early partitioning in override

* Unit tests

* Cleanup

* Adapt unit test to succeed

* Handle missed params

* Add accelerate

* Code cleanup

* Add doc

* Add doc

* Add doc",['7f26bb6ae47c352efeabf52f827108c42a1a55eb'],False,"['engine.py', 'config.py', 'partition_parameters.py', 'zero3.rst', 'requirements-dev.txt', 'test_zero_nesting_init.py']"
82c498d947151fc8bbc586031544dc0f5a693cd7,"Fix deadlock when SHM based allreduce spin too fast (#4048)

* Fix deadlock when allreduce spin too fast

* Change state to enum to increase readability

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['7f90ef4bddde0604f80857cf808d4309a1db672e'],False,['ccl.cpp']
8a63754bcef47e91ee6f38640043409d752d12a7,"save_non_zero_checkpoint on first partition group (#3787)

Co-authored-by: Zhen Zhang <zhzhn@amazon.com>",['82c498d947151fc8bbc586031544dc0f5a693cd7'],False,"['engine.py', 'test_mics_optimizer.py']"
f763b93d94ae149de8f2e0e0c9c8d05ead359493,"add reproducible compilation environment (#3943)

* add reproducible compilation environment

* fix ci

* fix typo for formatting check

* Fix casing for format

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Michael Wyatt <mrwyattii@gmail.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Logan Adams <loadams@microsoft.com>",['8a63754bcef47e91ee6f38640043409d752d12a7'],False,"['advanced-install.md', 'environment.yml']"
e31b40411feda607471eac05d12301b378df67cd,fix: remove unnessary `#` punct in the second `sed` command (#4061),['f763b93d94ae149de8f2e0e0c9c8d05ead359493'],False,['Dockerfile']
94c7233a8bb51e068ff8dd5d3e03f2e9b5ab248e,"Refactor autoTP inference for HE (#4040)

* Refactor autoTP inference for HE

* Formatting

* Move redundant functions to autotp

* Remove self from loading class

* formatting

* Some gpt2 autotp path fixes

* precommit",['e31b40411feda607471eac05d12301b378df67cd'],False,"['auto_tp.py', 'replace_module.py']"
1ba4098918d69fc3b2081f346802f2b58ac3d96e,"Fix Stable Diffusion Injection (#4078)

* Initial commit

* Clean up

* Fix formatting",['a7fe3bcc353c072846e4f86acff5cbfd758e2ec9'],False,"['engine.py', 'vae.py', 'replace_module.py', 'config.py', 'diffusers_attention.py']"
4cde5da88e1d24390031a199170d4a2cfb671afd,fix typo: change polciies to policies (#4090),['e8318634b4313eaad89842cf4322e1762d34ced3'],False,"['auto_tp.py', 'replace_module.py']"
85dc854b11a66ec18e0f54e23eb72eeeddf9c81a,"update ut/doc for glm/codegen (#4057)

* update ut/doc for glm/codegen

* formatting/spacing on docs

* re-order/alphabetize the models

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Logan Adams <loadams@microsoft.com>",['4cde5da88e1d24390031a199170d4a2cfb671afd'],False,"['automatic-tensor-parallelism.md', 'test_inference.py']"
57a27b0803ce1fb31a24f7df1a00f7f86c912f66,"add type checker ignore to resolve that pylance can't resolved noqa annotation (#4102)

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['241ae39a29ed74caf2d48e40341732475d585768'],False,"['cpu_accelerator.py', 'cuda_accelerator.py', 'mps_accelerator.py', 'npu_accelerator.py', 'real_accelerator.py', '__init__.py', 'env_report.py', 'git_version_info.py', 'utils.py', '__init__.py', 'builder.py', 'builder.py', 'builder.py', 'flatten_bench.py', 'unflatten_bench.py', 'modeling.py', 'modelingpreln.py', 'test_layer_norm.py', 'test_fp16.py']"
975bcbc0bdec3df7830f95f54906d904aa8add78,set temperature to avoid config validation error (#4107),['57a27b0803ce1fb31a24f7df1a00f7f86c912f66'],False,['test_inference.py']
1e0c39c6bf54f7b3f0ac1c31a34e3cabbd2c3c9d,"enable pipeline checkpoint loading mode (#3629)

In cpu ram limited machine, loading checkpoint at the start up may
cause oom as all rank in the same node are loading the opt state
in the same time. So for this scenario, we make a choice that loading
checkpoint could be made pipeline way.

Signed-off-by: Lei Wen <wenlei03@qiyi.com>
Co-authored-by: Lei Wen <wenlei03@qiyi.com>",['78d985ab21b126811dc891666c833378b157911d'],False,"['engine.py', 'config.py', 'mics.py', 'stage3.py', 'stage_1_and_2.py', 'test_zero_optimizer.py', 'test_zero_config.py']"
8a8683d343cd0c55be8a6c4c7e568dd23f574a63,"Fix Issue 4083 (#4084)

* removing bad check

* adding offload check for bf16 optimizer

* grad reduce for extra large param

* check grad_accum exists before converting

---------

Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>",['1e0c39c6bf54f7b3f0ac1c31a34e3cabbd2c3c9d'],False,"['engine.py', 'stage_1_and_2.py']"
0c75f4a3f937febc8c15610fcab7b81466b216c7,"Update nightly workflows to open an issue if CI fails (#3952)

* Update H100 workflow to open an issue if nightly CI fails

* Test running as not CI

* Add all nightly/switch envvar name

* Test with AMD

* Add way to get url, switch path of template

* Add additional checkout step

* Move actions checkout step

* Try absolute path with github workspace

* Create issue without template/path

* Re-enable and add debug logic

* add if failed()

* More debug

* Try without checkout action uses

* Rename file

* Update variables

* Update issue template

* Confirm removing permissions still work

* Revert ""Confirm removing permissions still work""

This reverts commit e7c2915adc4f13baf697da393a3a17098faa7cff.

* Re-enable permissions

* Remove PR trigger for AMD MI200 tests

* Revert ""Remove PR trigger for AMD MI200 tests""

This reverts commit 5c5c5fd67b34553bcb7efc1a03f57fa2d98d1931.

* Test update_existing

* Switch to composite action

* Fix line ending encoding issue

* Switch failure to be a variable

* Test with second workflow

* Format fix

* Switch failure to always

* Switch back to previously working way

* Test permission changes

* Revert ""Test permission changes""

This reverts commit e051da759bb19cf3697461487a00e90623d99177.

* Update existing bugs with newest build failure link

* Remove PR triggers for that were used for testing.",['d300517fbadfe03631672ee4164c2b1fc293f699'],False,"['ci_failure_report.md', 'amd-mi200.yml', 'nv-h100.yml', 'nv-nightly.yml', 'nv-torch-nightly-v100.yml', 'nv-torch19-p40.yml', 'nv-torch19-v100.yml']"
ff7d5275f2aa916cb5f320e0d817154e96f9cdb6,"Update torch1.9 tests to 1.10 to match latest accelerate. (#4126)

* Fix torch19 tests

* test pip list and --no-build-isolation

* Enable verbosity

* pin to older accelerate version

* Update oldest tested torch to 1.10

* Properly rename directories

* Return PR tests to CI again.

* Remove -vv",['0c75f4a3f937febc8c15610fcab7b81466b216c7'],False,"['nv-torch110-p40.yml', 'nv-torch110-v100.yml']"
629b203939a5cbf659c9316cb47c213c67665884,"Handle PermissionError in os.chmod Call - Update engine.py (#4139)

* Update engine.py

This branch includes changes to handle potential exceptions that may occur when attempting to change file permissions using the os.chmod function within the DeepSpeed engine. The specific issue addressed is the PermissionError that may arise when working with certain filesystems or under restricted permissions.

* Change to use logger

* Split permissions out and add unit test

* UnitTest(use DistTestClass) + trailing whitespace

* update unit test

* UT parametrize 1, 2 ,3

* trim white space from unit test

* change to PermissionError

* run pre-commit formats

* Catch FileNotFoundError & PermissionError",['ff7d5275f2aa916cb5f320e0d817154e96f9cdb6'],False,"['engine.py', 'test_zero_optimizer.py']"
7a282db8ca81bead4824990adbaf750a414beb15,"Generalize frozen weights unit test (#4140)

* Fix unit test

* Fix unit test",['629b203939a5cbf659c9316cb47c213c67665884'],False,['test_zero.py']
9d79cfd1e90cae9306dc1b5837d374b2c9489ac8,"Respect memory pinning config (#4131)

* Respect memory pinning config

* Bug fix",['7a282db8ca81bead4824990adbaf750a414beb15'],False,"['engine.py', 'stage_1_and_2.py']"
341cefd2a406f578ee05b3a9d0686553926a7e61,"Return nn.parameter type for weights and biases (#4146)

* Return nn.parameter type for weights and biases

* whitespace

* Fix bias tensor size",['a452301879eeff2c8573cfc9a7491a9353f16cc8'],False,['auto_tp.py']
740b78050de7d625bfcecf6220e0d9faa74e07fe,Fixes #4151 (#4152),['341cefd2a406f578ee05b3a9d0686553926a7e61'],False,['engine.py']
bd65eeaf708dfab70e372642284c6af06d506258,fix badges (#4162),['1a2957394638a57950495c17f0e0a4a742eb7226'],False,['README.md']
64c670ef021118ed42a98d5dde249f35878a07a7,"Add DS-Chat CI workflow (#4127)

* Add DS Chat CI workflow

* Add CRITIC_CKPT_DIR env variable to actions.yml

* Update step 2 opt 125m ckpt dir name

* Update test dir

* Add workflow_dispatch

* Add :

* Add nv-ds-chat badge to main README

* Open GH issue if DS Chat CI fails

* Remove pull_request and merge_group conditions

* Update and test torch version

* Remove PR trigger

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['bd65eeaf708dfab70e372642284c6af06d506258'],False,"['nv-ds-chat.yml', 'action.yml', 'README.md']"
19e9a7c0284cc886d84afd7ec58721af5e045811,"[CPU][Bugfix] Make uid and addr_port part of SHM name in CCL backend (#4115)

* distinguish shm name with uid and addr_port

* fix formatting",['64c670ef021118ed42a98d5dde249f35878a07a7'],False,['ccl.cpp']
7711bdbbd27c62ab4986f35c1ed01a0268fed92f,"MP ZeRO++  (#3954)

* zero++ tutorial PR (#3783)

* [Fix] _conv_flops_compute when padding is a str and stride=1 (#3169)

* fix conv_flops_compute when padding is a str when stride=1

* fix error

* change type of paddings to tuple

* fix padding calculation

* apply formatting check

---------

Co-authored-by: Cheng Li <pistasable@gmail.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>

* fix interpolate flops compute (#3782)

* use `Flops Profiler` to test `model.generate()` (#2515)

* Update profiler.py

* pre-commit run --all-files

* Delete .DS_Store

* Delete .DS_Store

* Delete .DS_Store

---------

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>
Co-authored-by: Cheng Li <pistasable@gmail.com>

* revert PR #3611 (#3786)

* bump to 0.9.6

* ZeRO++ chinese blog (#3793)

* zeropp chinese blog

* try better quality images

* make title larger

* even larger...

* various fix

* center captions

* more fixes

* fix format

* remove staging trigger (#3792)

* DeepSpeed-Triton for Inference (#3748)

Co-authored-by: Stephen Youn <styoun@microsoft.com>
Co-authored-by: Arash Bakhtiari <arash@bakhtiari.org>
Co-authored-by: Cheng Li <pistasable@gmail.com>
Co-authored-by: Ethan Doe <yidoe@microsoft.com>
Co-authored-by: yidoe <68296935+yidoe@users.noreply.github.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>

* ZeRO++ (#3784)

Co-authored-by: HeyangQin <heyangqin@microsoft.com>
Co-authored-by: GuanhuaWang <alexwgh333@gmail.com>
Co-authored-by: cmikeh2 <connorholmes@microsoft.com>
Co-authored-by: Ammar Ahmad Awan <ammar.awan@microsoft.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>
Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Reza Yazdani <reyazda@microsoft.com>

* adding zero++ to navigation panel of deepspeed.ai (#3796)

* Add ZeRO++ Japanese blog (#3797)

* zeropp chinese blog

* try better quality images

* make title larger

* even larger...

* various fix

* center captions

* more fixes

* fix format

* add ZeRO++ Japanese blog

* add links

---------

Co-authored-by: HeyangQin <heyangqin@microsoft.com>
Co-authored-by: Conglong Li <conglong.li@gmail.com>

* Bug Fixes for autotuner and flops profiler (#1880)

* fix autotuner when backward is not called

* fix format

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>

* Missing strided copy for gated MLP (#3788)

Co-authored-by: Ammar Ahmad Awan <ammar.awan@microsoft.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>

* Requires grad checking. (#3789)

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>

* bump to 0.10.0

* Fix Bug in transform.cu (#3534)

* Bug fix

* Fixed formatting error

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>

* bug fix: triton importing error (#3799)

Co-authored-by: Stephen Youn <styoun@microsoft.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>

* init commit for mixed precision lora

* fix format

* patch _allgather_params & minor fixes

* make sure initial quantization are finished

* make sure dequantization is finished

* skip quantization for small parameters

* fix format

* remove unused async_op

* lazy load of quantizer kernels

* add mixed precision lora tutorial

* cleanup mics

* cleanup mics

* replace get_accelerator().current_device()

* add kwargs to mics

* fix format

* seperate code and tutorial

* fix _all_gather in zero3

---------

Co-authored-by: Bill Luo <50068224+zhiruiluo@users.noreply.github.com>
Co-authored-by: Cheng Li <pistasable@gmail.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Guorun <84232793+CaffreyR@users.noreply.github.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>
Co-authored-by: stephen youn <13525892+stephen-youn@users.noreply.github.com>
Co-authored-by: Stephen Youn <styoun@microsoft.com>
Co-authored-by: Arash Bakhtiari <arash@bakhtiari.org>
Co-authored-by: Ethan Doe <yidoe@microsoft.com>
Co-authored-by: yidoe <68296935+yidoe@users.noreply.github.com>
Co-authored-by: GuanhuaWang <alexwgh333@gmail.com>
Co-authored-by: cmikeh2 <connorholmes@microsoft.com>
Co-authored-by: Ammar Ahmad Awan <ammar.awan@microsoft.com>
Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>
Co-authored-by: Reza Yazdani <reyazda@microsoft.com>
Co-authored-by: Masahiro Tanaka <81312776+tohtana@users.noreply.github.com>
Co-authored-by: Conglong Li <conglong.li@gmail.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Joe Mayer <114769929+jomayeri@users.noreply.github.com>
Co-authored-by: Ramya Ramineni <62723901+rraminen@users.noreply.github.com>",['f036f00c3763694e539a9070a98130e2667e49bd'],False,"['engine.py', 'config.py', 'mics.py', 'parameter_offload.py', 'partition_parameters.py', 'partitioned_param_coordinator.py', 'stage3.py']"
d9a889d55921e1300ca21b26c85d63e4de8e3a74,"Fix nv-nightly workflow (#4163)

* Disable nv-nightly workflow since it doesn't work

* Run on PRs to debug

* fix for nv-nightly

* fix

* OOM fix?

* Update nv-nightly.yml

---------

Co-authored-by: Logan Adams <loadams@microsoft.com>",['5e16eb2c939707d0d0062a458d77998fccb3afad'],False,"['common.py', 'test_inference.py']"
6c684e12901ddd13f0ac48043daf8f581d785445,Fix path (#4193),['d9a889d55921e1300ca21b26c85d63e4de8e3a74'],False,['model-compression.md']
9723a8794f0d85158e5564c1a07005bdac992904,"Add unit test to check HF low_cpu_mem_usage_flag  (#4184)

* Add unittest to check huggingface low_cpu_mem_usageflag

* change lag to true

* Formatting has changes

* Indentation fix

* Fix chanves

* final format fix

* Accidently dropped pytestmark from other test

* Remove invalid model test config as that was removed.

* Whitespace and PR feedback

* Format and PR feedback means that we can remove the import we added.

* Update tests/unit/inference/test_inference.py

* Update tests/unit/inference/test_inference.py

---------

Co-authored-by: Lok Chand Koppaka <lokoppak@microsoft.com>
Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>",['6c684e12901ddd13f0ac48043daf8f581d785445'],False,['test_inference.py']
426810a254d87d65cbd01a156dc7d90f2a878901,"Fix ZeRO parameter initialization for tensors with `requires_grad=True` (#4138)

* Fix ZeRO parameter initialization for tensors with `requires_grad=True`

* Simplify detach logic

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['9723a8794f0d85158e5564c1a07005bdac992904'],False,['partition_parameters.py']
b5453990b47db415e03c9abc26cec208e9a26863,"DeepSpeed Ulysses tutorial (#4200)

* add tutorial file from Minjia.

* fix format.

---------

Co-authored-by: Ammar Ahmad Awan <ammar.awan@microsoft.com>",['426810a254d87d65cbd01a156dc7d90f2a878901'],False,['ds-sequence.md']
6df158733db02a6d921d52443b67a6912054f7e4,"Load z3 checkpoints for inference (#4171)

* Load z3 checkpoints for inference

* PR feedback

* Fix API bugs

* Fix typo",['b5453990b47db415e03c9abc26cec208e9a26863'],False,"['engine.py', 'engine.py', 'parameter_offload.py', 'stage3.py']"
3e82cb6443b2b8b4da9f322a8e3c1fa18e80651a,[docs] fix pypi badge,['5de0662ce41478f76a36baebdab1fc0476620c10'],False,['README.md']
961827be5102ac72dfd07091899d1e3ba9f4de04,"DS-Ulysses formating (#4204)

* fix identation

* fix formatting

---------

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['3e82cb6443b2b8b4da9f322a8e3c1fa18e80651a'],False,"['README.md', 'README.md']"
10bef7acc555054ed72febe85e16ab2be712b58e,"Update Ulyssess (#4205)

* Update README.md

* Update README.md

* Format fix

---------

Co-authored-by: Logan Adams <loadams@microsoft.com>",['961827be5102ac72dfd07091899d1e3ba9f4de04'],False,['README.md']
3808273c2a005fea8bcde0e8c41890ecf778d765,"Add Japanese blog of DS-Ulysses (#4209)

* add Japanese blog of DS-Ulysses

* fix fig

---------

Co-authored-by: Conglong Li <conglong.li@gmail.com>",['c274e51212e075d2c911bbfdf6e18ede97b89746'],False,['README.md']
63e17769939e67872bfaf3b01014b5d495c3a535,"DeepSpeed Ulysses Chinese blog translation (#4210)

* Chinese translation with Conglong's feedback

* fix format

---------

Co-authored-by: Conglong Li <conglong.li@gmail.com>",['3808273c2a005fea8bcde0e8c41890ecf778d765'],False,['README.md']
9647ea791d40f867e7e6f75674a654f63b13aae0,"Add MuP optimizers (#2043)

* added paths for mup optimizers

* added tests

* formatting

* Add license, fix missing distributed test, formatting

* Add mpi4py to confirm tests work

* Undo requirements change

* Move to runtime folder

* Rework to match new format

* missing comma

* hidden dim fix

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Logan Adams <loadams@microsoft.com>",['d6c2e6b0113af0a06c46d9064a47193233a70e2e'],False,"['config.py', 'engine.py', 'requirements-dev.txt', 'test_mup_optimizers.py']"
0b7a760c35c6cfcfb4151d1991555ee9b05ad096,"Fixes timer error referenced in #4212 (#4213)

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['0712e299205b641ddce7f2c010d45f8fe635ed67'],False,['engine.py']
c69bd1f7b7768720a488b126fbf9cef6653c3890,"Fix pipline dataloader when batch elements contain tuple (#565)

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>
Co-authored-by: Ammar Ahmad Awan <ammar.awan@microsoft.com>",['0b7a760c35c6cfcfb4151d1991555ee9b05ad096'],False,['engine.py']
42c1e916f63883cd52928ba5c730f91827abda49,"feat(activation_checkpointing): add `non_reentrant_checkpoint` to support inputs require no grad (#4118)

* feat: add `non_reentrant_checkpoint`

* feat: add missing output postprocess and change the hook to record leaf forward tensor refs

* fix: make the multi_grad_hook registered after graph construction

* fix: backward compatibility for multi_tensor_hook

* fix: nonlocal reference error of deepspeed_saved_tensors

* fix: reduce repeating hook registration

* test: add test for `activation_checkpointing.checkpointing.non_reentrant_checkpoint`

* Pass correct node size for ZeRO++ (#4085)

* Pass correct node size

* formatting

---------

Co-authored-by: Connor Holmes <development@cmikeh2.me>
Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>

* add deepspeed chat arxiv report (#4110)

* add deepspeed chat arxiv report

* add zeroquant v2 and fp

* add selective enhencement

* add ignore for 'Youn' in spell checker

---------

Co-authored-by: yaozhewei <zheweiy@berkeley.edu>
Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>

* style: change flake8 detected style missmatch

* test: hack to clone the `test_activation_checkpointing` module for reuse and add regression tests

* doc: explain the introduction of `non_reentrant_checkpoint`

* doc: explain the test of `non_reentrant_checkpoint`

---------

Co-authored-by: Connor Holmes <connorholmes@microsoft.com>
Co-authored-by: Connor Holmes <development@cmikeh2.me>
Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>
Co-authored-by: Conglong Li <conglong.li@gmail.com>
Co-authored-by: yaozhewei <zheweiy@berkeley.edu>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['c69bd1f7b7768720a488b126fbf9cef6653c3890'],False,"['checkpointing.py', 'test_activation_checkpointing_non_reentrant.py']"
042115c80b11862f1e2cd030fd3747cadf9ef868,"Fix fused qkv sizing for bloom (#4161)

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['60d7b0a39da25e78027765e0ca49c95f28653150'],False,['fusedqkv_utils.py']
8145b5e41f777a73fe2afd09b8b8d11650c5a1f2,"added port argument for ssh (#4117)

* added port argument for ssh

* changed arg name; moved PDSH arg; added if-else for arg

* fixed missing key error

* updated test code to correspond to the change in multinode_re_runner.py

* changed default ssh port to None

* Update deepspeed/launcher/runner.py

* formatting

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Michael Wyatt <mrwyattii@gmail.com>",['042115c80b11862f1e2cd030fd3747cadf9ef868'],False,"['multinode_runner.py', 'runner.py', 'test_multinode_runner.py']"
194b0043a9362e64e2de57c90f5e1e43805e0c7d,"fix: linker issues in conda environments #3929 (#4235)

* fix: linker issues in conda environments #3929

* ignore: re-ordering

* Update builder.py",['57d629a17e93ec73333c80991f5998c585c8730a'],False,['builder.py']
6cbf66613137ab6b4cf6cfa7f61974f9ec5eb9ab,"fix MegatronLayerPolicy to be compatible with the newest ParallelTransformerLayer (#4236)

Co-authored-by: Reza Yazdani <44502768+RezaYazdaniAminabadi@users.noreply.github.com>",['5dbc531328dfd7e33889831995d1af44b698b2e9'],False,['megatron_gpt.py']
462def451e06f10f12916c49f34800d52ab52446,"Enable hpz when running with torch.no_grad (#4232)

* enable hpz when running with torch.no_grad

* change the way to detect no_grad

* fix format

---------

Co-authored-by: Ammar Ahmad Awan <ammar.awan@microsoft.com>",['6cbf66613137ab6b4cf6cfa7f61974f9ec5eb9ab'],False,['parameter_offload.py']
a23cda6c3bb27b141b13ba231a53190507bf35ae,"Allow modification of zero partitioned parameters (#4192)

* Modify zero parameters

* Docs

* py3.6 compatibility

* Update docs

* Update deepspeed/runtime/zero/stage3.py

Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>

* Add TODO

* Formatting

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>",['f96c1c0a783bc621bb607ec5b6650c804abfe7df'],False,"['stage3.py', '__init__.py', 'mixed_precision_linkage.py', 'tensor_fragment.py', 'zero3.rst', 'test_zero_tensor_fragment.py']"
430510bfce5c8e2e8f2108b73824aef4a37fcaf1,"Checks for user injection policy (#3052)

* check injection policy

* transformers v4

* move check_inference_tuple

* user injection policy check in infer engine

* fix pre-commit format

* fix formatting

* fix clang format

---------

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>
Co-authored-by: Lev Kurilenko <113481193+lekurile@users.noreply.github.com>",['a23cda6c3bb27b141b13ba231a53190507bf35ae'],False,"['engine.py', 'module_quantize.py', 'replace_module.py']"
c93e89a38c7aa8e0b7f7e54edce00b6cef1d7fd0,"Add check that opening issues on CI failure requires schedule (#4242)

* Add check that opening issues on CI failure requires build to be scheduled

* Update ()",['430510bfce5c8e2e8f2108b73824aef4a37fcaf1'],False,"['amd-mi200.yml', 'nv-ds-chat.yml', 'nv-h100.yml', 'nv-nightly.yml', 'nv-torch-nightly-v100.yml', 'nv-torch110-p40.yml', 'nv-torch110-v100.yml']"
55d9964c59c0c6e23158b5789a5c36c28939a7b0,"Fix nv-inference/un-pin transformers (#4269)

* Fix bump_patch_version.py to update version.txt post GH release

* Un-pin transformers

* Unmerge changes from another branch",['e801e6d718c386ef90fef7d9576cd40370e6a9cb'],False,['nv-inference.yml']
60a3e89eed258e9edfb998266cd2a9d03b7ae781,"use ```non_reentrant_checkpoint``` fix requires_grad of input must be true for activation checkpoint layer in pipeline train. (#4224)

* feat: add `non_reentrant_checkpoint`

* feat: add missing output postprocess and change the hook to record leaf forward tensor refs

* fix: make the multi_grad_hook registered after graph construction

* fix: backward compatibility for multi_tensor_hook

* fix: nonlocal reference error of deepspeed_saved_tensors

* fix: reduce repeating hook registration

* test: add test for `activation_checkpointing.checkpointing.non_reentrant_checkpoint`

* Pass correct node size for ZeRO++ (#4085)

* Pass correct node size

* formatting

---------

Co-authored-by: Connor Holmes <development@cmikeh2.me>
Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>

* add deepspeed chat arxiv report (#4110)

* add deepspeed chat arxiv report

* add zeroquant v2 and fp

* add selective enhencement

* add ignore for 'Youn' in spell checker

---------

Co-authored-by: yaozhewei <zheweiy@berkeley.edu>
Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>

* style: change flake8 detected style missmatch

* test: hack to clone the `test_activation_checkpointing` module for reuse and add regression tests

* doc: explain the introduction of `non_reentrant_checkpoint`

* doc: explain the test of `non_reentrant_checkpoint`

* apply non_reentrant_checkpoint in pipeline parallel training

* ut pass

* fix ci

* reduce check level for ci

---------

Co-authored-by: hughpu <hughpu@hotmail.com>
Co-authored-by: Hugh Pu <31498041+hughpu@users.noreply.github.com>
Co-authored-by: Connor Holmes <connorholmes@microsoft.com>
Co-authored-by: Connor Holmes <development@cmikeh2.me>
Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>
Co-authored-by: Conglong Li <conglong.li@gmail.com>
Co-authored-by: yaozhewei <zheweiy@berkeley.edu>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Masahiro Tanaka <81312776+tohtana@users.noreply.github.com>",['16d8953a3790e46bb47eab86bb48e61fb1351c8a'],False,"['engine.py', 'module.py', 'test_pipe.py']"
bce6ed1ca95f69c07b57d4d8534d2c430339f90f,"fix iteration timing used in autotuning when gradient_accumulation_steps > 1 (#2888)

* fix iteration timing when gas > 1

* fix formatting

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['18a04d04a52e70cf2e1d0eb6d38df0dad110f937'],False,['engine.py']
c55fad9f0c5f870f12376edc054d6d6413eafec1,"Update README.md (#4284)

fixed a few typos",['bce6ed1ca95f69c07b57d4d8534d2c430339f90f'],False,['README.md']
e8ed7419ed40306100f0454bf85c6f4cc4d55f34,"update deepspeed to run with the most recent triton 2.1.0 (#4278)

* fix codes to work with triton 2.1
tl.libdevice and triton.testing.allclose are gone with triton2.1

* formatting

* formatting

---------

Co-authored-by: Stephen Youn <styoun@microsoft.com>
Co-authored-by: Lev Kurilenko <113481193+lekurile@users.noreply.github.com>",['c55fad9f0c5f870f12376edc054d6d6413eafec1'],False,"['gelu.py', 'matmul_ext.py', 'requirements-triton.txt', 'inference_test_utils.py', 'test_attention.py', 'test_layer_norm.py']"
1f0a44d934b0c0a5005ae035bafa10c34f1b062b,"Keep hpz secondary tensor in forward pass (#4288)

* keep hpz secondary tensor in forward

* overwrite sec tensor

* fix format",['e8ed7419ed40306100f0454bf85c6f4cc4d55f34'],False,"['partition_parameters.py', 'stage3.py']"
e194956571fbefc5f3c2533d6aee4fff1d11bafd,"Support iterators with incompletely defined __len__ functions (#2445)

* support iterators with incompletely defined __len__ functions

* whitespace fix

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Logan Adams <loadams@microsoft.com>",['1f0a44d934b0c0a5005ae035bafa10c34f1b062b'],False,['engine.py']
542dc0d5cb58d9aba78eceb4510296eddbaa2e1f,"AMD Kernel Compatibility Fixes (#3180)

* Guard against APIs not available on AMD in reduction_utils, code cleanup

* More API alignment simplification

* Int conversion fix

* Syntax

---------

Co-authored-by: Logan Adams <loadams@microsoft.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Ammar Ahmad Awan <ammar.awan@microsoft.com>",['e194956571fbefc5f3c2533d6aee4fff1d11bafd'],False,"['conversion_utils.h', 'ds_kernel_utils.h', 'reduction_utils.h']"
aa4a7401f8c364c64adfc775346f1defa1113ac1,"ZeRO-Inference refresh (#4197)

* INT4 weight only quantization (#479)

* INT4 weight only quantization

* pre commit

* fix UT

* fix UT

* fix UT

* fix UT

* fix UT

* fix UT

* fix UT

* add zero3 test

* quantize small weight first to prevent oom

* fold quantization config into ds_config

* Fix license & refactor ds_config & rebase master

* fix UT

* Moving quantization into post_init_method and add int4 dequantization kernel (#522)

* Add experimental int4 dequantize kernel

* move quantiation into post_init_method

* fix

* Refactor: move int4 code to deepspeed/inference (#528)

* Move int 4 code to deepspeed/inference

* fix

* fix

* fix

* zero++ tutorial PR (#3783)

* [Fix] _conv_flops_compute when padding is a str and stride=1 (#3169)

* fix conv_flops_compute when padding is a str when stride=1

* fix error

* change type of paddings to tuple

* fix padding calculation

* apply formatting check

---------

Co-authored-by: Cheng Li <pistasable@gmail.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>

* fix interpolate flops compute (#3782)

* use `Flops Profiler` to test `model.generate()` (#2515)

* Update profiler.py

* pre-commit run --all-files

* Delete .DS_Store

* Delete .DS_Store

* Delete .DS_Store

---------

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>
Co-authored-by: Cheng Li <pistasable@gmail.com>

* revert PR #3611 (#3786)

* bump to 0.9.6

* ZeRO++ chinese blog (#3793)

* zeropp chinese blog

* try better quality images

* make title larger

* even larger...

* various fix

* center captions

* more fixes

* fix format

* remove staging trigger (#3792)

* DeepSpeed-Triton for Inference (#3748)

Co-authored-by: Stephen Youn <styoun@microsoft.com>
Co-authored-by: Arash Bakhtiari <arash@bakhtiari.org>
Co-authored-by: Cheng Li <pistasable@gmail.com>
Co-authored-by: Ethan Doe <yidoe@microsoft.com>
Co-authored-by: yidoe <68296935+yidoe@users.noreply.github.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>

* ZeRO++ (#3784)

Co-authored-by: HeyangQin <heyangqin@microsoft.com>
Co-authored-by: GuanhuaWang <alexwgh333@gmail.com>
Co-authored-by: cmikeh2 <connorholmes@microsoft.com>
Co-authored-by: Ammar Ahmad Awan <ammar.awan@microsoft.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>
Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Reza Yazdani <reyazda@microsoft.com>

* adding zero++ to navigation panel of deepspeed.ai (#3796)

* Add ZeRO++ Japanese blog (#3797)

* zeropp chinese blog

* try better quality images

* make title larger

* even larger...

* various fix

* center captions

* more fixes

* fix format

* add ZeRO++ Japanese blog

* add links

---------

Co-authored-by: HeyangQin <heyangqin@microsoft.com>
Co-authored-by: Conglong Li <conglong.li@gmail.com>

* Bug Fixes for autotuner and flops profiler (#1880)

* fix autotuner when backward is not called

* fix format

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>

* Missing strided copy for gated MLP (#3788)

Co-authored-by: Ammar Ahmad Awan <ammar.awan@microsoft.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>

* Requires grad checking. (#3789)

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>

* bump to 0.10.0

* Fix Bug in transform.cu (#3534)

* Bug fix

* Fixed formatting error

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>

* bug fix: triton importing error (#3799)

Co-authored-by: Stephen Youn <styoun@microsoft.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>

* Fix dequant bug

* Address PR feedback

* Use super() __exit__

* Fix unit tests

---------

Co-authored-by: Donglin Zhuang <donglinzhuang@outlook.com>
Co-authored-by: Heyang Qin <heyangqin@microsoft.com>
Co-authored-by: Bill Luo <50068224+zhiruiluo@users.noreply.github.com>
Co-authored-by: Cheng Li <pistasable@gmail.com>
Co-authored-by: Guorun <84232793+CaffreyR@users.noreply.github.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>
Co-authored-by: stephen youn <13525892+stephen-youn@users.noreply.github.com>
Co-authored-by: Stephen Youn <styoun@microsoft.com>
Co-authored-by: Arash Bakhtiari <arash@bakhtiari.org>
Co-authored-by: Ethan Doe <yidoe@microsoft.com>
Co-authored-by: yidoe <68296935+yidoe@users.noreply.github.com>
Co-authored-by: GuanhuaWang <alexwgh333@gmail.com>
Co-authored-by: cmikeh2 <connorholmes@microsoft.com>
Co-authored-by: Ammar Ahmad Awan <ammar.awan@microsoft.com>
Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>
Co-authored-by: Reza Yazdani <reyazda@microsoft.com>
Co-authored-by: Masahiro Tanaka <81312776+tohtana@users.noreply.github.com>
Co-authored-by: Conglong Li <conglong.li@gmail.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Joe Mayer <114769929+jomayeri@users.noreply.github.com>
Co-authored-by: Ramya Ramineni <62723901+rraminen@users.noreply.github.com>",['542dc0d5cb58d9aba78eceb4510296eddbaa2e1f'],False,"['quantization.h', 'pt_binding.cpp', 'quantize_int4.cu', '__init__.py', 'config.py', '__init__.py', 'layers.py', 'quantization.py', 'quantization_context.py', 'utils.py', 'config.py', 'partition_parameters.py', 'quantizer.py', 'test_int4_quantization.py']"
e75c285ad78cf2c7f0e8df543f6cb097b3abb55e,"fix user args parsing of string with spaces on runner (#4265)

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['aa4a7401f8c364c64adfc775346f1defa1113ac1'],False,['runner.py']
8c1eed2e4750d1f8e963f5e85ff38f9053072e46,"Update release and bump patch versioning flow (#4286)

* Update release.sh and bump_patch_version.py flow

* Style fix

* newline formattingh",['ffd82bb048b9047e399425beea267b3ff411f9fd'],False,"['bump_patch_version.py', 'release.sh']"
9adc73ff652def660488dfc05e47bd760a711f22,Handle empty parameter groups (#4277),['48f2192aef5a2918ddd1b07793e1a66fc87964d5'],False,"['engine.py', 'stage3.py', 'test_zero.py']"
11aa880efe10546813f19f4228350cb9322235b8,"Fix Zero3 contiguous grads, reduce scatter false  accuracy issue (#4321)

it is a corner case met when running Zero3 on flan-t5 HF model.
Where HF auto bucket size becomes exactly the same size if hidden_size^2
This is the exact size of some of the params. due to the bug the params
are not being reduced during backward.

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['180dd39714a79b1d30bdecfe5c1b2ffe37b30152'],False,['stage3.py']
0c83436a752622f15cf5dce1044e0317d8ea77b0,DS-Chat BLOOM: Fix Attention mask (#4338),['e20e4a9d02311f4f71b610928916e1c76a22d5e1'],False,['ds_attention.py']
9bf77782b2597e4c1b73988b553899245516097d,"Fix a bug in the implementation of dequantization for inference (#3433)

* bugfix in launch_dequantize()

Get rid of `hid_cnt` and simply set #blocks to output size / #groups

* add a unit test for dequantization

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Reza Yazdani <44502768+RezaYazdaniAminabadi@users.noreply.github.com>",['0c83436a752622f15cf5dce1044e0317d8ea77b0'],False,"['dequantize.cu', 'pt_binding.cpp', 'test_dequantization.py']"
8c7f7fd2fde57d8fceafcf7a5f3b58c08806c50c,"Fix skipped inference tests (#4336)

* fix accidentally skipped tests

* skip broken tests for now",['3ea42c6e26e9fe060e96c8c3cdd0950d2343823c'],False,['test_inference.py']
3f3e9fb11e661cb999ea2c8d6ee5e348235e576d,"Fix autotune to support Triton 2.1  (#4340)

* need to change autotune and config prunning codes for triton 2.1

* formatting

---------

Co-authored-by: Stephen Youn <styoun@microsoft.com>
Co-authored-by: Lev Kurilenko <113481193+lekurile@users.noreply.github.com>",['8c7f7fd2fde57d8fceafcf7a5f3b58c08806c50c'],False,"['attention.py', 'triton_matmul_kernel.py']"
367d6f9cecb9c6a9150ea0db6fa6e93e4b78e7e0,"Support InternLM (#4137)

* correct inference with some debug codes.

* remove prints

* update transformer import set_qkv and format

* support some lora abstract method

* fix attn_ob

* some debug

* leave orig layer set by user

* remove debugs

* move attn ob to mlp module

* move import transformer

* init orig class only once

* remove copyright

---------

Co-authored-by: Lev Kurilenko <113481193+lekurile@users.noreply.github.com>",['b9d719a6d3877f50b453b70eb0d83241ad9532b3'],False,"['__init__.py', 'internlm.py', 'replace_policy.py', 'utils.py', 'ds_attention.py', 'mlp_gemm.py', 'qkv_gemm.py']"
f876d81d34884aa17f485208e3f823500f49abe9,"DeepSpeed4Science (#4357)

* zero++ tutorial PR (#3783)

* [Fix] _conv_flops_compute when padding is a str and stride=1 (#3169)

* fix conv_flops_compute when padding is a str when stride=1

* fix error

* change type of paddings to tuple

* fix padding calculation

* apply formatting check

---------

Co-authored-by: Cheng Li <pistasable@gmail.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>

* fix interpolate flops compute (#3782)

* use `Flops Profiler` to test `model.generate()` (#2515)

* Update profiler.py

* pre-commit run --all-files

* Delete .DS_Store

* Delete .DS_Store

* Delete .DS_Store

---------

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>
Co-authored-by: Cheng Li <pistasable@gmail.com>

* revert PR #3611 (#3786)

* bump to 0.9.6

* ZeRO++ chinese blog (#3793)

* zeropp chinese blog

* try better quality images

* make title larger

* even larger...

* various fix

* center captions

* more fixes

* fix format

* remove staging trigger (#3792)

* DeepSpeed-Triton for Inference (#3748)

Co-authored-by: Stephen Youn <styoun@microsoft.com>
Co-authored-by: Arash Bakhtiari <arash@bakhtiari.org>
Co-authored-by: Cheng Li <pistasable@gmail.com>
Co-authored-by: Ethan Doe <yidoe@microsoft.com>
Co-authored-by: yidoe <68296935+yidoe@users.noreply.github.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>

* ZeRO++ (#3784)

Co-authored-by: HeyangQin <heyangqin@microsoft.com>
Co-authored-by: GuanhuaWang <alexwgh333@gmail.com>
Co-authored-by: cmikeh2 <connorholmes@microsoft.com>
Co-authored-by: Ammar Ahmad Awan <ammar.awan@microsoft.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>
Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Reza Yazdani <reyazda@microsoft.com>

* adding zero++ to navigation panel of deepspeed.ai (#3796)

* Add ZeRO++ Japanese blog (#3797)

* zeropp chinese blog

* try better quality images

* make title larger

* even larger...

* various fix

* center captions

* more fixes

* fix format

* add ZeRO++ Japanese blog

* add links

---------

Co-authored-by: HeyangQin <heyangqin@microsoft.com>
Co-authored-by: Conglong Li <conglong.li@gmail.com>

* Bug Fixes for autotuner and flops profiler (#1880)

* fix autotuner when backward is not called

* fix format

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>

* Missing strided copy for gated MLP (#3788)

Co-authored-by: Ammar Ahmad Awan <ammar.awan@microsoft.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>

* Requires grad checking. (#3789)

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>

* bump to 0.10.0

* Fix Bug in transform.cu (#3534)

* Bug fix

* Fixed formatting error

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>

* bug fix: triton importing error (#3799)

Co-authored-by: Stephen Youn <styoun@microsoft.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>

* DeepSpeed4Science (#569)

* Integrating evoformer attention

* add cutlass version check

* Updaate error message

* add benchmark

* Update

* Update evoformer_attn.py

* Update run_evoformer_test.py

* Update evoformer_attn.py

* Update run_evoformer_test.py

* support more GPU archs

* add copyright

* add tests

* Fix bugs

* Update benchmark

* update

* Fix nvcc macro

* clean code

* fix formatting

* fix yaml import

* skip unit test when not compatible

* fix yaml requirement

* revert changes

* update tutorial

* update

* fix formatting

* fix format

* skip evoformer attn in pre-compile-ops

* revert changes

* update tutorial

* fix cutlass check

* update tutorial

* refactor tutorial

* revise

* Updated the Megatron-DS section (#565)

* Updated the Megatron-DS section

* minor fix

* minor fix

* minor fix

* separate evoformer tutorial

* Revised the ds4science landing page (#566)

* Updated the Megatron-DS section

* minor fix

* minor fix

* minor fix

* Revised the landing page

* Revised the landing page

* Removing unused file

* fix links image position

* modify main page

* fix doc

---------

Co-authored-by: Shiyang Chen <csycfl@gmail.com>
Co-authored-by: Minjia Zhang <33713995+minjiaz@users.noreply.github.com>

---------

Co-authored-by: Heyang Qin <heyangqin@microsoft.com>
Co-authored-by: Bill Luo <50068224+zhiruiluo@users.noreply.github.com>
Co-authored-by: Cheng Li <pistasable@gmail.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Guorun <84232793+CaffreyR@users.noreply.github.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>
Co-authored-by: stephen youn <13525892+stephen-youn@users.noreply.github.com>
Co-authored-by: Stephen Youn <styoun@microsoft.com>
Co-authored-by: Arash Bakhtiari <arash@bakhtiari.org>
Co-authored-by: Ethan Doe <yidoe@microsoft.com>
Co-authored-by: yidoe <68296935+yidoe@users.noreply.github.com>
Co-authored-by: GuanhuaWang <alexwgh333@gmail.com>
Co-authored-by: cmikeh2 <connorholmes@microsoft.com>
Co-authored-by: Ammar Ahmad Awan <ammar.awan@microsoft.com>
Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>
Co-authored-by: Reza Yazdani <reyazda@microsoft.com>
Co-authored-by: Masahiro Tanaka <81312776+tohtana@users.noreply.github.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Joe Mayer <114769929+jomayeri@users.noreply.github.com>
Co-authored-by: Ramya Ramineni <62723901+rraminen@users.noreply.github.com>
Co-authored-by: Shiyang Chen <csycfl@gmail.com>
Co-authored-by: Minjia Zhang <33713995+minjiaz@users.noreply.github.com>",['367d6f9cecb9c6a9150ea0db6fa6e93e4b78e7e0'],False,"['nv-pre-compile-ops.yml', 'README.md', 'attention.cpp', 'attention.cu', 'attention_back.cu', 'epilogue_grad_bias.h', 'epilogue_pipelined.h', 'epilogue_rescale_output.h', 'epilogue_thread_apply_logsumexp.h', 'custom_mma.h', 'custom_mma_base.h', 'custom_mma_multistage.h', 'custom_mma_pipelined.h', 'find_default_mma.h', 'mma_accum_lambda_iterator.h', 'mma_from_smem.h', 'gemm_kernel_utils.h', 'epilogue_predicated_tile_iterator.h', 'make_residual_last.h', 'predicated_tile_access_iterator_residual_last.h', 'predicated_tile_iterator_atomic.h', 'predicated_tile_iterator_residual_last.h', 'transpose_warp_iterator.h', 'warp_iterator_from_smem.h', 'kernel_backward.h', 'kernel_forward.h', 'bias_broadcast.h', 'tile_smem_loader.h', '__init__.py', 'evoformer_attn.py', '_config.yml', 'navigation.yml', 'deepspeed4science.md', 'ds4sci_evoformerattention.md', '3pillars.png', 'DeepSpeed-pillars.png', 'evoformer.png', 'new-megatron-ds.png', 'index.md', 'evoformer_attn.py', 'DS4Sci_EvoformerAttention_bench.py', 'test_DS4Sci_EvoformerAttention.py']"
78c3b148a8a8b6e60ab77a5c75849961f52b143d,fix deepspeed4science links (#4358),['f876d81d34884aa17f485208e3f823500f49abe9'],False,"['deepspeed4science.md', 'ds4sci_evoformerattention.md']"
468882fb68f59eb4b23fa96bf29a0c7666b4dcbd,"Add the policy to run llama model from the official repo (#4313)

* Add the llama2 support from the official llama repo

* add back commented function

* add new policy & implementation for llama2

* add some changes to inject/run the 70b llama model

* remove debugging code

* remove more debugging code

* formatting

* use num_kv only when it has positive value

* use the num_kv param only if  it is positive

* fix syntax and format errors.

* fix an issue with the float32 transform kernel

---------

Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>
Co-authored-by: Ammar Ahmad Awan <ammar.awan@microsoft.com>",['78c3b148a8a8b6e60ab77a5c75849961f52b143d'],False,"['pt_binding.cpp', 'transform.cu', 'inference_cuda_layers.h', 'ds_llama2.py', 'auto_tp.py', '__init__.py', 'base.py', 'llama2.py', 'load_checkpoint.py', 'replace_policy.py', 'utils.py', 'config.py', 'ds_attention.py', 'softmax_context.py']"
85334238da42ac153fc1da54bbcc95995bcc28b9,added check to avoid undefined behavior when the input_id length is greater than max_tokens (#4349),['468882fb68f59eb4b23fa96bf29a0c7666b4dcbd'],False,['engine.py']
dcd3ae195450014b3077b9ebddbd6c668f049a03,Enable workflow dispatch on Torch 1.10 CI tests (#4361),['dcf649c3e0e5167dcddb5521e525339a819030e1'],False,"['nv-torch110-p40.yml', 'nv-torch110-v100.yml', 'test_int4_quantization.py']"
a99e5d3fb7cd08e32b9b37d8913657c8638083e2,"deepspeed4science japanese blog (#4369)

* fix

* deepspeed4science japanese blog",['3592a22cfe35752a76ba486533555c1fca0b0105'],False,"['README.md', 'README.md', 'README.md', '2023-09-19-deepspeed4science-japanese.md', 'index.md']"
c58146471e7ba7df9791f27bdb6c3f2afa0bd3e5,"Openfold fix (#4368)

* update

* format

---------

Co-authored-by: Conglong Li <conglong.li@gmail.com>",['a99e5d3fb7cd08e32b9b37d8913657c8638083e2'],False,"['attention.cu', 'attention_back.cu', 'gemm_kernel_utils.h', 'bias_broadcast.h', 'evoformer_attn.py', 'DS4Sci_EvoformerAttention_bench.py']"
17957728c0362bf8ae70feca308e491e55ef9feb,Fix multinode runner to properly append to PDSH_SSH_ARGS_APPEND (#4373),['727609df4adf55b862f753261493229aa802966c'],False,['multinode_runner.py']
463dea27229ca60faeb782d6ec27b650a981ad0b,Fix min torch version (#4375),['17957728c0362bf8ae70feca308e491e55ef9feb'],False,['test_int4_quantization.py']
4fc2c8e7d583977b17a4ce09cb5f364cbdd229e3,"Fix llama meta tensor loading in AutoTP and kernel injected inference (#3608)

* Adapt to Llama when using meta tensor to load

* Fix gated mlp parameter mp

* Re-enable meta tensor for kernel injection
Fix layer params loading in meta tensor

* Revert mlp_inter_mp for gated mlp as it is fixed

* Monkey patch for fixing llama output

* Fix formatting

* Add comment

---------

Co-authored-by: Lev Kurilenko <lekurile@microsoft.com>
Co-authored-by: Lev Kurilenko <113481193+lekurile@users.noreply.github.com>",['463dea27229ca60faeb782d6ec27b650a981ad0b'],False,['llama.py']
0e0748c579cff3f94dc0609677623d082c9a2132,"adds triton flash attention2 kernel (#4337)

* initial commit

* temp commit: needs debugging

* packed flash attn with mask works

* clean-up

* add bert/roberta tests to test_inference

* is_triton_supported added to Accelerator class
clean-up and formatting

* triton supports the flash attention when compute cap > 8.0

* formatting

* fix comments

* cleanup

* cleanup flash kernel

* fix according to the PR comment

---------

Co-authored-by: Stephen Youn <styoun@microsoft.com>
Co-authored-by: Lev Kurilenko <113481193+lekurile@users.noreply.github.com>",['4fc2c8e7d583977b17a4ce09cb5f364cbdd229e3'],False,"['abstract_accelerator.py', 'cpu_accelerator.py', 'cuda_accelerator.py', 'mps_accelerator.py', 'npu_accelerator.py', 'attention.py', 'inference_test_utils.py', 'test_attention.py']"
60bf78454c976c59eb9a77ea9ef92ca863669886,"Fix incorrect assignment of self.quantized_nontrainable_weights (#4399)

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['f8d3ec7fa142ef590ce602ef9cb15fafb7fff1f6'],False,['partition_parameters.py']
388c84834fca87465aff8bb8f6d85be88fa82ba6,add CPU autotp UT (#4263),['28b9d5c2313addde94a1aa17c09eeb97cb792d7e'],False,"['cpu-inference.yml', 'test_he_all.py', 'test_he_llama.py', 'test_he_lora.py', 'test_inference.py']"
7220e7f8f7099725f5513c3d55add936e7c38306,"fix cpu loading model partition OOM (#4353)

* fix cpu loading model partition OOM

* clean up

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['388c84834fca87465aff8bb8f6d85be88fa82ba6'],False,['auto_tp.py']
1760627eb9ce1aa28692ac5a0787a4cb3c11fb91,"Zero infinity xpu support (#4130)

* zero infinity xpu support

* remove env var depends

* client align mem

* sync with all accelerators'

* format fix

* add align in pin_memory api

* add missing brackets

* remove align

* modify pin_memory api

* modify pin_memory api to use only on align para

* change value of align bytes

* Update csrc/aio/common/deepspeed_aio_common.cpp

* add version check and change format

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['2f73b834b59c69f6c2ca1aa1866b33e7135664fe'],False,"['abstract_accelerator.py', 'cpu_accelerator.py', 'cuda_accelerator.py', 'mps_accelerator.py', 'npu_accelerator.py', 'deepspeed_aio_common.cpp', 'deepspeed_aio_thread.cpp', 'async_swapper.py', 'optimizer_utils.py', 'partitioned_optimizer_swapper.py', 'partitioned_param_swapper.py', 'utils.py', 'async_io.py']"
9a5529145253353434a3f71b7e995bb251ce7372,"[CCLBackend] Using parallel memcpy for inference_all_reduce (#4404)

* use parallel version of memcpy

* include max buf size to 16MB per rank

* support any input buffer size

* fix format error",['1760627eb9ce1aa28692ac5a0787a4cb3c11fb91'],False,['ccl.cpp']
58a206059fd051b7348e11e6dc95204015cd4014,"Small docstring fix (#4431)

* small docstring fix

* default is no longer none in zero_grad

* typo

* max_norm not in method signature

* Revert ""default is no longer none in zero_grad""

This reverts commit cb126bae0820392f3afcd42220b0639a5fbdd4b3.

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['9a5529145253353434a3f71b7e995bb251ce7372'],False,"['unfused_optimizer.py', 'utils.py']"
2c67b58b5f07ed8e08775df8442f5b6a91c16053,"fix: check-license (#4432)

Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>",['58a206059fd051b7348e11e6dc95204015cd4014'],False,['check-license.py']
986b5958e2465b608692ff958472a9a016a3e45a,"fix: wrong documentation of `ignore_unused_parameters` (#4418)

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['cd0d2ba2df1b9f38c69b47730d73ca63a2c45320'],False,['config.py']
7099f9933314b239d166671a5a95887a1467608d,"Fix a bug in DeepSpeedMLP (#4389)

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['fa582c581a608f6b0de3b3210d6c937f6a94a139'],False,['ds_mlp.py']
93a6d7a547d29522a063f67cc205f067d78ef73c,fix blog format (#4456),['43a7f735943ba12203f09a7991492c9d740f536e'],False,['README-Japanese.md']
2c220d6593772cc00fcb04faa97a56bfd62e4076,"DeepSpeed-VisualChat Chinese blog (#4458)

* DS-VChat-blog-clean (#4445)

Co-authored-by: Heyang Qin <heyangqin@microsoft.com>
Co-authored-by: Zhewei Yao <zheweiy@berkeley.edu>
Co-authored-by: Minjia Zhang <33713995+minjiaz@users.noreply.github.com>
Co-authored-by: Xiaoxia (Shirley) Wu <94406484+xiaoxiawu-microsoft@users.noreply.github.com>

* add links to readme

* fix

* update hero-figure

* update title

* Create README_Chinese.md

* Update README_Chinese.md

* Update README_Chinese.md

* unify name format

* improve blog

* blog index

---------

Co-authored-by: Ammar Ahmad Awan <ammar.awan@microsoft.com>
Co-authored-by: Heyang Qin <heyangqin@microsoft.com>
Co-authored-by: Zhewei Yao <zheweiy@berkeley.edu>
Co-authored-by: Minjia Zhang <33713995+minjiaz@users.noreply.github.com>
Co-authored-by: Xiaoxia (Shirley) Wu <94406484+xiaoxiawu-microsoft@users.noreply.github.com>
Co-authored-by: yaozhewei <zheweiyao@gmail.com>
Co-authored-by: xiaoxiawu-microsoft <xiaoxiawu@microsoft.com>",['f63c35b4b31afab3efc0989f9bf4aa22d58d63d6'],False,"['README.md', 'README-Chinese.md', 'README.md', '2023-10-04-deepspeed-visualchat-chinese.md', '2023-10-04-deepspeed-visualchat-japanese.md', '2023-10-04-deepspeed-visualchat.md', 'index.md']"
4294ea172caad1449af3af5e8941825ced4b5404,"CI fix for torch 2.1 release (#4452)

* Fix for torch 2.1 release
Co-authored-by: Logan Adams <loadams@microsoft.com>",['2c220d6593772cc00fcb04faa97a56bfd62e4076'],False,"['formatting.yml', 'nv-ds-chat.yml', 'nv-lightning-v100.yml', 'nv-mii.yml', 'nv-torch-latest-v100.yml', 'nv-torch-nightly-v100.yml', 'nv-transformers-v100.yml', 'torch.py', 'async_io.py', 'test_mics_optimizer.py']"
d72edb3b0d1588eee94a6c4fbea07f31aa5fb898,"fix lm head overriden issue, move it from checkpoint in-loop loading to out loop (#4206)

Signed-off-by: Wang, Yi A <yi.a.wang@intel.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['4294ea172caad1449af3af5e8941825ced4b5404'],False,"['load_checkpoint.py', 'replace_module.py']"
8e64c3b5509c1251fbd0956e28a3079f8bec4aee,"feat: add Lion optimizer (#4331)

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['d72edb3b0d1588eee94a6c4fbea07f31aa5fb898'],False,"['fused_lion.cpp', 'cpu_lion.h', 'simd.h', 'cpu_lion.cpp', 'cpu_lion_impl.cpp', 'fused_lion_frontend.cpp', 'multi_tensor_apply.cuh', 'multi_tensor_lion.cu', '__init__.py', '__init__.py', 'cpu_lion.py', 'fused_lion.py', 'multi_tensor_apply.py', 'config.py', 'engine.py', 'utils.py', 'advanced-install.md', 'cpu_lion.py', 'fused_lion.py', 'test_cpu_lion.py', 'test_lion.py']"
7ed952eff1c71f7652fe4786a16c8d917f9c37d6,"Fix bug in bfloat16 optimizer related to checkpointing (#4434)

* fix: bf16 optimizer if condition

* fix: unexpected keyword argument 'load_serial'

* fix: add load_serial arg to bf16_optimizer

* style: fix indentation

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['c4d467953320b3f420d2acf3d97ae423ad1f9003'],False,"['bf16_optimizer.py', 'engine.py']"
6763e2de610b006c0fafc2c19a31f6508c920d1b,"add lm_head and embed_out tensor parallel (#3962)

* add lm_head and embed_out tensor parallel

* fix load lm_head.weight name issue

* replace all_reduce with inference_all_reduce

* refactor lm_head tensor parallel

---------

Co-authored-by: Chen, Zhenhuan <zhenhuan.chen@intel.com>",['6b634d0e7e7a7059f9fdf8434b7d92d3da2953ff'],False,"['auto_tp.py', 'layers.py', 'replace_module.py']"
923f3590ee908d39cfcd3c59d3fc0ae27bbd7e8c,fix bad build command (#4483),['6763e2de610b006c0fafc2c19a31f6508c920d1b'],False,['release.yml']
e9503fe40ee127e74c8b0ea962aac4f0ae957624,fix missing package,['923f3590ee908d39cfcd3c59d3fc0ae27bbd7e8c'],False,['release.yml']
604d701e35548e5407b017c088bdc3760832c9e0,"Introduce pydantic_v1 compatibility module for pydantic>=2.0.0 support (#4407)

* Introduce pydantic_v1 compatibility module for pydantic>=2.0.0 support",['e7acee4933a596cf17fed50376823a5625d85f31'],False,"['config.py', 'config.py', 'config.py', 'pydantic_v1.py', 'config_utils.py', 'config.py', 'offload_config.py', 'environment.yml', 'requirements-readthedocs.txt', 'requirements.txt', 'test_ds_config_model.py']"
6c86ff393fbb579a5ca40f104cd70a14d3cb1eda,"adding 8bit dequantization kernel for asym fine-grained block quantization in zero-inference (#4450)

* kernels added for asym fine-grained block quantization with 8bits

* formatting

* clean up the code

* rename quantize_int4.cu to quantize_intX.cu

* rename test_int4_quantization.py to test_intX_quantization.py

* ""rename test_int4_quantization.py to test_intX_quantization.py""

This reverts commit 2d341405b2ed6cf69e83fcabad1804513ea92122.

* rename

* fix after the pr comments

* increased coverage of QuantLinear test
(w/ and w/o the cuda kernels)

* formatting

---------

Co-authored-by: Stephen Youn <styoun@microsoft.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['427253b94bf425395d48f494270550bac9ac880a'],False,"['quantization.h', 'pt_binding.cpp', 'quantize_intX.cu', 'utils.py', 'quantizer.py', 'test_intX_quantization.py']"
a25a67a083c8b13fe06735c720f626de34f03409,Fix scale factor on flops profiler (#4500),['6c86ff393fbb579a5ca40f104cd70a14d3cb1eda'],False,['profiler.py']
78c518ed979b4cf1df1816bdbe52b0d21531bee4,"Update README.md (#4518)

Fix arxiv link",['5bbbf41fe501296452368bc3c41577c0e528623f'],False,['README.md']
12aedac6ce8b25c95e5808b9b0b9ba27f3111ee0,"add available memory check to accelerators (#4508)

* add available memory check to accelerator

* catch case where nvmlInit fails

* add pynvml to reqs

* fix for cpu systems

* Update accelerator/cuda_accelerator.py

Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>

* simplify

---------

Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>",['78c518ed979b4cf1df1816bdbe52b0d21531bee4'],False,"['abstract_accelerator.py', 'cpu_accelerator.py', 'cuda_accelerator.py', 'mps_accelerator.py', 'npu_accelerator.py', 'requirements.txt']"
a7358817f59c528ea567902ab8958d5ebb44f803,"fix error type in ccl.py (#4521)

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['3e70a8871537e32e15925e446e9adfebbc7fe679'],False,['ccl.py']
8a93ded874469d148b8459e5c892188821c49def,"Fixed deepspeed.comm.monitored_barrier call (#4496)

* Fixed deepspeed.comm.monitored_barrier call

* Formatting

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['a7358817f59c528ea567902ab8958d5ebb44f803'],False,['comm.py']
beed962c252cab1a5f32f0d272ecf6a03df9d26e,"[Bug fix] Add rope_theta for llama config (#4480)

* Add rope_theta for llama config

* Add rope_theta to bias_add_transform_0213

* Fix CI problems

* Add rope_theta to linear layer

---------

Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>
Co-authored-by: Lev Kurilenko <113481193+lekurile@users.noreply.github.com>",['8a93ded874469d148b8459e5c892188821c49def'],False,"['apply_rotary_pos_emb.cu', 'pt_binding.cpp', 'transform.cu', 'inference_cuda_layers.h', 'llama.py', 'config.py', 'linear.py', 'softmax_context.py']"
3e4a587135447f4ba0f64010870acc297d333198,"Added rocblas header (#4538)

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['beed962c252cab1a5f32f0d272ecf6a03df9d26e'],False,['gemm_test.h']
c7724c61815a7c9c7827f2a82e853af3c21fd5b7,"Switch from HIP_PLATFORM_HCC to HIP_PLATFORM_AMD (#4539)

* Switch from HIP_PLATFORM_HCC to HIP_PLATFORM_AMD

* Merge changes and fix from #4528",['a5b1cb1eb510233ba1f35ee04bb1c3c0404051b4'],False,"['conversion_utils.h', 'cublas_wrappers.h', 'ds_kernel_utils.h', 'feed_forward.h', 'gemm_test.h', 'general_kernels.h', 'quantizer.h', 'reduction_utils.h', 'strided_batch_gemm.h', 'fused_lamb_cuda_kernel.cu', 'fake_quantizer.cu', 'token_sort.cu', 'spatial_cuda_layers.h', 'cublas_wrappers.cu', 'ds_transformer_cuda.cpp', 'apply_rotary_pos_emb.cu', 'pt_binding.cpp', 'softmax.cu', 'transform.cu', 'inference_context.h', 'inference_cublas_wrappers.h']"
0f2338f7b8c5bc0c9f54b09c4248d129af7a347f,"Fix RTD builds (#4558)

* Update .readthedocs.yml

* Update requirements-readthedocs.txt",['e238351101bcd23a9a0a32dba0b071685d670d6c'],False,"['.readthedocs.yml', 'requirements-readthedocs.txt']"
8fdd9b35e1e40852bd8986debf386ae77ef0fa48,"Enable universal checkpoint for zero stage 1 (#4516)

* Enable uni_ckpt for z1

* Remove logging fix to seperate PR. Relocate conversion script to avoid logging circular import issue

* Formatting fix

* PR feedback

* Handle replicated params

* Detect bf16_optimizer

* Docs

* Fix docs",['869629c2108f31e25803cd9e66c7ec76db3e5923'],False,"['constants.py', 'ds_to_universal.py', 'engine.py', 'engine.py', 'module.py', 'stage3.py', 'stage_1_and_2.py', 'model-checkpointing.rst']"
f15cccfa0c747824ad50f6277a5974612e115748,"[AutoTP] Make AutoTP work when num_heads not divisible by number of workers (#4011)

* allow number of heads not divisible by number of ranks

* get num_heads from model config, more robust

* simplify logic where num_head itself is sharded

* name tweaks

* make code more robust where num_attention_heads may not be defined in model_config

* support num_key_value_heads < num_attention_heads which is used by llama2

* add test for 5 ranks

* change odd rank # to 3 to avoid test skip

* add get_shard_size function

* modify sharding mechanism according to latest auto TP

* fix accuracy issue

* fix format

* skip tests with fusedqkv

* remove skip of fusedqkv tests

* skip test fusedqkv with odd number of ranks

* support model with n_heads in model_config

* fix TestInjectionPolicy::test[fp32-t5]

* fix uneven_heads on some fusedqkv types (#12)

* odd support fusedqkv

* fix format and clear text

* better fix when activation size cannot be divided by number of heads

* move tp_shard.py under module_inject

* Add get_num_kv_heads in tp_shard.py

* Refine according to comments

* remove old comment

* fix bug in getting num_kv_heads

* support uneven sharding of lm_head tensor parallel

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Molly Smith <112220543+molly-smith@users.noreply.github.com>
Co-authored-by: mzl <mingzhi.liu@intel.com>
Co-authored-by: Michael Wyatt <mrwyattii@gmail.com>
Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>",['8fdd9b35e1e40852bd8986debf386ae77ef0fa48'],False,"['auto_tp.py', 'auto_tp_model_utils.py', 'fusedqkv_utils.py', 'layers.py', 'replace_module.py', 'tp_shard.py', 'test_inference.py']"
ec029e7625ae3e6fb58c87a0483ea2c618ac3d3d,"Fix the sequence-parallelism for the dense model architecture (#4530)

Co-authored-by: Masahiro Tanaka <mtanaka@microsoft.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Sam Ade Jacobs <samjacobs@microsoft.com>
Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>",['f15cccfa0c747824ad50f6277a5974612e115748'],False,"['config.py', 'constants.py', 'engine.py', 'stage3.py', 'stage_1_and_2.py']"
8f168c2f8d0459592b51e3f2f245729f459bd00d,"fix multiple definition while building evoformer (#4556)

Current builder for evoformer use the same name for `attention.cpp` and
`attention.cu`, leading to same intermediate filename `attention.o`:
```shell
march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -
isystem /home/zejianxie/.conda/envs/dll/include -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem 
/home/zejianxie/.conda/envs/dll/include build/temp.linux-x86_64-cpython-
310/csrc/deepspeed4science/evoformer_attn/attention.o build/temp.linux-x86_64-cpython-
310/csrc/deepspeed4science/evoformer_attn/attention.o build/temp.linux-x86_64-cpython-
310/csrc/deepspeed4science/evoformer_attn/attention_back.o
```
and
```
`attention_impl(at::Tensor&, at::Tensor&, at::Tensor&, at::Tensor&, at::Tensor&, at::Tensor&, at::Tensor&)':
      tmpxft_0012bef1_00000000-6_attention.compute_86.cudafe1.cpp:(.text+0x330): multiple definition of `attention_impl(at::Tensor&, at::Tensor&, at::Tensor&, at::Tensor&, at::Tensor&, at::Tensor&, at::Tensor&)'; build/temp.linux-x86_64-cpython-310/csrc/deepspeed4science/evoformer_attn/attention.o:tmpxft_0012bef1_00000000-6_attention.compute_86.cudafe1.cpp:(.text+0x330): first defined here
      /home/zejianxie/.conda/envs/dll/bin/../lib/gcc/x86_64-conda-linux-gnu/11.4.0/../../../../x86_64-conda-linux-gnu/bin/ld: build/temp.linux-x86_64-cpython-310/csrc/deepspeed4science/evoformer_attn/attention.o:(.bss+0x0): multiple definition of `torch::autograd::(anonymous namespace)::graph_task_id'; build/temp.linux-x86_64-cpython-310/csrc/deepspeed4science/evoformer_attn/attention.o:(.bss+0x0): first defined here
```

I use following to reproduce and confirm my fix works:
```
git clone https://github.com/NVIDIA/cutlass --depth 1
CUTLASS_PATH=$PWD/cutlass DS_BUILD_EVOFORMER_ATTN=1 pip install ./DeepSpeed --global-option=""build_ext""
```

![image](https://github.com/microsoft/DeepSpeed/assets/41792945/9e406b37-330c-431c-8bf9-6be378dee4ff)

Co-authored-by: Conglong Li <conglong.li@gmail.com>",['764f5b073c26de8c9fd1d888371e6708d92bca2c'],False,"['attention_cu.cu', 'evoformer_attn.py']"
244040c18f463d498c7bd2d7364b1e0d7e544080,"Don't check overflow for bf16 data type (#4512)

Always check for fp16
bf16 dynamic range is similar to fp32. don't check overflow by default.

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['8f168c2f8d0459592b51e3f2f245729f459bd00d'],False,"['stage3.py', 'stage_1_and_2.py']"
4199dc25af07ce588c274aff76ed61fb455d003d,[docs] fix deepspeed.ai links,['45b07bf9444c07bccab54e74011b54dae3d6d2e7'],False,['index.md']
38b41dffa1d593ad8a8ab3ee2795dc203800f08c,"DeepSpeed-FastGen (#4604)

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>
Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>
Co-authored-by: Ammar Ahmad Awan <ammar.awan@microsoft.com>
Co-authored-by: Masahiro Tanaka <mtanaka@microsoft.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['737ef296cd745d72cd613473a56869c9f4a0ed41'],False,"['nv-a6000.yml', 'nv-pre-compile-ops.yml', '.gitmodules', '.pre-commit-config.yaml', 'MANIFEST.in', 'cuda_accelerator.py', 'activation_type.h', 'ds_kernel_utils.h', 'reduction_utils.h', 'gelu.cu', '__init__.py', '__init__.py', 'allocator.py', '__init__.py', 'base_engine.py', 'huggingface_engine.py', 'in_memory_engine.py', 'config_v2.py', 'engine_factory.py', 'engine_v2.py', 'inference_utils.py', '__init__.py', '__init__.py', '__init__.py', 'bias_activation.cpp', 'bias_activation.cu', 'bias_activation.h', 'bias_activation.py', '__init__.py', 'blas.h', 'blas_linear.py', 'blas_utils.h', 'core_ops.cpp', '__init__.py', 'cuda_fp_ln_base.py', 'cuda_ln.py', 'cuda_post_ln.py', 'cuda_pre_ln.py', 'layer_norm.cpp', 'layer_norm.cu', 'layer_norm.h', '__init__.py', 'rms_norm.cpp', 'rms_norm.cu', 'rms_norm.h', 'rms_norm.py', 'rms_norm_base.py', 'rms_pre_norm.py', '__init__.py', 'gated_activation.py', 'gated_activation_kernels.cpp', 'gated_activation_kernels.cu', 'gated_activation_kernels.h', 'LICENSE', '__init__.py', 'cutlass_ops.cpp', '__init__.py', 'mixed_gemm.cu', 'mixed_gemm.h', 'mixed_gemm.py', 'mixed_gemm_api.h', '__init__.py', 'mixed_moe_gemm.py', 'moe_gemm.cu', 'moe_gemm.h', 'moe_gemm.py', 'moe_gemm_api.h', 'weight_variant.h', 'ds_kernel.py', '__init__.py', '__init__.py', 'atom_builder.cpp', 'atom_builder.h', 'atom_builder.py', '__init__.py', 'attention_atom.h', 'blocked_flash.cpp', 'blocked_flash.h', 'blocked_flash.py', 'flash.h', '__init__.py', 'embed.cpp', 'embed.cu', 'embed.cuh', 'embed.h', 'embed.py', '__init__.py', 'blocked_kv_rotary.cpp', 'blocked_kv_rotary.cu', 'blocked_kv_rotary.cuh', 'blocked_kv_rotary.h', 'blocked_kv_rotary.py', 'blocked_trained_kv_rotary.py', 'linear_blocked_kv_copy.py', '__init__.py', 'logits_gather.cpp', 'logits_gather.cu', 'logits_gather.cuh', 'logits_gather.h', 'logits_gather.py', '__init__.py', 'moe_gather.cpp', 'moe_gather.cu', 'moe_gather.cuh', 'moe_gather.h', 'moe_gather.py', '__init__.py', 'moe_scatter.cpp', 'moe_scatter.cu', 'moe_scatter.cuh', 'moe_scatter.h', 'moe_scatter.py', 'ragged_dtypes.h', 'ragged_kernel_helpers.cpp', 'ragged_kernel_helpers.h', 'ragged_ops.cpp', '__init__.py', 'top_1_gating.cpp', 'top_1_gating.cu', 'top_1_gating.cuh', 'top_1_gating.h', 'top_1_gating.py', 'logging.py', 'AddingAModel.md', '__init__.py', '__init__.py', 'attn_output_parameters.py', 'embedding_parameters.py', 'invfreq_parameters.py', 'mlp_parameters.py', 'moe_parameters.py', 'norm_parameters.py', 'qkv_parameters.py', 'unembed_parameters.py', 'inference_model_base.py', 'inference_policy_base.py', 'inference_transformer_base.py', 'layer_container_base.py', '__init__.py', 'llama_v2_containers.py', 'llama_v2_model.py', 'llama_v2_policy.py', '__init__.py', 'container.py', 'model.py', 'policy.py', '__init__.py', 'container.py', 'model.py', 'policy.py', 'parameter_base.py', '__init__.py', 'attn.py', 'attn_out.py', 'embedding.py', 'mlp.py', 'qkv.py', 'types.py', 'unembed.py', 'utils.py', '__init__.py', '__init__.py', 'attention_configs.py', 'embedding_config.py', 'linear_config.py', 'moe_config.py', 'norm_config.py', 'unembed_config.py', 'ds_module.py', 'heuristics.py', '__init__.py', '__init__.py', 'dense_blocked_attention.py', '__init__.py', 'ragged_embedding.py', '__init__.py', 'blas_fp_linear.py', 'cutlass_fp_linear.py', '__init__.py', 'cutlass_multi_gemm.py', 'gate_fn.py', 'test.py', '__init__.py', 'cuda_post_ln.py', '__init__.py', 'cuda_pre_ln.py', 'cuda_pre_rms.py', '__init__.py', 'ragged_unembed.py', '__init__.py', 'attention_base.py', 'embedding_base.py', 'linear_base.py', 'moe_base.py', 'post_norm_base.py', 'pre_norm_base.py', 'unembed_base.py', 'module_registry.py', '__init__.py', 'blocked_allocator.py', 'fast_host_buffer.cu', 'ragged_ops.cpp', 'fast_host_buffer.h', 'kv_cache.py', 'manager_configs.py', 'ragged_manager.py', 'ragged_wrapper.py', 'sequence_descriptor.py', 'scheduling_utils.py', 'builder.py', 'inference_core_ops.py', 'inference_cutlass_builder.py', 'ragged_ops.py', 'ragged_utils.py', 'requirements-dev.txt', 'check-torchcuda.py', 'pytest.ini', '__init__.py', 'inference_test_utils.py', '__init__.py', '__init__.py', 'test_bias_activation.py', 'test_blas_linear.py', 'test_gated_activation.py', 'test_post_ln.py', 'test_pre_ln.py', 'test_rms_norm.py', '__init__.py', 'test_moe_gemm.py', '__init__.py', 'ragged_testing_utils.py', 'test_atom_builder.py', 'test_blocked_flash.py', 'test_blocked_kv_copy.py', 'test_blocked_rotary_emb.py', 'test_logits_gather.py', 'test_moe_gather.py', 'test_moe_scatter.py', 'test_ragged_embed.py', 'test_top_1_gating.py', '__init__.py', '__init__.py', 'test_layer_inheritance.py', 'test_mapping.py', 'test_multi_parameter_layer.py', 'test_parameter_list.py', 'utils.py', '__init__.py', 'test_attn_out_sharding.py', 'test_mlp_sharding.py', 'test_qkv_sharding.py', '__init__.py', 'test_blas_linear_module.py', 'test_blocked_attn.py', 'test_cuda_pre_ln_module.py', 'test_custom_module.py', 'test_cutlass_moe.py', 'test_post_ln_module.py', 'test_pre_rms_module.py', '__init__.py', 'test_blocked_allocator.py', 'test_manager_configs.py', 'test_ragged_wrapper.py']"
d89027be61c0eaf1567332564e78a412d7c3c8eb,"Fix figure in FlexGen blog (#4624)

Fix the latency-throughput figure for 13B.",['b51d15b37524106a58b2a8d93326e4f6afe81ccf'],False,"['README.md', 'throughput_latency_13B.png', 'throughput_latency_13B_no_arrow.png']"
de96e18ad5c6c58b21b906249296ca45ff46a24f,"Fix figure of llama2 13B in DS-FlexGen blog (#4625)

Fix figure of llama2 13B in DS-FlexGen blog",['d89027be61c0eaf1567332564e78a412d7c3c8eb'],False,['throughput_latency_13B_no_arrow.png']
a591992b80c8dc52ce25bdf0278ec1f8ae5ff671,"Fix config format (#4594)

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['de96e18ad5c6c58b21b906249296ca45ff46a24f'],False,['config.py']
b1cb0dfc4623fc44e9931d1481c246bd8a59b778,"Guanhua/partial offload rebase v2 (#590) (#4636)

This PR introduces Twin-Flow feature of ZeRO-Offload++, which improves
e2e training iteration time by up to 6x on DGX-H100s.

 This PR includes:

* Twin-Flow implementation inside ZeRO optimizer
* json config tutorial
* example using deepspeed
* unit tests


cc @jeffra @awan-10 @tjruwase @mrwyattii

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['a591992b80c8dc52ce25bdf0278ec1f8ae5ff671'],False,"['engine.py', 'config.py', 'offload_config.py', 'stage3.py', 'config-json.md', 'partial_offload_test.py', 'test_hybrid_adam.py', 'test_zero_offloadpp.py']"
558006666e2823d5bde254ee33076413eb39b781,"Fix issues with torch cpu builds (#4639)

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['00df0c1998ab84aaa7ceaae2b9e04857923c7fa2'],False,['builder.py']
f6fce50debcb6f6b24d163a6ea48f5bd3341f368,"Isolate src code and testing for DeepSpeed-FastGen (#4610)

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>
Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>
Co-authored-by: Ammar Ahmad Awan <ammar.awan@microsoft.com>
Co-authored-by: Masahiro Tanaka <mtanaka@microsoft.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['558006666e2823d5bde254ee33076413eb39b781'],False,"['nv-a6000.yml', 'nv-accelerate-v100.yml', 'nv-inference.yml', 'nv-lightning-v100.yml', 'nv-megatron.yml', 'nv-pre-compile-ops.yml', 'nv-torch-latest-cpu.yml', 'nv-torch-latest-v100.yml', 'nv-transformers-v100.yml', 'activation_type.h', 'conversion_utils.h', 'ds_kernel_utils.h', 'memory_access_utils.h', 'reduction_utils.h', 'inference_core_ops.py', 'inference_cutlass_builder.py', 'ragged_ops.py', 'ragged_utils.py', '__init__.py', 'inference_test_utils.py', '__init__.py', '__init__.py', 'test_bias_activation.py', 'test_blas_linear.py', 'test_gated_activation.py', 'test_post_ln.py', 'test_pre_ln.py', 'test_rms_norm.py', '__init__.py', 'test_moe_gemm.py', '__init__.py', 'ragged_testing_utils.py', 'test_atom_builder.py', 'test_blocked_flash.py', 'test_blocked_kv_copy.py', 'test_blocked_rotary_emb.py', 'test_logits_gather.py', 'test_moe_gather.py', 'test_moe_scatter.py', 'test_ragged_embed.py', 'test_top_1_gating.py', '__init__.py', '__init__.py', 'test_layer_inheritance.py', 'test_mapping.py', 'test_multi_parameter_layer.py', 'test_parameter_list.py', 'utils.py', '__init__.py', 'test_attn_out_sharding.py', 'test_mlp_sharding.py', 'test_qkv_sharding.py', '__init__.py', 'test_blas_linear_module.py', 'test_blocked_attn.py', 'test_cuda_pre_ln_module.py', 'test_custom_module.py', 'test_cutlass_moe.py', 'test_post_ln_module.py', 'test_pre_rms_module.py', '__init__.py', 'test_blocked_allocator.py', 'test_manager_configs.py', 'test_ragged_wrapper.py']"
ab6b1e16bb5df2b6af750975e4e0ae42b7e04680,"Add Japanese blog for DeepSpeed-FastGen (#4651)

This blog adds Japanese blog for DeepSpeed-FastGen.
(also includes small fix of typos in the original blog)

---------

Co-authored-by: Conglong Li <conglong.li@gmail.com>",['f6fce50debcb6f6b24d163a6ea48f5bd3341f368'],False,"['README.md', 'README.md', 'README.md', 'README.md', '2023-11-06-deepspeed-fastgen-japanese.md', 'index.md']"
136fe337fdd22defc29608f23c5b21a9c2b37297,"Fix for MII unit tests (#4652)

https://github.com/microsoft/DeepSpeed-MII/pull/262 Moved the legacy-MII
unit tests to a subdirectory. Reflecting that change here.

@loadams",['ab6b1e16bb5df2b6af750975e4e0ae42b7e04680'],False,['nv-mii.yml']
01af3e1ddfe954c3c84f2b15ed967ecc5e8e78c1,"Enhance the robustness of `module_state_dict` (#4587)

If user re-implements the `state_dict()` method of `self.module` and
manually excludes the frozen parameters, this line will lead to
`KeyError`.",['136fe337fdd22defc29608f23c5b21a9c2b37297'],False,"['engine.py', 'test_zero_optimizer.py', 'simple_model.py']"
b8e16642321bc36ad6990844ded6a61f9e490fd8,"Enable ZeRO3 allgather for multiple dtypes (#4647)

This PR addresses an error reported in #4295.
When parameters in multiple data types are given, DeepSpeed performs
allgather for each data type.

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['01af3e1ddfe954c3c84f2b15ed967ecc5e8e78c1'],False,"['partition_parameters.py', 'stage3.py']"
00757a194fc6c1a020f8d3e121ade55efcc4ea0d,"Fix rope_theta arg for diffusers_attention (#4656)

This PR updates `diffusers_attention` to properly pass the `rope_theta`
arg to the `linear_func` calls. This was added in GH-4480 and needed to
be updated for the diffusers attention module as well.

Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>",['0daf7f2aeb0ff599e7862dbae43ac54c033e7743'],False,['diffusers_attention.py']
1d1a20c5a1dd980438393500f14d8dc188f6258a,"Fix the openfold training. (#4657)

This PR removes the bias created as placeholders, which causes a crash
in openfold's training pipeline.

---------

Co-authored-by: Conglong Li <conglong.li@gmail.com>",['3b1cf1fdc44017e929ce7c12b548c3686e52a528'],False,"['custom_mma_multistage.h', 'evoformer_attn.py', 'DS4Sci_EvoformerAttention_bench.py', 'test_DS4Sci_EvoformerAttention.py']"
8ad187d84f5d1ab350c18a40ba18f77cc9b26611,"Universal ckp fixes (#4588)

Signed-off-by: Moshe Island <misland@habana.ai>
Co-authored-by: Moshe Island <misland@habana.ai>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['1d1a20c5a1dd980438393500f14d8dc188f6258a'],False,"['constants.py', 'ds_to_universal.py', 'universal_checkpoint.py', 'checkpointing.py', 'bf16_optimizer.py', 'engine.py', 'fused_optimizer.py']"
4388a605f854db91302c4f89053ee861eb31bacd,"Update lr_schedules.py (#4563)

add cosine annealing scheduler

this scheduler is widely used in image classification task, and many llm
(e.g. llama) use this also.

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['da652d0e0b4ff0b7c870e74730b9d3d8f51bf8aa'],False,"['lr_schedules.py', 'test_lr_schedulers.py']"
a361bac74e72733d489c24de42a2bf651c24ee5a,"Fix UNET and VAE implementations for new diffusers version (#4663)

This PR updates the DepSpeed UNET and VAE implementations to support
`diffusers>=0.23.0`.",['4388a605f854db91302c4f89053ee861eb31bacd'],False,"['unet.py', 'vae.py']"
6ea44d02c674393c524ada811ea376c55438a913,"fix num_kv_heads sharding in autoTP for the new in-repo Falcon-40B (#4654)

to be compatible with the latest Falcon-40B's `num_kv_heads` in
https://huggingface.co/tiiuae/falcon-40b/commit/4a70170c215b36a3cce4b4253f6d0612bb7d4146

![image](https://github.com/microsoft/DeepSpeed/assets/5948851/d20aa6f2-b9af-4104-b9d3-8ba1ab588a6e)

error message like:

![image](https://github.com/microsoft/DeepSpeed/assets/5948851/06ef6dd2-25d5-4b51-8789-36e1b3f94a32)

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Lev Kurilenko <113481193+lekurile@users.noreply.github.com>",['a361bac74e72733d489c24de42a2bf651c24ee5a'],False,['auto_tp.py']
0a6095faa00e2bc0a121711947fc7cde9eda8be0,[MII] catch error wrt HF version and Mistral (#4634),['0abf4dfd48164c4ef08af22d84ec00ce574e4740'],False,['engine_factory.py']
4b7cae7bea839b1d19f55193ecb311381163791a,"[NPU] Add NPU support for unit test (#4569)

Unit tests would fail or skip when device=npu, and we definitely want to
test all these wonderful features by official unit tests.
Here comes the commit to add NPU support for unit test. P.S. see what we
have already done #4567.


**What I do in this commit**
1. Just add npu logic branch 
feat: Add npu support for skip_on_arch in tests/unit/util.py
feat: Add npu support for skip_on_cuda in tests/unit/util.py
feat: Add npu support for tests/unit/common.py

2. Set_device of accelerator before deepspeed.init_distributed in
tests/unit/common.py
It would be friendlier and easier for other device like npu, if we can
set_device of accelerator before init_distributed. Plus, setting device
param before init sounds more reasonable.

3. Solve the problem of calling get_accelerator().random().fork_rng with
non-cuda device
Function `train_cifar()` in `tests/unit/alexnet_model.py` calls
`get_accelerator().random().fork_rng` without passing `device_type`
explicitly. Unfortunately, `torch.random.fork_rng()` has default value
setting `device_type=cuda` and non-cuda devices would fail to run. So my
solution is explicitly passing
`device_type=get_accelerator().device_name()`, and either cuda or
non-cuda devices would perform correctly.

---------

Co-authored-by: ryan <ruanzhixiang1@huawei.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['0a6095faa00e2bc0a121711947fc7cde9eda8be0'],False,"['__init__.py', 'real_accelerator.py', 'alexnet_model.py', 'common.py', 'util.py']"
efd4556345c82d15f1aa9bf277240f7ac415b8b4,"Add stable diffusion unit test (#2496)

Add a unit test for Stable Diffusion without using stable-diffusion
model that needs HF token.

Midjourney model does not need the HF token and has the same structure.

We verified this by printing both models. The following is the
structure:

``` StableDiffusionPipeline {
  ""_class_name"": ""StableDiffusionPipeline"",
  ""_diffusers_version"": ""0.7.2"",
  ""feature_extractor"": [
    ""transformers"",
    ""CLIPFeatureExtractor""
  ],
  ""safety_checker"": [
    ""stable_diffusion"",
    ""StableDiffusionSafetyChecker""
  ],
  ""scheduler"": [
    ""diffusers"",
    ""PNDMScheduler""
  ],
  ""text_encoder"": [
    ""transformers"",
    ""CLIPTextModel""
  ],
  ""tokenizer"": [
    ""transformers"",
    ""CLIPTokenizer""
  ],
  ""unet"": [
    ""diffusers"",
    ""UNet2DConditionModel""
  ],
  ""vae"": [
    ""diffusers"",
    ""AutoencoderKL""
  ]
}
```
@cmikeh2

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Logan Adams <loadams@microsoft.com>
Co-authored-by: Lev Kurilenko <lekurile@microsoft.com>
Co-authored-by: Lev Kurilenko <113481193+lekurile@users.noreply.github.com>",['953e3e31f88a0c228976826abe3b1f90bb756866'],False,"['nv-sd.yml', 'requirements-sd.txt', 'pytest.ini', 'test_stable_diffusion.py']"
54110305299e2716bb550f13830401fa8c8ab31d,"Inference Checkpoints in V2 (#4664)

Add capability to snapshot an engine and resume from it, reducing load
times for large models. Includes new unit tests to validate this
pipeline on a small scale.

---------

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>
Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>
Co-authored-by: Ammar Ahmad Awan <ammar.awan@microsoft.com>
Co-authored-by: Masahiro Tanaka <mtanaka@microsoft.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Reza Yazdani <44502768+RezaYazdaniAminabadi@users.noreply.github.com>
Co-authored-by: Reza Yazdani <reyazda@microsoft.com>",['c1ba6a104f56327d6cf0fb605e32cbdc53e5c174'],False,"['nv-accelerate-v100.yml', 'nv-inference.yml', 'nv-lightning-v100.yml', 'nv-megatron.yml', 'nv-pre-compile-ops.yml', 'nv-torch-latest-cpu.yml', 'nv-torch-latest-v100.yml', 'nv-transformers-v100.yml', '__init__.py', '__init__.py', 'allocator.py', 'huggingface_engine.py', 'engine_factory.py', 'engine_v2.py', 'inference_parameter.py', '__init__.py', '__init__.py', 'embedding_parameters.py', 'invfreq_parameters.py', 'moe_parameters.py', 'qkv_parameters.py', 'flat_model_helpers.py', 'inference_model_base.py', 'inference_policy_base.py', 'inference_transformer_base.py', 'layer_container_base.py', '__init__.py', 'llama_v2_containers.py', 'llama_v2_policy.py', '__init__.py', 'policy.py', '__init__.py', 'policy.py', 'parameter_base.py', '__init__.py', 'blas_fp_linear.py', 'cutlass_fp_linear.py', 'cutlass_multi_gemm.py', 'gate_fn.py', 'test.py', 'cuda_post_ln.py', 'cuda_pre_ln.py', 'cuda_pre_rms.py', 'embedding_base.py', 'linear_base.py', 'moe_base.py', 'post_norm_base.py', 'pre_norm_base.py', 'ragged_ops.cpp', '__init__.py', 'test_bias_activation.py', 'test_blas_linear.py', 'test_gated_activation.py', 'test_post_ln.py', 'test_pre_ln.py', 'test_rms_norm.py', '__init__.py', 'test_moe_gemm.py', '__init__.py', 'ragged_testing_utils.py', 'test_atom_builder.py', 'test_blocked_flash.py', 'test_blocked_kv_copy.py', 'test_blocked_rotary_emb.py', 'test_logits_gather.py', 'test_moe_gather.py', 'test_moe_scatter.py', 'test_ragged_embed.py', 'test_top_1_gating.py', '__init__.py', '__init__.py', 'test_layer_inheritance.py', 'test_mapping.py', 'test_multi_parameter_layer.py', 'test_parameter_list.py', 'utils.py', '__init__.py', 'test_attn_out_sharding.py', 'test_mlp_sharding.py', 'test_qkv_sharding.py', '__init__.py', 'test_blas_linear_module.py', 'test_blocked_attn.py', 'test_cuda_pre_ln_module.py', 'test_custom_module.py', 'test_cutlass_moe.py', 'test_post_ln_module.py', 'test_pre_rms_module.py', '__init__.py', 'test_blocked_allocator.py', 'test_manager_configs.py', 'test_ragged_wrapper.py', 'test_contiguify.py', 'test_layer_inheritance.py', 'test_mapping.py', 'test_multi_parameter_layer.py', 'test_parameter_list.py', 'utils.py']"
901d80701258f59fc951c273f4c488ca04b10867,"KV Cache Improved Flexibility (#4668)

This KV-cache adds the foundation for appropriately supporting two key
KV-cache improvements:

1. Delineation between local/dense KV caches/models at the cache level
in addition to the attention module level.
2. Support for multiple types of disjoint KV caches (such as alternating
local + dense attention GPT-Neo).

Follow up item: Determine appropriate statistics for weighting local +
dense KV block ratios when both are present.

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['54110305299e2716bb550f13830401fa8c8ab31d'],False,"['engine_v2.py', '__init__.py', 'inference_model_base.py', 'inference_transformer_base.py', 'kv_cache.py', 'manager_configs.py', 'ragged_manager.py', 'sequence_descriptor.py', 'inference_test_utils.py', 'ragged_testing_utils.py']"
00e7dc5e5116a5384647db917c3a1fa723bf4a5b,"Fix for when prompt contains an odd num of apostrophes (#4660)

The apostrophes in args.user_args are mixed with the single quotes used
to enclose x in the parsing code.

For example,
`deepspeed --num_nodes 1 --num_gpus 8 --no_local_rank run_generation.py
--model_name_or_path tiiuae/falcon-40b --max_new_tokens 128 --prompt
""I'm a student""`

instead of (which doesn't work):
`deepspeed --num_nodes 1 --num_gpus 8 --no_local_rank run_generation.py
--model_name_or_path tiiuae/falcon-40b --max_new_tokens 128 --prompt
'I'm a student'`

To resolve this issue, we can make a change to enclose x in double
quotes instead of single quotes.

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['25e9cd54fcf05256a8e5334849aa9adee5feef97'],False,['runner.py']
5acef9e18f9463405a9816307b43581524b42c65,fixes issues introduced in #4668,['ce5e56a82eef66888456e75c45b5ed1214cfc54e'],False,"['ragged_manager.py', 'sequence_descriptor.py']"
ce0ebdade285dc208777af74c11117bbae34fa96,"[Bug fix] WarmupCosineLR issues (#4688)

Original code missing a `self.` before `warmup_num_steps` so that
`warmup_num_steps` might be 0 and cause math domain error when doing
`math.log(0)`

```py
        self.warmup_num_steps = max(2, warmup_num_steps)
        self.inverse_log_warm_up = 1.0 / math.log(warmup_num_steps)
```",['bcdabf44aeeb57d3fea75ecb91ca446c9372fbb9'],False,['lr_schedules.py']
a3926bbbf6d0025b5c6076a280e6b91ebd08aada,"infV2 fix for OPT size variants (#4694)

Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['ce0ebdade285dc208777af74c11117bbae34fa96'],False,"['nv-a6000.yml', 'engine_factory.py', 'layer_container_base.py', 'container.py', 'policy.py']"
0ec2d3e4bfa2d0a5237e9747da1ef9d5e4a4453b,"Add get and set APIs for the ZeRO-3 partitioned parameters (#4681)

The DeepSpeed currently supports a set of debugging APIs to
[get](https://deepspeed.readthedocs.io/en/latest/zero3.html#debugging)
and
[set](https://deepspeed.readthedocs.io/en/latest/zero3.html#modifying-partitioned-states)
the **full** model states (parameters, gradients, and optimizer states).
However, in some scenarios, only **local states** are needed, for
example, when pruning some model layers based on a local criterion.
After calling `model_engine.step()`, we need to apply the local mask to
the partitioned parameters owned by each process. Therefore, I am
submitting this PR to introduce some new APIs for `get` and `set` ZeRO-3
partial model states.

### APIs intro
```python
def safe_get_local_fp32_param(param):
    """"""Get the fp32 partitioned parameter.""""""

def safe_get_local_grad(param):
    """"""Get the fp32 gradient of a partitioned parameter.""""""

def safe_get_local_optimizer_state(param, optim_state_key):
    """"""Get the fp32 optimizer state of a partitioned parameter.""""""

def safe_set_local_fp32_param(param, value):
    """"""Update the partitioned fp32 parameter.""""""

def safe_set_local_optimizer_state(param, value, optim_state_key):
    """"""Update the fp32 optimizer state of a partitioned parameter.""""""
```

### Usage
```python
# local API
from deepspeed.utils import (
    safe_get_local_fp32_param,
    safe_get_local_grad,
    safe_get_local_optimizer_state,
    safe_set_local_fp32_param,
    safe_set_local_optimizer_state
    )
```
### TODO
- [x] Add local APIs
- [x] Add UTs
- [x] Update Docs

@tjruwase

---------

Signed-off-by: yliu <test@do_not_reply@neuralstudio.intel.com>
Co-authored-by: yliu <test@do_not_reply@neuralstudio.intel.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['a3926bbbf6d0025b5c6076a280e6b91ebd08aada'],False,"['stage3.py', '__init__.py', 'tensor_fragment.py', 'zero3.rst', 'test_zero_tensor_fragment.py']"
a603a2130f63207c00b626c062b868ee90145994,"Remove unneeded dict reinit (fix for #4565) (#4702)

As discussed in #4565 with @tjruwase 

Fix #4565 
Fix #4696",['0ec2d3e4bfa2d0a5237e9747da1ef9d5e4a4453b'],False,['stage3.py']
61391229c956356b71fe675a45d10d9b45111a31,"Update flops profiler to recurse (#4374)

Fixes #4334.",['a603a2130f63207c00b626c062b868ee90145994'],False,['profiler.py']
fd0a52c1acc8851a5ace7c3c757475f2e665de0e,"use all_gather_into_tensor instead of all_gather (#4705)

when using allgather, the output is a list, and in the implementation of
torch, the list will be flattened and unflattened, which will result in
additional allocation of GPU memory and D2D operations. But these all
gather operations already have a flat GPU memory, using
all_gather_into_tensor replaces all_gather will save GPU memory
allocation and additional D2D operations.
additionally, batching all gatherers does not reduce the peak usage of
GPU memory, so allgather_bucket_size has no effect.

Signed-off-by: --local <zhiwei.tao@enflame-tech.com>
Co-authored-by: --local <zhiwei.tao@enflame-tech.com>",['6b8103b46e165d44f01f8e119589319f7eaf4d2e'],False,"['moe_inference.py', 'checkpointing.py', 'bf16_optimizer.py', 'utils.py', 'partition_parameters.py', 'stage3.py', 'stage_1_and_2.py', 'test_partition.py']"
ba6bfd0b76ad93e2de0d719316ab1f14fa308393,"fix mics run with offload++ (#4749)

fix mics run error by making it compatible with zero-offload++

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['3418b869f0ec784b90a35c1253320780b03645a9'],False,['mics.py']
8640b8e52a93ba4e6b0da0e953c5761fc6977aa9,"Fix logger formatting for partitioning flags (#4728)

Logger takes only the first argument as a message.

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['ba6bfd0b76ad93e2de0d719316ab1f14fa308393'],False,['engine.py']
02288bc1fdd4dd0b48f3933db8948f5a0d5c9545,"fix: to solve #4726 (#4727)

To solve #4726 , I change the dtype of loss tensor into float32 in the
last stage of pipeline.

**test result**
before dist.broadcast

```
[2023-11-24 14:06:04,709] [INFO] [engine.py:590:_aggregate_total_loss] [Rank 2] before dist.broadcast(is_last_stage) (tensor([2.3203, 2.3203], device='cuda:2'), torch.float32, device(type='cuda', index=2)), src_rank=2 (1, 2)
[2023-11-24 14:06:04,710] [INFO] [engine.py:590:_aggregate_total_loss] [Rank 3] before dist.broadcast(is_last_stage) (tensor([2.3203, 2.3203], device='cuda:3'), torch.float32, device(type='cuda', index=3)), src_rank=3 (1, 2)
```

After dist.broadcast, you can see the broadcast result is correct
between rank 2 and rank 0 as well as rank 3 and rank 1.
```
[2023-11-24 14:06:05,016] [INFO] [engine.py:608:_aggregate_total_loss] [Rank 1] after dist.broadcast(other stage) (tensor([2.3203, 2.3203], device='cuda:1'), torch.float32)
[2023-11-24 14:06:05,043] [INFO] [engine.py:608:_aggregate_total_loss] [Rank 0] after dist.broadcast(other stage) (tensor([2.3203, 2.3203], device='cuda:0'), torch.float32)
```

For more information. please refer #4726.

Co-authored-by: ryan <ruanzhixiang1@huawei.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['8640b8e52a93ba4e6b0da0e953c5761fc6977aa9'],False,['engine.py']
f57fc4c95a6a5194757b57704f60f009dde25680,"Fix DS Stable Diffusion for latest diffusers version (#4770)

This PR fixes the DeepSpeed `UNet` forward function to work with the
latest `diffusers` version.

Manual `nv-sd` workflow run:
https://github.com/microsoft/DeepSpeed/actions/runs/7092016979

Fixes #4760",['4997b0fd22a0bdd81abb1266b877cf74e29f565b'],False,['unet.py']
9dfb06de36bb29293b1e94dc1e48d6f2adf54d2c,"Resolve any '..' in the file paths using os.path.abspath() (#4709)

This PR is to resolve any '..' in the file paths like below using
os.path.abspath()

```
sources:  ['/opt/conda/envs/py_3.9/lib/python3.9/site-packages/deepspeed/ops/../inference/v2/kernels/core_ops/core_ops.cpp', '/opt/conda/envs/py_3.9/lib/python3.9/site-packages/deepspeed/ops/../inference/v2/kernels/core_ops/bias_activations/bias_activation.cpp', '/opt/conda/envs/py_3.9/lib/python3.9/site-packages/deepspeed/ops/../inference/v2/kernels/core_ops/bias_activations/bias_activation.cu', '/opt/conda/envs/py_3.9/lib/python3.9/site-packages/deepspeed/ops/../inference/v2/kernels/core_ops/cuda_layer_norm/layer_norm.cpp', '/opt/conda/envs/py_3.9/lib/python3.9/site-packages/deepspeed/ops/../inference/v2/kernels/core_ops/cuda_layer_norm/layer_norm.cu', '/opt/conda/envs/py_3.9/lib/python3.9/site-packages/deepspeed/ops/../inference/v2/kernels/core_ops/cuda_rms_norm/rms_norm.cpp', '/opt/conda/envs/py_3.9/lib/python3.9/site-packages/deepspeed/ops/../inference/v2/kernels/core_ops/cuda_rms_norm/rms_norm.cu', '/opt/conda/envs/py_3.9/lib/python3.9/site-packages/deepspeed/ops/../inference/v2/kernels/core_ops/gated_activations/gated_activation_kernels.cpp', '/opt/conda/envs/py_3.9/lib/python3.9/site-packages/deepspeed/ops/../inference/v2/kernels/core_ops/gated_activations/gated_activation_kernels.cu']

extra_include_paths:  ['/opt/conda/envs/py_3.9/lib/python3.9/site-packages/deepspeed/ops/../inference/v2/kernels/includes', '/opt/conda/envs/py_3.9/lib/python3.9/site-packages/deepspeed/ops/../inference/v2/kernels/core_ops/bias_activations', '/opt/conda/envs/py_3.9/lib/python3.9/site-packages/deepspeed/ops/../inference/v2/kernels/core_ops/blas_kernels', '/opt/conda/envs/py_3.9/lib/python3.9/site-packages/deepspeed/ops/../inference/v2/kernels/core_ops/cuda_layer_norm', '/opt/conda/envs/py_3.9/lib/python3.9/site-packages/deepspeed/ops/../inference/v2/kernels/core_ops/cuda_rms_norm', '/opt/conda/envs/py_3.9/lib/python3.9/site-packages/deepspeed/ops/../inference/v2/kernels/core_ops/gated_activations']

```
It fixes the hipify errors that occur during JIT build of
'inference_core_ops' extension due to "".."" prefix in the paths,

https://github.com/microsoft/DeepSpeed/blob/0ec2d3e4bfa2d0a5237e9747da1ef9d5e4a4453b/op_builder/inference_core_ops.py#L73

https://github.com/microsoft/DeepSpeed/blob/0ec2d3e4bfa2d0a5237e9747da1ef9d5e4a4453b/op_builder/inference_core_ops.py#L90

cc @jithunnair-amd

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['f57fc4c95a6a5194757b57704f60f009dde25680'],False,['builder.py']
15ed83a9a69e4c558e0a5fac940d683993a025ca,"Update dockerfile with updated versions (#4780)

Fixes #4763",['9dfb06de36bb29293b1e94dc1e48d6f2adf54d2c'],False,['Dockerfile']
5f41bd06dd317f00e552ce8a7104c09021e4a86c,"Fix Hybrid Engine metrics printing (#4789)

The metrics print statement in the Hybrid Engine had a bug where
`self._total_batch_size` was assumed to exist even when this variable
was not initialized due to no corresponding inference policy.

This PR fixes this by performing a check when constructing the metrics
print.

Addresses: https://github.com/microsoft/DeepSpeedExamples/issues/593",['93a81b5362a83bacd7b40c838295909f347e37af'],False,['hybrid_engine.py']
29f840fd1ad6cda556fcec1cd34c17153a5c6450,"fix autoTP issue for mpt (trust_remote_code=True) (#4787)

to fix https://github.com/microsoft/DeepSpeed/issues/4774

Signed-off-by: Wang, Yi A <yi.a.wang@intel.com>",['5f41bd06dd317f00e552ce8a7104c09021e4a86c'],False,['auto_tp.py']
2bdf061f4dc8be70878f032d2e48d2130514f991,"[BUG] partition_balanced return wrong result. (#4312)

# Background

In pipeline parallelism, deepspeed uses `ds_utils.partition_balanced` to
balance the partitioning of the model according to the number of
parameters or class names.

https://github.com/microsoft/DeepSpeed/blob/581e44dd1ab3c409a5905335867c761d5cb4db5b/deepspeed/runtime/pipe/module.py#L380-L395

# What wrong?
```
>>> import deepspeed
>>> deepspeed.__version__
'0.10.3+542dc0d5'
>>> from deepspeed.runtime import utils as ds_utils
>>> ds_utils.partition_balanced([1, 1, 1, 1, 1], 4)
[0, 2, 4, 5, 5]
>>> 
```
the result [0, 2, 4, 5, 5] means [2, 2, 1, 0] layers for each part,
which is not balanced at all. the last part will throw an exception
because there are no parameters to training.

i add some unit test for this function, and i will fix it later if
anyone need it.

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['ce6070800a698fafe36a4752e8643847bc2bc8aa'],False,"['utils.py', 'test_partition_balanced.py']"
7b818ee96177771c4ce25db0900509c69cea95a0,"improve the way to determine whether a variable is None (#4782)

refactor: improve the way to decide whether a variable is None
fix: type mismatch for judging if current accelerator is in
SUPPORTED_ACCELERATOR_LIST

---------

Co-authored-by: ryan <ruanzhixiang1@huawei.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['2bdf061f4dc8be70878f032d2e48d2130514f991'],False,"['cpu_accelerator.py', 'cuda_accelerator.py', 'mps_accelerator.py', 'npu_accelerator.py', 'real_accelerator.py', 'layers.py', 'ragged_embedding.py', 'auto_tp.py', 'fusedqkv_utils.py', 'replace_module.py', 'tp_shard.py', 'stage3.py', 'comm.py', 'test_ds_arguments.py', 'test_zero_config.py']"
b18681692c06c5448f97c3784011379606420cee,"Fix for stage3 when setting different communication data type (#4540)

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['2ce6bf8ce019bf445a5148ffc55dfd8178dd280e'],False,['stage3.py']
3324efd9306bc19b3cd7a669b339f84983d3ebf8,"Switch paths-ignore to single quotes, update paths-ignore on nv-pre-compile-ops (#4805)",['a7900bcc3d2f7789cc734aa28a11d2f3b3d8b04f'],False,"['nv-a6000.yml', 'nv-accelerate-v100.yml', 'nv-inference.yml', 'nv-lightning-v100.yml', 'nv-megatron.yml', 'nv-pre-compile-ops.yml', 'nv-torch-latest-cpu.yml', 'nv-torch-latest-v100.yml', 'nv-transformers-v100.yml']"
7914b195c0a41ed01823ceac469b58407d9de021,"fix for tests using torch<2.1 (#4818)

Our torch 1.10 tests have been failling since the merge of #4569. This
added a `device_type` kwarg to the `torch.random.fork_rng` call. But
this is not compatible with older versions of torch. Added in
https://github.com/pytorch/pytorch/pull/98069

Fixes #4644, #4503",['3324efd9306bc19b3cd7a669b339f84983d3ebf8'],False,['alexnet_model.py']
84eaf5ac843234737f0b49e36a818d1aabd1776f,Accelerate CI fix (#4819),['8998707a2fc8584712a4cb3dc465d02e7d9f50da'],False,['nv-accelerate-v100.yml']
4a6e0c06240b45185709ac4a2902ec42518049d2,fix [BUG] 'DeepSpeedGPTInference' object has no attribute 'dtype' for… (#4814),['84eaf5ac843234737f0b49e36a818d1aabd1776f'],False,['ds_transformer.py']
d1f1d45f4b4eb86bf5b82ed617f09f528bb00d11,"Update broken link in docs (#4822)

resolves #4821",['4a6e0c06240b45185709ac4a2902ec42518049d2'],False,['getting-started.md']
b83b1c2e1c4dc4c91c4ad78773dc2232ca9f7070,Update imports from Transformers (#4817),['d1f1d45f4b4eb86bf5b82ed617f09f528bb00d11'],False,"['requirements-dev.txt', 'requirements-inf.txt', 'test_intX_quantization.py', 'test_zero_nesting_init.py']"
faa00b1373e2e5628c660d0f40cab485cd960c33,fix falcon model load from_config meta_data error (#4783),['bc1b5a6c06049f39d1e5c18bbe0f29a09e11f4a3'],False,['auto_tp.py']
d37fc25d568bbcfb36772d9eae4539d8deac1bd2,"Refactor launcher user arg parsing (#4824)

Splitting work from #4769 because we are still debugging transformers
integration issues.

Parsing was broken for user arguments (see #4795). Additionally, parsing
of user arguments is tricky and there are lots of edge cases. For
example: #4660, #4716, #3967. I've attempted to accommodate all of the
possible types of string inputs and added unit tests.",['449e454f83bb6a14b0de359660d4b206d5c3feed'],False,"['multinode_runner.py', 'runner.py', 'test_user_args.py']"
65b7727758a4c0ee08597a88ab4f051abcfc2a8a,"Fix 4649 (#4650)

Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>",['d37fc25d568bbcfb36772d9eae4539d8deac1bd2'],False,['runner.py']
4559dadd367453befd6c8f7d0049f8e900c897c3,"Cache metadata for TP activations and grads (#4360)

PartitionedTensor.from_meta will cause device to host synchronization
when reading the meta tensor in
meta = meta.tolist()
Added cpu cache for the meta tensor to avoid this synchronization in
every activation and grad communication between the ranks. The meta
tensor is assumed to be static since activation shape must be static.
The user must call reset_activation_shape if any of the dimentions
change.

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['4d866bd55a6b2b924987603b599c1f8f35911c4b'],False,['engine.py']
c20f6fa4e0a7772400fc6b52999a2a9c3263cec0,"support baichuan model: (#4721)

* fix Baichuan meta data error
* add BaichuanLayer and DecoderLayer to glmtype when prepare tp fused
qkvw
   * add get_alibi_mask function for Baichuan to enable TP

---------

Co-authored-by: Lai, Yejing <yejing.lai@intel.com>
Co-authored-by: Reza Yazdani <44502768+RezaYazdaniAminabadi@users.noreply.github.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>",['83fa673aaec23879248861a61028b775498a67a5'],False,"['engine.py', 'auto_tp_model_utils.py', 'fusedqkv_utils.py', 'replace_module.py']"
d5a7c1e0b494fbd0958bf8274bde0bacb2c16854,"Capture short kernel sequences to graph (#4318)

**Motivation:**
1. This is a series of cases where short kernel sequences are launched
and executed serially（no dynamic shape）, with the launch overhead being
much higher than the execution overhead. We can use a graph to solve
this problem. Compared to ```multi-tensor-apply```, using graph is more
concise and only requires PyTorch as a dependency.
2. Some device software stacks also support lazy-mode PyTorch, enabling
full utilization of the compiler to perform graph optimization. However,
in lazy mode, operation accumulation time (host time) could become
significantly higher compared to device time in such scenario, and
devices are usually not well utilized. By using the same API(after
adding to accelerator cc @delock ) with cuda graph, this issue could
also be resolved.

**Change:**
We modified three functions, 
```update_hp_grads```. Here, we executed the operations for the CPU and GPU separately because the graph is unable to record the execution of CPU operations. Additionally, the data input required by the graph must not have its address modified, or the address modification must be captured by the capture operation(In this case, set ```replay_first_step``` to ```True```). Therefore, we changed ```grad=None``` to ```grad.zero_()```. Similarly, we have also placed some inputs that require fixed addresses in the ```graph_cache``` 

For ```clip_tensors_by_global_norm```, ```clip_coef``` is a scalar with a non-fixed value, so it needs to be moved to the GPU when using a graph.


For ```total_norm = sum ([t. data. float (). norm (norm_type). item () * * norm_type for t in input_tensors])```, ```item () ```, synchronous operation is also not supported by graph. We directly put the ```sum``` and ```* * norm_type``` on the GPU to execute the computation.

Other similar scenarios can also use this ```graph_process()```, or a slightly modified version of ```graph_process()```

you can checkout
[4abab21](https://github.com/microsoft/DeepSpeed/pull/4318/commits/4abab212c8f5aef1eec4f8abe10b4262bb5a5c8a)  and set it to True here to do some benchmarking.
https://github.com/microsoft/DeepSpeed/pull/4318/commits/4abab212c8f5aef1eec4f8abe10b4262bb5a5c8a#diff-f8f0b3feb55b0374615405e542c1c3e0f017982b177c46c562bf688532ac935cR42

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['060a8e185a33bdd45cacc826f5e309a7e3675f8a'],False,"['abstract_accelerator.py', 'cpu_accelerator.py', 'cuda_accelerator.py', 'mps_accelerator.py', 'npu_accelerator.py', 'engine.py', 'unet.py', 'vae.py', 'clip_encoder.py', 'bf16_optimizer.py', 'config.py', 'constants.py', 'engine.py', 'utils.py']"
9e455d76516e785cbaf058d351b6a78d02c42ed8,"Checkpointing: Avoid assigning tensor storage with different device (#4836)

On some back-ends, assigning tensor.data to a storage being on a
different
device than the tensor is not supported.

The fix is to save the storage in a temp data member and restore
tensor.data
when needed.

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['d5a7c1e0b494fbd0958bf8274bde0bacb2c16854'],False,['checkpointing.py']
c00388a2ef933f243b28d89bafb1b329d72557ad,"Mixtral FastGen Support (#4828)

Adds support for Mixtral with FastGen. Key features implemented:

1. Top-2 MoE support
2. Better support for RoPE thetas
3. The mistral model implementation

---------

Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>",['18643914bb5b4be9150711fa26abddc2de4641e7'],False,"['huggingface_engine.py', 'engine_factory.py', '__init__.py', 'top_k_utils.h', 'blocked_kv_rotary.cpp', 'blocked_kv_rotary.cu', 'blocked_kv_rotary.cuh', 'blocked_kv_rotary.h', 'blocked_kv_rotary.py', 'moe_gather.cpp', 'moe_gather.cu', 'moe_gather.cuh', 'moe_gather.h', 'moe_gather.py', 'moe_scatter.cpp', 'moe_scatter.cu', 'moe_scatter.cuh', 'moe_scatter.py', 'ragged_ops.cpp', '__init__.py', 'top_k_gating.cpp', 'top_k_gating.cu', 'top_k_gating.cuh', 'top_k_gating.h', 'top_k_gating.py', '__init__.py', 'moe_parameters.py', '__init__.py', 'container.py', 'model.py', 'policy.py', 'inference_transformer_base.py', '__init__.py', 'container.py', 'model.py', 'policy.py', 'model.py', 'policy.py', '__init__.py', 'container.py', 'model.py', 'policy.py', 'container.py', 'model.py', 'policy.py', '__init__.py', 'attention_configs.py', 'moe_config.py', 'dense_blocked_attention.py', 'cutlass_multi_gemm.py', 'ragged_ops.py', 'test_moe_gather.py', 'test_moe_scatter.py', 'test_top_k_gating.py', 'test_parameter_list.py', 'test_blocked_attn.py', 'test_cutlass_moe.py']"
c37fe9cbfb8bc10c8dd6ccd8cac9b34ded218990,"Fix exception handling in get_all_ranks_from_group() function (#4862)

In the latest Pytorch nightly, the exception thrown from
`torch.distributed.distributed_c10d.get_global_rank()` is changed from
`RuntimeError` to `ValueError` so we need to update our try-catch in
`deepspeed.comm`

Tested with torch version 2.3.0.dev20231221+cu121

Fixes: https://github.com/microsoft/DeepSpeed/issues/4853",['75c772021484fa0f9d9a3872a90c0876f7cf59d8'],False,['comm.py']
40342055cefda4c453e803759861b00ca5cfb879,"Remove hooks on gradient accumulation on engine/optimizer destroy (#4858)

Fixes: #4856 
See: https://github.com/pytorch/pytorch/issues/46386",['3e94f8c75116377d4b1c32b8c674368a27fb2a77'],False,"['engine.py', 'stage3.py', 'stage_1_and_2.py', 'debug.py']"
9ec55bd99bf39866facc8d0925b3780e37270296,"Fix f-string messages (#4865)

Fix error messages that missed the f prefix.

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['ea0d81143c6ba0801828919ea53888843d0fb19f'],False,"['contiguous_memory_allocator.py', 'stage_1_and_2.py', 'test_ds_config_dict.py']"
d873ce61593e3d436a8684731901693dee7bd957,"[NPU] Fix npu offload bug (#4883)

There are some syntax errors in the NPU offload. 

There may be no AVX instruction set on our server due to environment
variables, as a result, this problem is not verified in our tests.

Sorry for the inconvenience and we will be more cautious in the next
PRs.

Co-authored-by: jializheng <jializheng@huawei.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['9ec55bd99bf39866facc8d0925b3780e37270296'],False,"['cpu_adagrad.h', 'cpu_adam.h', 'cpu_lion.h']"
834272531aa4368f793cc78418612e1e09166094,"Add support of Microsoft Phi-2 model to DeepSpeed-FastGen (#4812)

This PR adds support for Microsoft Phi-2 model.

HF output with prompt ""DeepSpeed is"":
```
a company that helps make videos and movies look really good. They have a special way of making videos that makes them look like they were made in a movie theater. This is called ""4K Ultra HD"" and it makes the videos look very clear and detailed. DeepSpeed also has a special way of making videos that makes them look like they were made in a movie theater. This is called ""4K Ultra HD"" and it makes the videos look very clear and detailed. DeepSpeed also has a special way of making videos that makes them look like they were made in a movie theater. This is called ""4K Ultra HD""
```

DeepSpeed-FastGen output with prompt ""DeepSpeed is"":
```
a company that helps make videos and movies look really good. They have a special way of making videos that makes them look like they were made in a movie theater. This is called ""4K Ultra HD"" and it makes the videos look very clear and detailed. DeepSpeed also has a special way of making videos that makes them look like they were made in a movie theater. This is called ""4K Ultra HD"" and it makes the videos look very clear and detailed. DeepSpeed also has a special way of making videos that makes them look like they were made in a movie theater. This is called ""4K Ultra HD""
```

---------

Co-authored-by: Connor Holmes <connorholmes@microsoft.com>
Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>",['691458f8b6e6a58ceae7e41776e55c0410303009'],False,"['README.md', 'engine_factory.py', 'blocked_kv_rotary.cpp', 'blocked_kv_rotary.cu', 'blocked_kv_rotary.cuh', 'blocked_kv_rotary.h', 'blocked_kv_rotary.py', 'blocked_trained_kv_rotary.py', 'linear_blocked_kv_copy.py', '__init__.py', 'model.py', 'model.py', 'model.py', 'model.py', '__init__.py', 'containers.py', 'model.py', 'policy.py', 'attention_configs.py', 'dense_blocked_attention.py', 'ragged_unembed.py', 'ragged_manager.py', 'test_blocked_kv_copy.py', 'test_blocked_rotary_emb.py']"
c84c28d23b4e9ee13ea4d561f8da129f73d05571,"Support cpu tensors without direct device invocation (#3842)

Motivation:
Fix for reproducible issue #3837 on cpu. On cpus direct invocation of
torch.cpu.tensor leads to dtype mismatch.
Another way would be to have something like :
[""torch.DoubleTensor"" if device_type == 'cpu else
'""torch.{}.DoubleTensor"".format(device_type)] for all elements in the
supported list , but that would eliminate ""torch.cpu.DoubleTensor"" ,etc
from the scope.
@jeffra requesting review.
  
CLA is signed

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: inkcherry <mingzhi.liu@intel.com>",['834272531aa4368f793cc78418612e1e09166094'],False,"['engine.py', 'sparse_tensor.py']"
af033831f2aeeca203c0918e8a552915331021e3,"Release overlap_comm & contiguous_gradients restrictions for ZeRO 1 (#4887)

The `overlap_comm` and `contiguous_gradients` options have been ignored
in ZeRO stage 1 since https://github.com/microsoft/DeepSpeed/pull/1246.
Back in that time, ZeRO 1 and 2 are separately implemented (see
https://github.com/microsoft/DeepSpeed/tree/6ae756c03f12674f17aef90622e7664a8af9d2af/deepspeed/runtime/zero).
ZeRO 1 does not have gradient hooks registered to overlap backward and
gradient all-reduce, so it's fine to ignore `overlap_comm` and
`contiguous_gradients`. However, in the current implementation, ZeRO 1
and 2 share almost the same implementation (`stage_1_and_2.py`).
Features like `overlap_comm` and `contiguous_gradients` can also be
enabled for ZeRO 1 (Please correct me if I made a mistake).

With this PR, turning on `overlap_comm` and `contiguous_gradients` for
ZeRO 1 on the [SFT
task](https://github.com/microsoft/DeepSpeedExamples/tree/master/applications/DeepSpeed-Chat/training/step1_supervised_finetuning)
produces exactly the same training curve as the latest master.


![image](https://github.com/microsoft/DeepSpeed/assets/39846316/bda3be7b-c236-4e08-b687-b3cd01f5cc73)

I also see a ~1.05x e2e speedup by overlapping backward and gradient
all-reduce. I can confirm by the trace that backward and all-reduce do
overlap, and the separate gradients are indeed copied to a flat buffer.
These options are also effective for ZeRO 1.


![image](https://github.com/microsoft/DeepSpeed/assets/39846316/5f876296-e1b4-404b-8b33-03cee8e5e6b2)


![image](https://github.com/microsoft/DeepSpeed/assets/39846316/9654f6be-5c7a-401a-b0bc-413ecd3f4e6b)

Related issue: https://github.com/microsoft/DeepSpeed/issues/2295

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['85132adc31956bcab78a20ac9dcd5789d654c309'],False,['engine.py']
1787673edc7e45cd79fe10b95f92a02d3eb91505,"fix num_kv_heads sharding in uneven autoTP for Falcon-40b (#4712)

Falcon-40b will fail on uneven autotp. Need to add 'num_kv_heads' in the
kv_head_names list.

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>",['b596963b06045055e6d3d21f7831f2fd11b4e763'],False,['auto_tp.py']
d058d4b39bd352f49cc50880b89d2f6109891397,"Nvme offload checkpoint (#4707)

Previous PR #4416 had too many issues, closing that one and re-opening.
This PR includes a passing test.

This is a proposal for an implementation of checkpointing models when
training with ZeRO-3 with NVMe offload:

1. Currently, the names of the files used in the checkpoint are based on
the Python id of the parameter object, which is just the parameter's
address in memory. This is not stable across runs, which has two
disadvantages:
- The NVMe offloading files grow with every run of the model even if the
architecture did not change. This wastes disk space and, at least for
me, was a surprise when I first saw it. It is not related to
checkpointing.
- Without a way to match the file to the offloaded tensor we can't
reload the checkpoint.

We propose an alternative naming scheme. The parameters are named after
their ds_id instead of their Python id, and the tensors are named after
their state_name and (new) parameter id.

2. A model checkpoint now has to include all the offloaded tensor files.
During checkpoint save/load we copy all the tensor files to/from the
""offloaded_tensors"" subdirectory of the checkpoint. We provide some
logging on the remaining space on the file system due to the potential
size of these files, especially as they accumulate in each checkpoint.
We do not copy the gradient files.

3. When loading the checkpoint, the optimizer already has prepared
buffers for swapping. We need to purge them so that they are replaced
with the freshly copied on-disk buffers from the checkpoint.

The key differences between this PR and the previous one:

- There's a test for a simple model with parameter/optimizer offload set
to cpu/cpu, cpu/nvme and nvme/nvme.
 - Gradient files are not copied.
 - FP16 and FP32 parameter buffers are handled correctly during load.

Fixes #2082.

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['1787673edc7e45cd79fe10b95f92a02d3eb91505'],False,"['engine.py', 'optimizer_utils.py', 'partitioned_optimizer_swapper.py', 'pipelined_optimizer_swapper.py', 'stage3.py', 'test_nvme_checkpointing.py']"
d7b764e3d86e499479d321ad065c3f0d9710ece1,"Unit tests for MiCS (#4792)

In response to the ask from
https://github.com/microsoft/DeepSpeed/pull/2964#issuecomment-1832161865,
I added three more unit tests related to MiCS.

There are two knowledge issues:
- Testing on Torch 2.1.0 triggers `_IllegalWorker` in coalesced all
gather. I made changes to ignore this condition. and Currently, I don't
know the reason.
- The MiCS implementation is not working with offloading, so the failure
in `TestZeroPartialOffloadConfigSweep` is expected.

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['838c69be39b5453a79e7381f4886b9cb16a15e89'],False,"['mics.py', 'test_zero.py', 'test_zero_context.py', 'test_zero_offloadpp.py']"
75db3d7da7b92243dcea8ed03e58205736e68260,"Fix SD workflow to work with latest diffusers version (#4918)

This PR fixes the Stable Diffusion workflow to work with the latest
`diffusers` version (`0.25.0`).

Fixes #4911.

Manual test:
https://github.com/microsoft/DeepSpeed/actions/runs/7452977322",['d7b764e3d86e499479d321ad065c3f0d9710ece1'],False,"['vae.py', 'requirements-sd.txt']"
d8d865f4926e66acefe2535c360568a302a6dc8f,"[Fix] Fix cpu inference UT failure (#4430)

This PR fix UT test error as described in this PR and the following test
job. This PR skips `TestModelTask` if dtype is not supported by
accelerator, or `InferenceBuilder` is not implemented by accelerator.
https://github.com/microsoft/DeepSpeed/pull/4419

https://github.com/microsoft/DeepSpeed/actions/runs/6341645987/job/17235544538

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Liangliang-Ma <1906710196@qq.com>
Co-authored-by: Quentin Anthony <qganthony@yahoo.com>
Co-authored-by: Dashiell Stander <dash.stander@gmail.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Ramya Ramineni <62723901+rraminen@users.noreply.github.com>
Co-authored-by: Xie Zejian <xiezej@gmail.com>
Co-authored-by: Conglong Li <conglong.li@gmail.com>
Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>",['75db3d7da7b92243dcea8ed03e58205736e68260'],False,"['cpu-inference.yml', 'ccl.cpp', 'ccl.py', 'accelerator-abstraction-interface.md', 'test_inference.py', 'test_inference_config.py']"
16c265c0ce103147d027d9cae32dd7680766af21,"fix falcon-40b accuracy issue (#4895)

This [PR](https://github.com/microsoft/DeepSpeed/pull/4721) added the
""DecoderLayer"":glmtype. It will cause the Falcon model to choose
""glmtype"" fused_qkv_type. Falcon model (including Falcondecoderlayer)
needs to choose 'bloomtype' explicitly.

Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>",['43eba775ee56e45782666cbe52f05c1cc5668bfe'],False,['fusedqkv_utils.py']
c1e02052aca93a79e08ea5a9064f4353db03a34c,"Refactor the positional emebdding config code (#4920)

The Mixtral PR https://github.com/microsoft/DeepSpeed/pull/4828 has
introduced the positional embedding config class which is a required
argument of `make_attn_layer()` function. This has forced the user to
override and duplicate the `make_attn_layer()` call for new model
implementations using RoPE (This has also broken the Falcon model
implementations). This PR:

- refactors the inference transformer base class to avoid code
duplication by adding a new abstract `positional_embedding_config`
property
- Fixes the Falcon model implementation to use positional embedding
config.

The models `llama_v2`, `OPT`, `Mistral 7B`, `Mixtral`, `Falcon` and
`Phi-2` are tested with the PR!

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['16c265c0ce103147d027d9cae32dd7680766af21'],False,"['model.py', 'inference_transformer_base.py', 'model.py', 'model.py', 'model.py', 'model.py', 'model.py', 'dense_blocked_attention.py']"
acdf570a1bc8dbdf5cd985788ee83d3adff0acbb,Pin to triton 2.1.0 to fix issues with nv-inference (#4929),['c1e02052aca93a79e08ea5a9064f4353db03a34c'],False,['requirements-triton.txt']
f5179031623442ae19fed8158e3781c81efe088d,"Fix confusing width in simd_load (#4714)

I found the width using in simd_load is different from simd_store.
This implementation confuses me.
The reason lies in the missing parentheses for the type conversion of x
in the SIMD_LOAD2 macro definition, disrupting the intended semantics of
width variable.
I try to make a quick fix for it.

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['ade98365fc0f7521c3dee20dfd33637a17ceab26'],False,['simd.h']
62004e0d739b4b40c79c4977c7771ee735abe4b4,"InfV2 - remove generation config requirement (#4938)

Fix for https://github.com/microsoft/DeepSpeed-MII/issues/287

Some HF models do not have a generation config. We should only try to
load it if the `max_seq_length` is not present in the model config.

Also fix bug introduced in #4936",['c6b975b84465d1b770f960ba44528631bc91939b'],False,"['nv-a6000.yml', 'huggingface_engine.py']"
13d84b4912492d40b68f4da7bc570e47b54bce2a,"Cache HF model list for inference tests (#4940)

Cache the model list in blob storage so it can be shared across CI
runners. Code borrowed from MII:
https://github.com/microsoft/DeepSpeed-MII/blob/95d1e1c8890a016f2b5788414754abbbfd4540ae/mii/utils.py#L39",['62004e0d739b4b40c79c4977c7771ee735abe4b4'],False,['test_inference.py']
05cc3462c948c7bc6484818830133393ff83f824,"Fix docs inconsistency on default value for `ignore_unused_parameters` (#4949)

Link to code where the default is set:
https://github.com/microsoft/DeepSpeed/blob/13d84b4912492d40b68f4da7bc570e47b54bce2a/deepspeed/runtime/zero/config.py#L242C4-L242C42",['13d84b4912492d40b68f4da7bc570e47b54bce2a'],False,['config-json.md']
43daf413b27ca732255f8e838b30ead5733afc92,Fix bug in CI model caching (#4951),['05cc3462c948c7bc6484818830133393ff83f824'],False,['test_inference.py']
29417ab55f0cc4038e459f055e3c5aeb8878a80c,"fix uneven issue & add balance autotp (#4697)

This PR aims to balance the shard size of each worker as even as
possible.
1. We refactor the tp_shard logic that can make AutoTP work when
split_shape % num_kv_heads != 0.
2. When num_kv_heads is defined, the attention module relies on it to
sharding, but the mlp and lm_head modules can use near even division to
get more balance shard. It will get better performance.

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Lev Kurilenko <113481193+lekurile@users.noreply.github.com>",['43daf413b27ca732255f8e838b30ead5733afc92'],False,"['auto_tp.py', 'layers.py', 'tp_shard.py']"
a85b6e472534d2e0b61fe234fae4f6a2332c95bf,"Fix bug where ZeRO2 never uses the reduce method. (#4946)

On this PR https://github.com/microsoft/DeepSpeed/pull/4695, the
gradient synchronization operation is moved to the `allreduce_bucket`
method, but on this method, rank is set to None, and it will never use
the reduce method even if `use_multi_rank_bucket_allreduce` is set to
False.

Co-authored-by: jializheng <jializheng@huawei.com>",['d0b238a3bd49ed8dba0143694e7ad4e4507c21b6'],False,['stage_1_and_2.py']
69a459887f461eddb0e2e62e92edb87836e7cf1e,"Stage_1_and_2.py: fix assert for reduce_scatter configurations combinations (#4964)

today it does not allow to set reduce_scatter with:
- invalid comm data type
- gradient_predivide_factor != 1.0
- postscale_gradients is False but reduce scatter is relevant only for
Zero2, also reduce_scatter is set to True by default. So setting
prescale_gradients=True in json will end up with assert on Zero1.

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['e278076495078cb0450f911dd86d32fbc4f726a2'],False,['stage_1_and_2.py']
752a50acddaf122f86e9b3f6f60d7d233a76402e,"[MiCS]Add the path to support sequence_data_parallel on MiCS (#4926)

This pr is to match the latest update on sequence_data_parallel for
MiCS, sequence_data_parallel_group is added in the latest
Megatron-DeepSpeed init method
https://github.com/microsoft/Megatron-DeepSpeed/blob/main/pretrain_gpt.py#L39,
if we want to enable zero3+MiCS on GPT training it will be unsupported,
add the path to support the sequence_data_parallel can fix this issue.

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['69a459887f461eddb0e2e62e92edb87836e7cf1e'],False,['mics.py']
1b34a4d3053db1d3e3fac0481aed796fa04b9c76,"Update the DeepSpeed Phi-2 impl. to work with the HF latest changes (#4950)

The latest changes in Huggingface Phi-2 implementation
(https://huggingface.co/microsoft/phi-2/commit/cb2f4533604d8b67de604e7df03bfe6f3ca22869#d2h-025836)
have broken the DeepSpeed implementation. This PR address the related
issues.

---------

Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>",['752a50acddaf122f86e9b3f6f60d7d233a76402e'],False,"['engine_factory.py', 'containers.py', 'model.py', 'policy.py']"
740080c050e700f970deb73acc70615aaadfdecd,"Prevent infinite recursion when DS_ACCELERATOR is set to cuda (#4962)

When DS_ACCELERATOR is overriden to CUDA, `get_accelerator` attempts to
check if `is_current_accelerator_supported`. But since that calls
`get_accelerator` again and `ds_accelerator` has not been initialized,
DeepSpeed runs into infinite recursion.

```
    elif is_current_accelerator_supported():
  File ""/usr/local/lib/python3.8/dist-packages/deepspeed/accelerator/real_accelerator.py"", line 48, in is_current_accelerator_supported
    return get_accelerator().device_name() in SUPPORTED_ACCELERATOR_LIST
  File ""/usr/local/lib/python3.8/dist-packages/deepspeed/accelerator/real_accelerator.py"", line 101, in get_accelerator
    elif is_current_accelerator_supported():
  File ""/usr/local/lib/python3.8/dist-packages/deepspeed/accelerator/real_accelerator.py"", line 48, in is_current_accelerator_supported
    return get_accelerator().device_name() in SUPPORTED_ACCELERATOR_LIST
  File ""/usr/local/lib/python3.8/dist-packages/deepspeed/accelerator/real_accelerator.py"", line 101, in get_accelerator
    elif is_current_accelerator_supported():
  File ""/usr/local/lib/python3.8/dist-packages/deepspeed/accelerator/real_accelerator.py"", line 48, in is_current_accelerator_supported
    return get_accelerator().device_name() in SUPPORTED_ACCELERATOR_LIST
  File ""/usr/local/lib/python3.8/dist-packages/deepspeed/accelerator/real_accelerator.py"", line 101, in get_accelerator
    elif is_current_accelerator_supported():
  File ""/usr/local/lib/python3.8/dist-packages/deepspeed/accelerator/real_accelerator.py"", line 48, in is_current_accelerator_supported
    return get_accelerator().device_name() in SUPPORTED_ACCELERATOR_LIST
  File ""/usr/local/lib/python3.8/dist-packages/deepspeed/accelerator/real_accelerator.py"", line 59, in get_accelerator
    if ""DS_ACCELERATOR"" in os.environ.keys():
  File ""/usr/lib/python3.8/_collections_abc.py"", line 717, in __contains__
    return key in self._mapping
  File ""/usr/lib/python3.8/_collections_abc.py"", line 666, in __contains__
    self[key]
  File ""/usr/lib/python3.8/os.py"", line 672, in __getitem__
    value = self._data[self.encodekey(key)]
RecursionError: maximum recursion depth exceeded
```

This change fixes that by comparing the accelerator directly with the
supported list of accelerators.",['1b34a4d3053db1d3e3fac0481aed796fa04b9c76'],False,['real_accelerator.py']
870ae041d42190be8139afc12bef51d6ed7719f3,"Fixes for training models with bf16 + freshly initialized optimizer via `load_module_only` (#4141)

This PR makes some fixes to the case where we want to resume training
from a DeepSpeed ZeRO checkpoint and initialize a new optimizer, while
not using the old optimizer in the checkpoint or relying on its
existence at all.

in this situation, despite passing `load_module_only=True` and
`load_optimizer_states=False` to `load_checkpoint()`, the previous
behavior was that:
- `self._load_zero_checkpoint` would still be called, which attempts to
load from the (in this case, nonexistent) checkpoint files. This PR
stops this function from being called if using `load_module_only=True`
and `load_optimizer_states=False`. Alternatively, calling this function
may be alright if `""load_from_fp32_weights"": true` is set in the
DeepSpeed ZeRO config (reference:
https://github.com/microsoft/DeepSpeed/blob/ff7d5275f2aa916cb5f320e0d817154e96f9cdb6/deepspeed/runtime/engine.py#L733)
but this parameter does not seem to be documented in the docs for ZeRO
config dicts.
- in `_load_checkpoint`, the following codeblock: 
```
if self.optimizer is not None and self.fp16_enabled():
    self.optimizer.refresh_fp32_params()
```
results in `self.optimizer.refresh_fp32_params()` being called only if
using FP16. As a result, the FP32 optimizer state is never initialized
from the 16-bit model weights. This PR removes the fp16-specific
condition.


Previously reported in:
https://github.com/EleutherAI/gpt-neox/issues/947
https://github.com/EleutherAI/gpt-neox/issues/843

Should also close:
https://github.com/microsoft/DeepSpeed/issues/4017

Fixes: #4944 and #4017

This caused problems for a freshly-converted LLama checkpoint, which did
not contain optimizer states, when trying to train with this model as
initialization. I have confirmed the following fixes prevent this
behavior.

cc @Quentin-Anthony @zhangir-azerbayev

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['740080c050e700f970deb73acc70615aaadfdecd'],False,['engine.py']
3110c38852ceb8d531f9577cbf6b74db5cbe5838,"params partition for skip_init (#4722)

Some models use ```skip_init``` to initialize weights. ```skip_init```
first initializes on a meta device in ```__init__``` of a module and
then uses ```to_empty()```. This conflicts with the deepspeed hook
```module.__init__``` mechanism. it's necessary to wait for
```skip_init``` to finish before executing ```_post_init_method```.
However, the ```from ... import skip_init``` behavior typically occurs
outside the context, there seems to be no good way to directly hook into
```skip_init```. Hence, the approach here is to delay the execution of
```_post_init_method``` to resolve this issue.
Known affected models include HuggingFace models like chatglm2 and
chatglm3.""

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Masahiro Tanaka <81312776+tohtana@users.noreply.github.com>",['870ae041d42190be8139afc12bef51d6ed7719f3'],False,"['partition_parameters.py', 'test_zero.py']"
96c5a873e6e73dcff9effdc10509a2e6ab564a39,"Add API to set a module as a leaf node when recursively setting Z3 hooks (#4966)

ZeRO3 does not work with MoE models because the order of executing
modules can change at every forward/backward pass (#4094, #4808).

This PR adds an API to stop breaking down a module for parameter
fetching. The following shows an example of the usage:
```python
import torch
import deepspeed
import deepspeed.comm as dist
from transformers.deepspeed import HfDeepSpeedConfig
from transformers import AutoTokenizer, AutoModelForCausalLM
from transformers.models.mixtral.modeling_mixtral import MixtralSparseMoeBlock

model_id = ""mistralai/Mixtral-8x7B-v0.1""
ds_config = {
      ""bf16"": {
          ""enabled"": True,
      },
      ""zero_optimization"": {
          ""stage"": 3,
      },
      ""train_micro_batch_size_per_gpu"": 1,
  }

hfdsc = HfDeepSpeedConfig(ds_config)

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16)

deepspeed.utils.set_z3_leaf_modules(model, [MixtralSparseMoeBlock])
model.eval()

ds_engine = deepspeed.initialize(model=model, config_params=ds_config)[0]
ds_engine.module.eval()
model = ds_engine.module

inputs = tokenizer.encode(""DeepSpeed is"", return_tensors=""pt"").to(""cuda"")
outputs = model.generate(inputs, max_new_tokens=200)
output_str = tokenizer.decode(outputs[0])
if dist.get_rank() == 0:
  print(f""output: {output_str}"")
```

By passing names of modules to `set_z3_leaf_modules`, DeepSpeed engine
stops breaking down the module.

In this example, `MixtralSparseMoeBlock` has multiple experts as its
submodule. Using `set_z3_leaf_modules`, the DeepSpeed engine fetches
parameters of all the submodules when pre-fetching the parameters of
`MixtralSparseMoeBlock`.",['5dea776a843da475c1f94d47979ad21e56b2a00e'],False,"['parameter_offload.py', 'partitioned_param_coordinator.py', '__init__.py', 'z3_leaf_module.py', 'test_zero_leaf_module.py']"
e62a47e2e83647cc119c39cc7cf98fcfb5451cba,"Fix T5 and mistral model meta data error (#4958)

Fix 'NotImplementedError: Cannot copy out of meta tensor; no data!',
when loading T5 and mistral from device meta.

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['96c5a873e6e73dcff9effdc10509a2e6ab564a39'],False,['auto_tp.py']
9d2660d2a3fac767972f01ac96858b2605ffc0e4,"Fix the MoE-params gradient-scaling (#4957)

This PR fixes a bug that I introduced in a previous
[PR](https://github.com/microsoft/DeepSpeed/pull/4695). The MoE-Params'
gradients got accidentally double-scaled due to passing
`self.ipg_bucket_has_moe_params` to the all_reduce functions. Since, we
have already done the scaling the MoE parameters
[here](https://github.com/microsoft/DeepSpeed/blob/master/deepspeed/runtime/zero/stage_1_and_2.py#L1054),
we can safely pass `divide=False`. The divide argument may not be needed
anymore, however, I just let it be there as I think it may be needed for
the sequence-parallelism accuracy stability adjustments.

cc: @tjruwase",['7fb5bade3e57c0acd126a5e9e871fc28d569b931'],False,['stage_1_and_2.py']
b81bed69a8db3c1e3263c27f48dcecf12b354931,"fix some typo under blogs/ (#4988)

fix some typo under blogs/

detail info
	modified:   blogs/comm-opt/README.md
	modified:   blogs/deepspeed-fastgen/README.md
	modified:   blogs/deepspeed-offloadpp/README.md
	modified:   blogs/deepspeed-triton/README.md
	modified:   blogs/deepspeed-ulysses/README.md
	modified:   blogs/deepspeed-visualchat/10-03-2023/README-Japanese.md
	modified:   blogs/deepspeed-visualchat/10-03-2023/README.md",['9d2660d2a3fac767972f01ac96858b2605ffc0e4'],False,"['README.md', 'README.md', 'README.md', 'README.md', 'README-Japanese.md', 'README.md']"
4738a5e61f5a58b4098be6b9a6ddbaee41197e59,Fix placeholder value in FastGen Blog (#5000),['b81bed69a8db3c1e3263c27f48dcecf12b354931'],False,['README.md']
a59455fa7371d3387b271a59ed3f4db01ffdaf86,"fix for DS_ENV issue (#4992)

When there exist two `.deepspeed_env` files (e.g., at `~/.deepspeed_env`
and `./.deepspeed_env`), only the local version will be read and used.
The behavior changed with the support of `$DS_ENV`
(https://github.com/microsoft/DeepSpeed/pull/4006). This PR allows
multuple `.deepspeed_env` files from all the
`DEEPSPEED_ENVIRONMENT_PATHS` paths to be read and used when launching.

/cc @loadams @mrwyattii",['4738a5e61f5a58b4098be6b9a6ddbaee41197e59'],False,['runner.py']
7d5113984ece6cbdb833cd396c46c231a941a0a5,"Make installable without torch (#5001)

Fixes #4984",['538ffb4b60850f12f488abef85185e7c7a3655f8'],False,"['release.yml', 'async_io.py', 'builder.py']"
75ed63c94f75014a33038c0c527d0752e5f15101,"Enable hpz based on secondary tensor presence (#4906)

Previously we use a series of forward/backward flags to control if hpz
should be enabled on certain allgather call. This PR simplifies this by
enabling hpz only when its secondary tensor exists (and invalidating its
secondary tensor whenever master weights changes). This should:
1. Prevent potential out-of-sync issue compared with our currently way
of overwriting secondary tensor
2. Improve throughput because now hpz will be enabled in a lot of
different scenarios including i) activation checkpointing, ii) gradient
accumulation, iii)`torch.no_grad` context, iv) `model.eval()` mode,
v)LoRA frozen weights, vi) gradient overflow

This is to fix https://github.com/microsoft/DeepSpeed/issues/4851

Convergence test:

- llama-2-7b random weights, using
https://github.com/microsoft/DeepSpeedExamples/blob/master/applications/DeepSpeed-Chat/training/step1_supervised_finetuning/training_scripts/llama2/run_llama2_7b.sh.

> zero-3 Baseline: Evaluating perplexity, Epoch 4/4: ppl:
5.151907920837402, loss: 1.6393671035766602
> hpz with this PR: ppl: 5.081737518310547, loss: 1.6256532669067383

- llama-2-7b pretrained weights with lora, using
https://github.com/microsoft/DeepSpeedExamples/blob/master/applications/DeepSpeed-Chat/training/step1_supervised_finetuning/training_scripts/llama2/run_llama2_7b_lora.sh.

> zero-3 Baseline: Evaluating perplexity, Epoch 4/4: ppl:
1.8326854705810547, loss: 0.6057823896408081
> hpz with this PR: ppl: 1.8326854705810547, loss: 0.6057823896408081

Performance test on 32 V100, still using
https://github.com/microsoft/DeepSpeedExamples/blob/master/applications/DeepSpeed-Chat/training/step1_supervised_finetuning/training_scripts/llama2/run_llama2_7b.sh.
- gradient accumulation step = 8
> master branch with hpz: SamplesPerSec=17.567813158654847
> this patch with hpz: SamplesPerSec=24.121657876029225
- lora
> master branch with hpz: SamplesPerSec=33.88883430864484
> this patch with hpz: SamplesPerSec=43.39463460004735

---------

Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>",['bc8b1ae33c74c74dde97936b9b1a81c9571d4a3e'],False,"['nv-nightly.yml', 'parameter_offload.py', 'partition_parameters.py', 'partitioned_param_coordinator.py', 'stage3.py', 'test_zeropp.py']"
0268d551188cb8f0021d76b7d6aedffbe613e32b,"Enable workflow dispatch on all workflows (#5016)

Enable `workflow_dispatch:` on all workflows where we did not have it
previously. This allows us to run the workflows on branches without
creating a PR or without needing to merge to master to see the results
of CI/scheduled builds.",['75ed63c94f75014a33038c0c527d0752e5f15101'],False,"['amd-mi200.yml', 'cpu-inference.yml', 'formatting.yml', 'nv-accelerate-v100.yml', 'nv-h100.yml', 'nv-inference.yml', 'nv-lightning-v100.yml', 'nv-megatron.yml', 'nv-mii.yml', 'nv-pre-compile-ops.yml', 'nv-sd.yml', 'nv-torch-latest-cpu.yml', 'nv-torch-latest-v100.yml', 'nv-torch-nightly-v100.yml', 'nv-torch110-p40.yml', 'nv-torch110-v100.yml', 'python.yml']"
d2e9adce39ce2805d1259e42ac4b4b759122ab72,"Fix error report of DSElasticAgent._set_master_addr_port() (#4985)

**The error**
Fixes #4459 
```
rogpt1: Traceback (most recent call last):      
rogpt1:   File ""/opt/conda/lib/python3.10/runpy.py"", line 196, in _run_module_as_main
rogpt1:     return _run_code(code, main_globals, None,                                                                
rogpt1:   File ""/opt/conda/lib/python3.10/runpy.py"", line 86, in _run_code       
rogpt1:     exec(code, run_globals)
rogpt1:   File ""/opt/conda/lib/python3.10/site-packages/deepspeed/launcher/launch.py"", line 355, in <module>
rogpt1:     main()
rogpt1:   File ""/opt/conda/lib/python3.10/site-packages/deepspeed/launcher/launch.py"", line 308, in main
rogpt1:     agent.run()
rogpt1:   File ""/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py"", line 129, in wrapper
rogpt1:     result = f(*args, **kwargs)
rogpt1:   File ""/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py"", line 723, in run
rogpt1:     result = self._invoke_run(role)
rogpt1:   File ""/opt/conda/lib/python3.10/site-packages/deepspeed/elasticity/elastic_agent.py"", line 126, in _invoke_run
rogpt1:     self._initialize_workers(self._worker_group)
rogpt1:   File ""/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py"", line 129, in wrapper
rogpt1:     result = f(*args, **kwargs)
rogpt1:   File ""/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py"", line 692, in _initialize_workers
rogpt1:     self._rendezvous(worker_group)
rogpt1:   File ""/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py"", line 129, in wrapper
rogpt1:     result = f(*args, **kwargs)
rogpt1:   File ""/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py"", line 556, in _rendezvous
rogpt1:     self._set_master_addr_port(
rogpt1: TypeError: DSElasticAgent._set_master_addr_port() takes 3 positional arguments but 4 were given
```

**The reason**
PyTorch will use four arguments to call function
[_set_master_addr_port()](https://github.com/pytorch/pytorch/blob/e732adf0a7f31159dc827e563106279bf969144a/torch/distributed/elastic/agent/server/api.py#L513)
but the ""_set_master_addr_port()"" of ""DSElasticAgent"" only implement
three arguments.

**The solution**
Add the last ""local_addr"" argument by following PyTorch. And, to avoid
changing the current behavior, set its default value to ""None"".

---------

Co-authored-by: Robin Dong <rdong@woolworths.com.au>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Logan Adams <loadams@microsoft.com>",['62afafe812db11c3e795b46883f914b782b124d9'],False,['elastic_agent.py']
66fb92413ca4515f37a4b219328cf3fca48825a3,"launcher/launcher_helper.py: fix PMI name and add EnvironmentError (#5025)

Hi, for my last PR https://github.com/microsoft/DeepSpeed/pull/4699
about launcher_helper, it mistakenly used two ""PMIX"". In this PR I
corrected them to be ""PMIX"" and ""PMI"". And I also added
_EnvironmentError_ to make sure env not get _NONE_ type, otherwise it
would trigger env setting error.",['6e1a6801d1be23571447be1f6d4d0e3921dd05a9'],False,['launcher_helper.py']
f5920457d3650e60a98e9d39b927e224e4203449,"Remove coverage reports from workflows and fix for inference CI (#5028)

Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>",['8c30e7c4db638b2532f5898b431d243408069757'],False,"['nv-inference.yml', 'nv-torch-latest-v100.yml', 'requirements-dev.txt', 'requirements-inf.txt', 'test_inference.py']"
db22e7a7e8af27d1cfee5ed89bbe646a65191eb7,"Remove Megatron-DeepSpeed CI workflow (#5038)

Megatron-DeepSpeed workflow has been broken for many months (since
https://github.com/microsoft/Megatron-DeepSpeed was rebased on main
Megatron repo). Removing this until we are able to update the this fork
to run the tests again.",['f5920457d3650e60a98e9d39b927e224e4203449'],False,"['nv-megatron.yml', 'README.md']"
6857061a71ba68b933f44a2986529cbd5452a110,"Fix P40 CI failures (#5037)

P40 tests:
https://github.com/microsoft/DeepSpeed/actions/runs/7718410286/job/21039607501",['db22e7a7e8af27d1cfee5ed89bbe646a65191eb7'],False,"['nv-torch110-p40.yml', 'cuda_accelerator.py', 'test_zero_tensor_fragment.py', 'util.py']"
f0e5bece6082d7922a24a0877adb6d522a2db7b7,Fix for nightly torch CI (#5039),['6857061a71ba68b933f44a2986529cbd5452a110'],False,['mics.py']
546d08ee6eb9b97fda2fe4437fb1be22c34e276e,Fix nv-accelerate and nv-torch-latest-v100. (#5035),['f0e5bece6082d7922a24a0877adb6d522a2db7b7'],False,"['nv-accelerate-v100.yml', 'nv-torch-latest-v100.yml']"
93e9537d4ccf0e54042ce98a910dcbc125bb8485,Fix nv-torch-latest-cpu CI (#5045),['8f6277001a6d1a2b1cddaf65863fbb37fd3fe7a8'],False,['cuda_accelerator.py']
971d82b57326ea272ba2060e74f75ab5ae7026a0,"MoE type hints (#5043)

This PR fixes 5 pyright errors in `deepspeed`. My main goal is to fix
the type signatures of
`split_params_into_different_moe_groups_for_optimizer` since this
affects my project's linting.

I made a few other improvements along the way:

* use more descriptive variable names (`param_group` instead of `v1`,
`moe_group` instead of `v`)
* remove a few unused variables by choosing better-suited iterators like
`dict.values()` instead of `dict.items()` or `nn.Module.parameters()`
instead of `nn.Module.named_parameters()`
* fix incorrect function type signatures
* [use simple `dict()` shallow copy instead of of unnecessary for loop
excluding a key is then immediately overwritten:
](https://github.com/microsoft/DeepSpeed/compare/master...ringohoffman:moe-type-hints?expand=1#diff-cec48b3c7def770ef2d14ac7398bfbdf0f209d2558645ffd47d0028988fa66a3L134-L138)
* [ternary to reduce duplicating long
expression](https://github.com/microsoft/DeepSpeed/compare/master...ringohoffman:moe-type-hints?expand=1#diff-cec48b3c7def770ef2d14ac7398bfbdf0f209d2558645ffd47d0028988fa66a3L101-L104)
* `isinstance()` instead of `type(...) is ...`
* `typing.cast(List[nn.Parameter], param_group['params'])` as a general
pattern for improved type hinting of its elements during iteration

---------

Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>",['88cca60afbe9fb89938f57b7e7086b859cd29309'],False,"['experts.py', 'utils.py', 'mixture-of-experts.md']"
449f9ad01ffab74df2d9c6fef83d6f198c908c86,"Fix broken model names in inference CI (#5053)

Some model names were updated on HF. Reflecting those changes here.",['76ec8b4927974431a4e5de476ecacafceda2abe4'],False,"['nv-inference.yml', 'test_inference.py']"
19e0dc39ba9448d95aa49884e76f857f3357ef8e,"Delay reduce-scatter for ZeRO3 leaf modules (#5008)

ZeRO3 sets hooks on parameters to run reduce-scatter. This is often
problematic for MoE models. Our data parallel processes may activate
different sets of experts, but the hook is not fired unless the expert
is activated at a forward pass. The reduce-scatter is called only on
some processes in this case.

This PR delays reduce-scatter for ZeRO3 leaf modules (Refer to #4966) to
address the issue.
We no longer set reduce-scatter hooks on parameters of the leaf modules.
Instead, we launch reduce-scatter on all parameters belonging to the
leaf module when exiting the module during the backward pass.

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['6de31de73fdf0a5e0f90c92e10cff4e72e91cf65'],False,"['parameter_offload.py', 'stage3.py', 'utils.py', '__init__.py', 'z3_leaf_module.py', 'test_zero_leaf_module.py']"
592325abde09a3ad4d5e3417706da39fba661193,"[Zero++ qgZ] Fall back to reduce_scatter if `tensor.numel() % (2 * global_world_size) != 0` (#5056)

## Why?

See https://github.com/microsoft/DeepSpeed/issues/5054. The actual rule
here is that qgZ doesn't work if `tensor.numel() % (2 *
global_world_size) != 0`. I will explain later. It may usually happen
when tensor size is odd or global_world_size is odd

## What?

1. Fall back to reduce scatter if tensor.numel() % (2 *
global_world_size) != 0 because all-to-all will have size mismatch
error.
2. Add logging when falling back to inform users qgZ is not taking
effect
3. Add a test for the fallback cases. non-fallback all-to-all cannot be
tested because we don't support multinode testing for now.

## Analysis?

In,
[all_to_all_quant_reduce](https://github.com/microsoft/DeepSpeed/blob/93e9537d4ccf0e54042ce98a910dcbc125bb8485/deepspeed/runtime/comm/coalesced_collectives.py#L31),

1. The initial tensor is of size (dim_1, dim_2, ... dim_n), and numel is
A (=dim_1*dim_2...*dim_n).

2. After swizzle_quant, `intra_quant_int4` is of size (A // 2) if A % 2
== 0. the tensor is quantized from `fp16/bf16` to `int4`. However,
`intra_quant_int4` is actually represented by `int8`, which means every
two int4 tensors is grouped into one and stored in `int8` format. Note
that if A % 2 != 0, the quantization can still process, but the size
differ by cases. I cannot find the underlying rule)

3. At `all_to_all_single(local_output, intra_quant_int4,
group=groups[f'local_{intra_idx}'])`, we should assert that
`intra_quant_int4 % local_world_size == 0`, which means (A // 2) %
`local_world_size` == 0.

4. At quantized_reduction, `intra_quant_int4` is chunked to
`local_world_size` pieces and reduce them together.

5. global_input_tensor is of size A // (2 * `local_world_size`) after
reduction

6. At `all_to_all_single(global_output, global_input_tensor,
group=groups[f'global_{inter_idx}'])`, we should assert that
`global_input_tensor % n_nodes == 0`, which means `( A // (2 *
local_world_size) ) % n_nodes == 0`

We can conclude that if `A % (2 * global_world_size) == 0`, then the
above step can run safely. Otherwise, unexpected things may happen (size
mismatch, cuda bad address, etc). Therefore, to be safe, we only use qgZ
if the condition satisfied.

@GuanhuaWang and me also discussed about adding paddings before
all-to-all, but it will have correctness issues and might involve cuda
level changes. Thus, the best solution now is to fallback.


## End-to-end test

1. Tested with dummy nn 3 nodes 2 gpus each (fallback)
Ended normally but with fallback warning in the middle

```
[1] LOSS: 3.6015625
[0] LOSS: 3.6015625
[2024-02-02 00:46:10,692] [WARNING] [coalesced_collectives.py:52:all_to_all_quant_reduce] gqZ falls back to reduce_scatter because tensor size = 1024 is not divisible by (2 * global_world_size) = 12. Please consider allocating a new world to enable gqZ
[2024-02-02 00:46:10,692] [WARNING] [coalesced_collectives.py:52:all_to_all_quant_reduce] gqZ falls back to reduce_scatter because tensor size = 1024 is not divisible by (2 * global_world_size) = 12. Please consider allocating a new world to enable gqZ
```

3. Tested with dummy nn 2 nodes 2 gpus each (non-fallback)

Ended normally and no warning.

```
[1] LOSS: 3.544921875
[0] LOSS: 3.58984375
[2024-02-02 00:37:05,388] [INFO] [logging.py:96:log_dist] [Rank 0] step=32, skipped=0, lr=[0.00015], mom=[(0.9, 0.999)]
[2024-02-02 00:37:05,388] [INFO] [timer.py:260:stop] epoch=0/micro_step=32/global_step=32, RunningAvgSamplesPerSec=53506.35861091424, CurrSamplesPerSec=54696.98861519354, MemAllocated=0.02GB, MaxMemAllocated=0.02GB
[1] LOSS: 3.431640625
[0] LOSS: 3.572265625
[2024-02-02 00:37:05,393] [INFO] [logging.py:96:log_dist] [Rank 0] step=33, skipped=0, lr=[0.00015], mom=[(0.9, 0.999)]
[2024-02-02 00:37:05,393] [INFO] [timer.py:260:stop] epoch=0/micro_step=33/global_step=33, RunningAvgSamplesPerSec=53531.621993677356, CurrSamplesPerSec=54300.77616234266, MemAllocated=0.02GB, MaxMemAllocated=0.02GB
```
4.  Tested with llama-7b 3 nodes 2 gpus each (fallback)

Ended normally but with fallback warning in the middle
```
size) = 12. Please consider allocating a new world to enable gqZ
[2024-02-02 00:52:04,819] [WARNING] [coalesced_collectives.py:52:all_to_all_quant_reduce] gqZ falls back to reduce_scatter because tensor size = 45088768 is not divisible by (2 * global_world_size) = 12. Please consider allocating a new world to enable gqZ
[2024-02-02 00:52:04,820] [WARNING] [coalesced_collectives.py:52:all_to_all_quant_reduce] gqZ falls back to reduce_scatter because tensor size = 45088768 is not divisible by (2 * global_world_size) = 12. Please consider allocating a new world to enable gqZ
```

6. Tested with llama-7b 2 nodes 2 gpus each (non-fallback)

Ended normally and no warning.
```
  0%|                                                                                                                  | 1/6241 [01:26<150:33:40, 86.86s/it]tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
{'loss': 2.4093, 'learning_rate': 0, 'epoch': 0.0, 'max_steps': 6241, 'global_step': 1, 'current_step_time_seconds': 88.56739020347595, 'average_step_time_seconds': 88.56739115715027, 'estimated_time_to_completion_seconds': 552660.5208206177, 'estimated_total_training_time_seconds': 552749.0882117748}
  0%|                                                                                                                  | 2/6241 [02:19<115:32:25, 66.67s/it]tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
{'loss': 2.6963, 'learning_rate': 0, 'epoch': 0.0, 'max_steps': 6241, 'global_step': 2, 'current_step_time_seconds': 52.532806396484375, 'average_step_time_seconds': 70.55009877681732, 'estimated_time_to_completion_seconds': 440162.06626856327, 'estimated_total_training_time_seconds': 440303.1664661169}
  0%|                                                               
```

## Unit test

TestAllToAllQuantReduceFallback

---------

Signed-off-by: byhsu <byhsu@linkedin.com>
Co-authored-by: byhsu <byhsu@linkedin.com>",['2eafe41be7049721b77c2f2b0ee702fea1702239'],False,"['coalesced_collectives.py', 'test_coalesced_collectives.py']"
9922270f47be0c349bd9ccce033d5ccdd22bd764,"Further refactor deepspeed.moe.utils + deepspeed.moe.layer type hints (#5060)

When unpacking a `dict`, keys that appear after the unpacking can
overwrite the keys of the unpacked `dict`, meaning we can avoid avoid
the pattern of skipping certain keys; also use `defaultdict` to avoid
having to do the boilerplate of assigning the elements of `group_moe`.

More type hints and small stylistic changes to `deepspeed.moe.layer`

---------

Co-authored-by: Michael Wyatt <mrwyattii@gmail.com>
Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>",['3e6d6069574c1721d24a300dd5f7149cf92fee48'],False,"['layer.py', 'utils.py']"
f02d7bdadf2cee23719d148339a88b24d668a149,"Fix verification for ZeRO3 leaf module (#5074)

This PR improves verification for ZeRO3 leaf module.

The leaf module requires input tensors with `requires_grad=True` to
launch reduce_scatter from backward hooks. Currently we throw an error
if any of input tensors to the leaf module does not *require grad*.
This prevents us from using leaf modules in some scenarios including
inference or activation checkpointing as reported in #5008

This PR addresses the issue by checking output tensors as well as input
tensors. the hook does not throw an error if no output tensor requires
grad.",['9922270f47be0c349bd9ccce033d5ccdd22bd764'],False,['stage3.py']
5a721de32c21082b81e3f6ae4d7d927fcfa7ce39,"Stop tracking backward chain of broadcast in initialization (#5075)

DeepSpeed engine generates the following warning upon initialization.
This warning is triggered by a broadcast that synchronizes model
parameters across ranks. Although this is harmless in terms of both
accuracy and, likely, performance, it may confuse users and potentially
cause compatibility issues with future versions of PyTorch.
This PR runs the broadcast within a `torch.no_grad` context to prevent
tracking of the backward computation chain.

```
/home/aiscuser/.conda/envs/wbcast/lib/python3.9/site-packages/torch/autograd/__init__.py:266: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987277512/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.) 

  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass 
```

Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>",['f02d7bdadf2cee23719d148339a88b24d668a149'],False,['engine.py']
889620b0a44be9622ebcdd65be5a809a2f3c310b,Update nv-torch-latest-version,['5a721de32c21082b81e3f6ae4d7d927fcfa7ce39'],False,['nv-torch-latest-cpu.yml']
55eb78ee1fbfe10950121cabc678edb32643a0b1,"Revert ""Update nv-torch-latest-version""

This reverts commit 889620b0a44be9622ebcdd65be5a809a2f3c310b.",['889620b0a44be9622ebcdd65be5a809a2f3c310b'],False,['nv-torch-latest-cpu.yml']
e469e7d98c26602ccebc1f3757ace8132cbb5247,"Update torch version for nv-torch-latest-cpu (#5086)

Given the name of this test, we should be running a newer version of
torch than we were, this updates to a newer version.

Total test coverage remains the same:
```
torch 1.12
========== 151 passed, 891 skipped, 20 warnings in 126.58s (0:02:06) ===========
========= 4 passed, 61 skipped, 5342 deselected, 2 warnings in 48.66s ==========

torch 2.2
========== 151 passed, 880 skipped, 20 warnings in 157.64s (0:02:37) ===========
========= 4 passed, 62 skipped, 5330 deselected, 2 warnings in 55.73s ==========
```",['55eb78ee1fbfe10950121cabc678edb32643a0b1'],False,['nv-torch-latest-cpu.yml']
c3cfe96bb3374ae6d8ff200e7487a1562de43e11,"Enable torch.compile with ZeRO (Experimental) (#4878)

This PR enables `torch.compile` with ZeRO stages 1/2/3. You need to add
`compile` section in your DeepSpeed config. The fields in the section
are passed to `torch.compile`.

```json
  ""compile"": {
    ""disable"": false,
    ""backend"": ""inductor""
  }
```

To enable a custom backend, you can pass the fully qualified name of the
backend function. For example, if you have a backend class `my_backend`
in `my_backend.py` in the current directory, you can enable it by
`""backend"": ""my_backend.my_backend""`. You can find an example in [a unit
test](https://github.com/microsoft/DeepSpeed/blob/eb9d4e06e9596f391aea305a6a5c6ec70cc28b58/tests/unit/runtime/compile/test_config.py#L116).

Currently we validated the results with Megatron-DeepSpeed. See the
[example](https://github.com/microsoft/Megatron-DeepSpeed/tree/tohtana/enable_compile/examples_deepspeed/compile)
for the details.

NOTICE: This PR is a draft. We will need to validate the coverage and
accuracy with many more examples.

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>",['e212845e396e9fb1decce09dfa2800e3488a1704'],False,"['__init__.py', 'torch.py', 'compiler.py', 'config.py', 'engine.py', 'mics.py', 'parameter_offload.py', 'partition_parameters.py', 'partitioned_param_coordinator.py', 'utils.py', 'common.py', 'test_compile_wrapper.py', 'test_compile_zero.py', 'test_load_config.py', 'util.py']"
b42a470615453ed23c406d362eba074e36eb8126,"HPU Accelerator: fix supported_dtypes API (#5094)

was returning bfloat16 instead of half",['ec49222cd8434fcfc3206fc8579aa8a6819cc6e4'],False,['hpu_accelerator.py']
4f477328c411270cf378a2318bc4f51c512ad2c8,"[NPU] replace 'cuda' with get_accelerator().device_name() (#5095)

Replace 'cuda' with `get_accelerator().device_name()` to support other
accelerators.

I searched the whole repo trying to fix same issue and this seems to be
the only one.",['b42a470615453ed23c406d362eba074e36eb8126'],False,['stage3.py']
688239e3f24f7ba11d3fe90bbe9670b7a61e5440,"[xs] fix ZEROPP convergence test  (#5061)

there may be chances where the dataset shard loaded contains example
with empty text `''` which will make the test fail (which occurred on my
end) so fixing by dropping the empty examples

---------

Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['961bc85624174e8ca8ee7626b3f3b53c6c768085'],False,['test_zeropp.py']
3255569b785f4561a0bce7913f7f00c1f6fde0c6,"Switch hasattr check from compile to compiler (#5096)

torch.compile introduced in torch 2.0 but torch.compiler was introduced
in torch 2.1, this fixes issues for those building with torch 2.0 like
the A6000 builds.

```
>>> torch.__version__
'2.1.0+cu121'
>>> hasattr(torch, ""compile"")
True
>>> hasattr(torch, ""compiler"")
True

>>> torch.__version__
'2.0.0+cu117'
>>> hasattr(torch, ""compile"")
True
>>> hasattr(torch, ""compiler"")
False
```",['688239e3f24f7ba11d3fe90bbe9670b7a61e5440'],False,['compiler.py']
697f945a05fd75eb3ef299309ee36a6283df2c98,"Split is_synchronized_device api to multiple apis (#5026)

Deepspeed currently calls is_synchronized_device() to decide how to use
the device.
HPU does not fit into this definition since it behaves like all streams
are blocking streams,
meaning they preserve order between each other but asynchronous to CPU. 
see cudaStreamCreateWithFlags. 

**has_data_dependency_resolving()**
HPU device is considered synchronized wrt CPU. Operations executed in
the script order
regardless of stream they were enqueued on. Tensor data is guaranteed to
be valid.
No need to stream dependencies or CPU synchronizations.

**use_host_timers()**
HPU device execution is async. To measure device execution time we must
use device timers.

**has_memory_backpressure()**
limiting number of inflight fetched params and number of inflight grad
reduce_scatter calls
is not necessary since HPU will stop enqueuing calls if memory is full,
creating internal
backpressure for the CPU until memory is available.

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['3255569b785f4561a0bce7913f7f00c1f6fde0c6'],False,"['abstract_accelerator.py', 'cpu_accelerator.py', 'cuda_accelerator.py', 'hpu_accelerator.py', 'npu_accelerator.py', 'xpu_accelerator.py', 'partitioned_param_coordinator.py', 'stage3.py', 'stage_1_and_2.py', 'timer.py']"
2518cc429d51b1371e63e1aeecd22fd92c9e89e4,"Support `exclude_frozen_parameters` for `zero_to_fp32.py` script (#4979)

Adds support for the `zero_to_fp32.py` script to merge only the
trainable parameters through a new argument `only_trainable_params`.
Fixes #3437

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['3c811c966bc2cbabdd4b097946d09c05b176beee'],False,['zero_to_fp32.py']
0a10bd427e035cbd185c2d44346996e8c1a0b42d,"Fix alignment of optimizer states when loading (#5105)

The ZeRO 1/2 optimizer pads optimizer states according to NCCL's
alignment. However, it does not account for NCCL's alignment when
loading from an elastic checkpoint, resulting in improperly restored
optimizer states. The existing test case only verifies parameter groups
and fails to catch this specific issue.

This PR addresses the misalignment and enhances the unit test to ensure
that optimizer state tensors are correctly matched post-restoration.",['2518cc429d51b1371e63e1aeecd22fd92c9e89e4'],False,"['stage_1_and_2.py', 'common.py', 'test_zero_optimizer.py']"
d04a8386d1823326ff2e01442823310a9f9f0c5b,"Skip Triton import for AMD (#5110)

When testing DeepSpeed inference on an `AMD Instinct MI250X/MI250` GPU,
the `pytorch-triton-rocm` module would break the `torch.cuda` device
API. To address this, importing `triton` is skipped when the GPU is
determined to be `AMD`.

This change allows DeepSpeed to be executed on an AMD GPU w/o kernel
injection in the DeepSpeedExamples [text-generation
example](https://github.com/microsoft/DeepSpeedExamples/tree/master/inference/huggingface/text-generation)
using the following command:
```bash
deepspeed --num_gpus 1 inference-test.py --model facebook/opt-125m
```

TODO: Root-cause the interaction between `pytorch-triton-rocm` and
DeepSpeed to understand why this is causing the `torch.cuda` device API
to break.",['0a10bd427e035cbd185c2d44346996e8c1a0b42d'],False,['__init__.py']
18179807f5fa5dbb93b7af51d51f9002e1539478,"Remove optimizer step on initialization (#5104)

All ZeRO 1/2/3 stages call the optimizer's `step()` on its
initialization. This increments a counter in the optimizer and produces
a different result in parameter update with the normal usage of PyTorch.
This PR eliminates `step()` in the initialization and lazily configures
some internal states (linking *hp_params*) after the first `step()`
call.

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['25a02047ae1726f2c67fba75d262e1e1afdfe306'],False,"['bf16_optimizer.py', 'stage3.py', 'stage_1_and_2.py', '__init__.py', 'mixed_precision_linkage.py', 'tensor_fragment.py', 'test_zero.py', 'test_zero_tensor_fragment.py']"
f295aea09eb38aace6ac2155a452bbba7df57aa3,"Stop tracking backward chain of broadcast (ZeRO3) (#5113)

The broadcast that ZeRO3 uses on initialization displays a warning shown
below. This PR avoids this by passing `.data` to the broadcast.

The same issue of ZeRO 1/2 was addressed in #5075 using `torch.no_grad`,
which affects multiple lines in the scope. This PR also changes the fix
for ZeRO 1/2 to passing `.data` to broadcast for consistency and safety.
```
/home/mtanaka/.conda/envs/tcomp/lib/python3.9/site-packages/torch/autograd/graph.py:681: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/conda/conda-bld/pytorch_1704786093577/work/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
```",['41bc9feed00d5893803a66d1657e684fde67b795'],False,"['engine.py', 'partition_parameters.py']"
d67d4e5b2f178822689f883f01075b8c916aba84,"disable compile tests for torch<2.1 (#5121)

Tests running older version of torch will fail the compile tests added
in #4878.",['9aaead552087c539a2eef178c2e6eeb399d47718'],False,"['test_compile_wrapper.py', 'test_compile_zero.py', 'test_load_config.py']"
d532f643654043cdd168db0c57f918f90ac8b805,Update inference test model names (#5127),['d67d4e5b2f178822689f883f01075b8c916aba84'],False,['test_inference.py']
9c69662032c381edede22446a3b0bef57febef14,"Fix issue with zero-sized file after merging file on curriculum `map_reduce` (#5106)

In `deepspeed/runtime/data_pipeline/data_sampling/indexed_dataset.py`
when calling `merge_file_` , the following operation may not flush the
merged file in time, before it's needed:

```
        # Concatenate data
        with open(data_file_path(another_file), 'rb') as f:
            shutil.copyfileobj(f, self._data_file)
```

this leads to `self._data_file` having size zero, and later to the
following error (with stack trace):
```
  File ""~/my_code/deepspeed_trainer.py"", line 999, in my_func
    data_analyzer.run_reduce()
  File ""~/my_env/lib/python3.11/site-packages/deepspeed/runtime/data_pipeline/data_sampling/data_analyzer.py"", line 413, in run_reduce
    self.merge_map_results(self.dataset, self.metric_names, self.metric_types, self.save_path,
  File ""~/my_env/lib/python3.11/site-packages/deepspeed/runtime/data_pipeline/data_sampling/data_analyzer.py"", line 371, in merge_map_results
    index_to_sample = MMapIndexedDataset(index_to_sample_fname, skip_warmup=True)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""~/my_env/lib/python3.11/site-packages/deepspeed/runtime/data_pipeline/data_sampling/indexed_dataset.py"", line 486, in __init__
    self._do_init(path, skip_warmup)
  File ""~/my_env/lib/python3.11/site-packages/deepspeed/runtime/data_pipeline/data_sampling/indexed_dataset.py"", line 502, in _do_init
    self._bin_buffer_mmap = np.memmap(data_file_path(self._path), mode='r', order='C')
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""~/my_env/lib/python3.11/site-packages/numpy/core/memmap.py"", line 268, in __new__
    mm = mmap.mmap(fid.fileno(), bytes, access=acc, offset=start)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: cannot mmap an empty file
```

This PR fixes that issue by forcing the destination file to be flushed
and adding an assert to make sure the concatenation succeeded.

deepspeed version: '0.13.2'

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['d532f643654043cdd168db0c57f918f90ac8b805'],False,"['data_analyzer.py', 'indexed_dataset.py']"
083197ea1be39cf6781da4f8005eacc2ee40819e,"Update return codes in PyTest to properly error out if tests fail (#5122)

Update PyTest return codes to better reflect the [documented exit
codes](https://docs.pytest.org/en/7.1.x/reference/exit-codes.html). This
was causing issues in the nv-inference test.

Build with nv-inference with just the returncode changes, showing that
as [now
failing](https://github.com/microsoft/DeepSpeed/actions/runs/7880331865/job/21502182220)
with CUDA OOM errors.

Sample previous passing build
[here](https://github.com/microsoft/DeepSpeed/actions/runs/7719051411/job/21041568991).",['9c69662032c381edede22446a3b0bef57febef14'],False,"['nv-inference.yml', 'common.py']"
b20c46745b3d018cc9417818c9b26bd981163af1,"add missing methods to MPS_Accelerator (#5134)

#5026 introduced new abstract methods for the base accelerator class.
These methods were not defined for `MPS_Accelerator`. Fixes #5132",['083197ea1be39cf6781da4f8005eacc2ee40819e'],False,"['mps_accelerator.py', 'test_accelerator.py']"
3e5d4004732bd2fd91a44c2e8ec5a4e95da6f7e2,"Solve tensor vs numpy dtype conflicts in data efficiency map-reduce. (#5108)

The map-reduce types are a mess. By looking at the file
`deepspeed/runtime/data_pipeline/data_sampling/indexed_dataset.py`, we
see that the reduce only accepts numpy types due to the following check:

```
dtypes = {
    1: np.uint8,
    2: np.int8,
    3: np.int16,
    4: np.int32,
    5: np.int64,
    6: np.float64,
    7: np.double,
    8: np.uint16,
    9: np.uint32,
    10: np.uint64
}


def code(dtype):
    for k in dtypes.keys():
        if dtypes[k] == dtype:
            return k
    raise ValueError(dtype)
```

Now the issue is that python and torch types are not equal (in python)
for the same representation:
```
> type(int) == type(np.int64)
True

> type(torch.int64) == type(np.int64)
False
```

And the user-specified `metric_function` needs to return a tensor, so it
will have automatically have a torch type. If the user does not specify
a tensor, then this fails:

In `deepspeed/runtime/data_pipeline/data_sampling/data_analyzer.py`:
```
def update_metric_results(self, data, metric_types, metric_dtypes, metric_functions, metric_results):
        for m_idx in range(len(metric_types)):
            [...]
            if metric_type == 'single_value_per_sample':
                metric_values = metric_function(data)
                for row in range(metric_values.size()[0]):
```
Only a `torch.Tensor` has the `.size()` attribute:
```
> np.array([1,2,3]).size()
TypeError: 'int' object is not callable

> torch.tensor([1,2,3]).size()
torch.Size([3])
```

So to my understanding: the user must create a `DataAnalyser` with a
`metric_dtypes` which is of a numpy dtype, yet needs to provide a
`metric_function` function that returns a torch dtype that **must match
the same data type as numpy**, e.g.

```
            def metric_functions(int_list):
                return torch.tensor(int_list).as(torch.int64).   #<-- TORCH type required here

            data_analyzer = DataAnalyzer(
                dataset=train_dataset,
                metric_names=[""seqlen""],
                metric_functions=[metric_functions],
                metric_types=['single_value_per_sample'],
                metric_dtypes=[np.int64],   ### <--- NUMPY type required here
                )
```

Finally there's no datatype check, so if a user forgets to add
`.as(torch.int64)` to the `metric_functions`, then the files output by
threads will be called e.g.
`seqlen/worker0_thread0/seqlen_metric_to_sample_730.0.csv` as the
integer `730` is defaulted to `float`. This would later fail as the
reduce step would look for
`seqlen/worker0_thread0/seqlen_metric_to_sample_730.csv` instead.

This PR adds support to both `np.ndarray` and `torch.tensor` return
dtypes on function `metric_function`. When dealing with tensors, it
converts to the corresponding numpy dtype before outputting. It also
adds several `asserts` to make sure use provides the correct return type
and dtype on `metric_function` and `metric_dtype`, respectively.

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['b20c46745b3d018cc9417818c9b26bd981163af1'],False,"['data_analyzer.py', 'indexed_dataset.py']"
2b411103adc080bc72f7a49b33ededea6bb487f8,"Fix broadcast deadlock for incomplete batches in data sample for data analysis  (#5117)

When the batch is not a full batch (`drop_last=False`), then the size of
the current batch is smaller than the expected:
```
self.global_batch_size = self.micro_batch_times_data_parallel_size * self.gradient_accumulation_steps
```

The `get_next_global_batch()` method will try to broadcast the tensor of
a size smaller than `self.global_batch_size` from a master rank (`0`).
However, in this case, the master rank will send a shorter tensor. This
leads to an unexpected behaviour (deadlock, crash, or `None` tensor on
receiving ranks). The documentation for the
[broadcast](https://pytorch.org/docs/stable/distributed.html#torch.distributed.broadcast)
operation says ""tensor must have the same number of elements in all
processes participating in the collective."" In the following call,
`tensor` can have different sizes when comparing master with other
participant ranks. File
`deepspeed/runtime/data_pipeline/data_sampling/data_sampler.py`, like
`289`:
```
dist.broadcast(batch, 0, group=self.data_parallel_group)
```

This PR fixes that bug, by filling incomplete batch indices with `-1` so
that the batch tensor is always of the same size.

Note: an alternative resolution is to broadcast beforehand the size of
the batches tensor, but adds an extra comm step. The current method of
extending the `batch` tensor with `-1`s is memory-safe as the batch
tensor will match the one used in previous iterations with a full batch.",['3e5d4004732bd2fd91a44c2e8ec5a4e95da6f7e2'],False,['data_sampler.py']
a7864846a4c7c9c7c0a4ab463293d595767bf4b8,"Avoid zero-sized microbatches for incomplete minibatches when doing curriculum learning (#5118)

Related to curriculum learning and the data efficiency module.

The `get_start_end_idx()` method that is used to compute which batch
indices to allocate across data parallel ranks, assumes the batch to be
of size `micro-batch size * data_parallel_size` and allocates sequential
subsets of indices across data loader processes.

When `drop_last=False`, then the global batch size will very likely be
smaller than `micro-batch size * data_parallel_size`, and
`get_start_end_idx()` will give a full `self.microbatch_size` sized
batch to a few initial nodes and the remaining ones will have a
zero-sized microbatch. This leads to load imbalance and (probably) wrong
updates as gradients are averaged across different microbatch sizes.

This PR fixes that by distributing the same amount (+-1 sample) across
all data loader ranks, when batch is not complete.

---------

Co-authored-by: Conglong Li <conglong.li@gmail.com>",['2b411103adc080bc72f7a49b33ededea6bb487f8'],False,['data_sampler.py']
fac1df9983510ac7a04799b2829bf6e1cea08bd8,"tensorboard logging: avoid item() outside gas to improve performance (#5135)

item() causes device/host synchronization

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['2d0a6bc20ae20c37085ab5bb22db2f0e50725400'],False,['engine.py']
1c9e5ef2d214b0dac0a66e17df20ef6d0c6092a9,"Check overflow on device without host synchronization for each tensor (#5115)

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['fac1df9983510ac7a04799b2829bf6e1cea08bd8'],False,['stage_1_and_2.py']
95be499a6801f7e05a962aa19a03a9e1542b9021,"Method `run_map_reduce` to fix errors when running `run_map` followed by `run_reduce` (#5131)

In the map-reduce in data analysis, the run_reduce will merge several
files into one. There are two open issues:

- when running `run_map` followed by `run_reduce`, the `run_reduce` may
start before all nodes finished the `run_map`, leading to having nodes
loading files that are not populated/flush (zero-sized error)
- when running `run_reduce`, all nodes are loading the partial result
files output by all nodes, and all nodes will write the same files that
result from the merge. This leads to strange IO errors. `run_reduce`
should only be run by one node, and all nodes should wait for
`run_reduce` to finish before they feed the dataset to
`deepspeed.initialize()`.

This PR fixes both these issues when running `run_map` followed by
`run_reduce`, by providing a method `run_map_reduce` that fixes this
logic. It adds `dist.barrier`s to both steps (where barrier runs on an
user-specifed `comm_group`), and non-master running the reduce
operation.

**NOTE:** an alternative workaround is by not providing the
`run_map_reduce` method in this PR but having the correct barriers and
safeguards added to `run_map` and `run_reduce` as in
[5130](https://github.com/microsoft/DeepSpeed/pull/5130).

---------

Co-authored-by: Conglong Li <conglong.li@gmail.com>",['39f6ee59769fd5b1d9bfc4a3daedd63195ee73f6'],False,['data_analyzer.py']
a3be0d441ac5ac6df29a650f5ee7ca7c0d1d337e,"TestEmptyParameterGroup: replace fusedAdam with torch.optim.AdamW (#5139)

to avoid cuda specific optimizer, with general one.

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['9bd62e0b2675eae64dedc625d73dd16fa93254c2'],False,['test_zero.py']
177dc14331a64e61f6dcce2c4b8071576bcb22db,"Fix UserWarning: The torch.cuda.*DtypeTensor constructors are no long… (#5018)

> …er recommended. It's best to use methods such as torch.tensor(data,
dtype=*, device='cuda') to create tensors.

By currying the returned tensor properties in CUDA_Accelerator, the
above warning in PyTorch is avoided.

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['a3be0d441ac5ac6df29a650f5ee7ca7c0d1d337e'],False,['cuda_accelerator.py']
fa3662f7f5e7c5633dc85caf46010c9cefcdb18e,"Pin to PyTest 8.0.0 (#5163)

Fix failures in nv-accelerate-v100 unit tests.

Fix running on this PR:
https://github.com/microsoft/DeepSpeed/actions/runs/7923894998/job/21634533848
Original failure
[here](https://github.com/microsoft/DeepSpeed/actions/runs/7961285826/job/21748664124?pr=5129#step:7:415):
```
    from _pytest.doctest import (
E   ImportError: cannot import name 'import_path' from '_pytest.doctest' (/tmp/actions-runner/_work/DeepSpeed/DeepSpeed/unit-test-venv/lib/python3.8/site-packages/_pytest/doctest.py)
```",['a37e59b590a42d425ef962640762c99ced17b752'],False,['requirements-dev.txt']
f062a1b24d3c20178ce3d2abd69999340d8ec670,"get_grad_norm_direct: fix a case of empty norm group (#5148)

fix for [#5145 ](https://github.com/microsoft/DeepSpeed/issues/5145)
empty norm group create a norm tensor with shape=[1], while other norms
will be shapeless. torch.stack does not support such case. Fixing empty
group norm to be shapless as well, instead of shape=[1].

---------

Co-authored-by: Lev Kurilenko <113481193+lekurile@users.noreply.github.com>
Co-authored-by: Lev Kurilenko <lekurile@microsoft.com>",['fa3662f7f5e7c5633dc85caf46010c9cefcdb18e'],False,['stage_1_and_2.py']
a84d07c51ea9d984c9aa324240983b6ab5ffadb3,"MOE: Fix save checkpoint when TP > 1 (#5157)

When using MOE, currently, only mp_rank_00_model_states.pt is saved.
This fails when using TP > 1.
Fix it by saving all required mp_rank_xx_model_states.pt files.

Signed-off-by: Moshe Island <misland@habana.ai>
Co-authored-by: Moshe Island <misland@habana.ai>",['7f0950f819c0482df793dcf7d8f3ffd7b7969241'],False,['engine.py']
005afe124f56b2243785067a898134fa2bf8735c,"Fix gradient clipping (#5150)

The gradient clipping API doesn't apply the coefficient correctly. This
PR resolves the issue and adds a test case.

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['a84d07c51ea9d984c9aa324240983b6ab5ffadb3'],False,"['utils.py', 'test_runtime_utils.py']"
b00533e47975597e88bb5ea14b3a315a04728add,"Use ninja to speed up build (#5088)

Deepspeed have too many ops now, and it take too many time to pre-build
all ops.
I notice deepspeed disabled `ninja` 4 years ago
(https://github.com/microsoft/DeepSpeed/pull/298) and I think we should
consider enable it now.
The issue mentioned in https://github.com/microsoft/DeepSpeed/pull/298
can be solved by resolving `include_dirs` to absolute path.

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Logan Adams <loadams@microsoft.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>",['005afe124f56b2243785067a898134fa2bf8735c'],False,"['bias_activation_cuda.cu', 'layer_norm_cuda.cu', 'rms_norm_cuda.cu', 'gated_activation_kernels_cuda.cu', 'embed_cuda.cu', 'blocked_kv_rotary_cuda.cu', 'logits_gather_cuda.cu', 'moe_gather_cuda.cu', 'moe_scatter_cuda.cu', 'top_k_gating_cuda.cu', 'builder.py', 'builder.py', 'builder.py', 'inference_core_ops.py', 'ragged_ops.py', 'builder.py', 'setup.py']"
98c96e790b4e8c42afb7c759bfda1992fc6be23b,"Update flops profiler to handle attn and __matmul__  (#4724)

Fixes #4723

- handle `F.scaled_dot_product_attention` in transformer models. 
- handle expreesions like `a@b`

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['b00533e47975597e88bb5ea14b3a315a04728add'],False,['profiler.py']
dd3690c5348ddfa475cef112eb01aebe6a409847,"Fix allreduce for BF16 and ZeRO0 (#5170)

This PR fixes an issue with allreducing for ZeRO0 + BF16. (This replaces
#5154)

DeepSpeed uses `BF16_Optimizer` when ZeRO0 and BF16 are enabled. The
optimizer accumulates gradients on FP32 buffer soon after a backward
pass completes. However, DeepSpeed engine performs allreduce on BF16
gradients.

This PR fixes the issue by performing allreduce on the FP32 buffer. It
also eliminates an assertion that prohibits BF16+PP+Z1, which is
actually runnable.

This shows loss curves of the following conditions:
- BF16/Z0,Z1,Z2,Z3/NoPP
- BF16/Z0,Z1/PP(2 stages)
(all used 8GPUs, gradient accumulation step: 4)

![image](https://github.com/microsoft/DeepSpeed/assets/81312776/0dc1e9ef-43bc-4b47-8b9e-d6aca137a217)

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['98c96e790b4e8c42afb7c759bfda1992fc6be23b'],False,['engine.py']
d5fa87ff3f9ca181856258d437c2f2ef4cf9efc3,"Write multiple items to output file at once, in distributed data analyzer. (#5169)

Minor improvements of
[https://github.com/microsoft/DeepSpeed/pull/5129](https://github.com/microsoft/DeepSpeed/pull/5129).
- Writes all buffers at once to the output file, instead of iteratively
(`indexed_dataset.py`, method `add_items()`).
- Fixes the wrong initialisation of `num_workers` and `worker_id` that
were being ignored when they were provided by the user.

---------

Co-authored-by: Conglong Li <conglong.li@gmail.com>",['dd3690c5348ddfa475cef112eb01aebe6a409847'],False,"['data_analyzer.py', 'indexed_dataset.py']"
8d0150d9173c72eecb5ab0ab3356caedfb2388f7,"Fix typos in blogs/ (#5172)

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['d5fa87ff3f9ca181856258d437c2f2ef4cf9efc3'],False,"['README.md', 'README.md', 'README.md']"
2a6c58df34ea04e0aa451a4623b7c7d49b087a50,"Inference V2 Human Eval (#4804)

This PR adds a Human Eval CI workflow and associated unit test for
Inference V2.

---------

Co-authored-by: Arash Bakhtiari <arash@bakhtiari.org>
Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>
Co-authored-by: Masahiro Tanaka <mtanaka@microsoft.com>
Co-authored-by: Masahiro Tanaka <81312776+tohtana@users.noreply.github.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['8d0150d9173c72eecb5ab0ab3356caedfb2388f7'],False,"['nv-human-eval.yml', 'pytest.ini', 'test_human_eval.py']"
4ec8762b22509710c5cda5fd26287f6d1ff99830,"Reduce ds_id name length (#5176)

Fixing issue #5087 . Limited the naming of the ds_id in ZeRO 3 to the
first and last parameters of the group instead of every parameter in the
group.",['2a6c58df34ea04e0aa451a4623b7c7d49b087a50'],False,['stage3.py']
daa5a05a4753434e0551b8a125d1ffd3fc2666cf,"Update version.txt after v0.13.3 release (#5185)

The auto PR creation for this in the release flow is broken - this is
the manual intervention of that.",['afdf028a9ae2d0b3c36fda53533e415d489a39db'],False,['version.txt']
5d75460640aa8f8a67baadaea23783ec708e8e1b,Fixes for `--extra-index-url` (#5183),['daa5a05a4753434e0551b8a125d1ffd3fc2666cf'],False,"['nv-inference.yml', 'nv-nightly.yml', 'nv-torch-latest-cpu.yml']"
48749411b864da40d6de5d076f2db2f63de7106f,"Disable ninja by default (#5194)

#5192 reports an issue with the latest DeepSpeed release (0.13.3)
related to pre-compilation and the recently re-enabled `ninja` support
in #5088. Reverting to disabling `ninja` by default, but users can still
enable it with `DS_ENABLE_NINJA=1` until we can further debug to
understand the problem.",['d274ebf3474503f56c707e6379245356dfbd4699'],False,"['nv-pre-compile-ops.yml', 'setup.py']"
aed599b4422b1cdf7397abb05a58c3726523a333,"Fix assertion to run pipeline engine with a compiled module (#5197)

This PR fixes assertion in the pipeline engine to compute a compile
module with pipeline parallelism.",['07a1ede4d51dc9151e5de72d88ac669cd610459f'],False,['engine.py']
4578c2490b086ef786bfdcf3755074a1259af2c6,"[zero++] Synchronize at the end of secondary partitioning and simplify the logic (#5216)

## 1. Why?

We have a very long thread investigating [the
issue](https://github.com/microsoft/DeepSpeed/issues/5059). To
summarize, this is because

a. The 2nd partitioning is asynchronous because it copies
device-to-device from full tensor to 2nd tensor
b. When using prefetching, the all-gather of 2nd tensor can happen
before 2nd partitioning ends. At that moment, the value of 2nd tensor
might contain bad values.


![image](https://github.com/microsoft/DeepSpeed/assets/24364830/ad6ee6a2-8e1e-4214-a0d2-ee5314b252b8)

Also, we found that the logic of copying is wrong and lengthy, so we
simplified it to only two lines.

Kudos to @yundai424, Haowen Ning, @samadejacobs for the investigation
effort.

## 2. What? 

After multiple careful tests, we found patching
`get_accelerator().synchronize()` to ensure all cuda stream finished
before 2nd partitioning can prevent the issue

## 3. Tests

I validated the correctness of the simplification of 2nd partition
logic. The loss is ""exactly"" the same before and after simplification
under the same random seed.

Before

```
[
  {""loss"": 2.0731},
  {""loss"": 2.0288},
  {""loss"": 1.927},
  {""loss"": 1.8347},
  {""loss"": 1.8347},
  {""loss"": 1.7896},
  {""loss"": 1.602},
  {""loss"": 1.766},
  {""loss"": 1.8751},
  {""loss"": 1.6776}
]

```

After

```
[
  {""loss"": 2.0731},
  {""loss"": 2.0288},
  {""loss"": 1.927},
  {""loss"": 1.8347},
  {""loss"": 1.8347},
  {""loss"": 1.7896},
  {""loss"": 1.602},
  {""loss"": 1.766},
  {""loss"": 1.8751},
  {""loss"": 1.6776}
]


```

## 4. TODO

We need further investigation on the issue @samadejacobs 
1) Revisit ZeRO-3 prefetch design 
2) Refactor hpz to reuse primary tensor for secondary partition.

---------

Signed-off-by: byhsu <byhsu@linkedin.com>
Co-authored-by: byhsu <byhsu@linkedin.com>",['857584fd0c8865efaa65285ff4772b3e6142f6af'],False,['partition_parameters.py']
3e06a154b4d6a2f93c2c5504f52a932184def112,"Rename nv-torch-latest-cpu workflow to cpu-torch-latest (#5226)

Rename nv-torch-latest-cpu workflow to remove the nv- prefix as it does
not run on nvidia hardware.

FYI @delock",['bcc617a0009dd27b4e144de59979bd7770eaf57c'],False,"['cpu-torch-latest.yml', 'README.md']"
e6e8c1378de035df59034d09373b44af3319b6d7,"Fix moe cpu offload (#5220)

The MoE- param gradients norms don't need to be averaged when created on
CPU only when using 1-DP training. However, I just moved the tensor back
to GPU to get average when having data-parallel on the MoE parameters
and using CPU-offload.

This PR addresses https://github.com/microsoft/DeepSpeed/issues/5203

---------

Co-authored-by: Reza Yazdani <reza.yazdani@snowflake.com>",['3e06a154b4d6a2f93c2c5504f52a932184def112'],False,['stage_1_and_2.py']
bc0d24651d7d40d5b78ff5a2f1702c7169852433,"fix fused_qkv model accuracy issue (#5217)

Fused_qkv model can not correctly choose the fused_qkv type. Need to
update the module_name_matches.

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['acf07398b7887f70ab5ffcff2733747f8440b276'],False,['fusedqkv_utils.py']
5a2e705b888bbcb41beef32b7f58a0e9010d287a,"MOE gate fixes and enhancements (#5156)

Fixes the following issues:
- Fix capacity when using TP for non-MoE by aligning the capacity to TP
- Fix TopKGate.wg (gate weight) when using ZeRO with fp16 or bf16
- Fix top2 aux loss to be similar to top1 aux loss

Following are few configurable enhancements:
- Support top2 with disable token dropping
- Support disable top2 2nd expert sampling

---------

Signed-off-by: Moshe Island <misland@habana.ai>
Co-authored-by: Moshe Island <misland@habana.ai>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['db70c183d8780e429886e6bb2fe06133e70f63be'],False,"['layer.py', 'sharded_moe.py']"
ccfdb84e2a4a373ac657a99afd2d97e1d741b22b,"FP6 quantization end-to-end. (#5234)

The user interface: https://github.com/microsoft/DeepSpeed-MII/pull/433
nv-a6000 ci running against the MII branch linked above is
[here](https://github.com/microsoft/DeepSpeed/actions/runs/8192124606)

Co-authored-by: Zhen Zheng
[zhengzhen@microsoft.com](mailto:zhengzhen@microsoft.com)
Co-authored-by: Shiyang Chen [csycfl@gmail.com](mailto:csycfl@gmail.com)
Co-authored-by: Arash Bakhtiari
[abakhtiari@microsoft.com](mailto:abakhtiari@microsoft.com)
Co-authored-by: Haojun Xia
[xhjustc@mail.ustc.edu.cn](mailto:xhjustc@mail.ustc.edu.cn)

---------

Co-authored-by: ZHENG, Zhen <zhengzhen.z@qq.com>
Co-authored-by: Shiyang Chen <csycfl@gmail.com>
Co-authored-by: Haojun Xia <xhjustc@mail.ustc.edu.cn>
Co-authored-by: Arash Bakhtiari <arash@bakhtiari.org>
Co-authored-by: Michael Wyatt <mrwyattii@gmail.com>
Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>",['5a2e705b888bbcb41beef32b7f58a0e9010d287a'],False,"['nv-a6000.yml', 'config_v2.py', '__init__.py', 'core_ops.cpp', '__init__.py', 'cuda_linear.py', 'cuda_linear_kernels.cpp', 'cuda_linear_kernels.h', 'fp6_linear.cu', 'fp6_linear.cuh', 'configs.h', 'kernel_matmul.cuh', 'kernel_reduction.cuh', 'ptx_cp.async.cuh', 'ptx_mma.cuh', 'utils_core.cuh', 'utils_gmem.cuh', 'utils_paralleldequant.cuh', 'weight_prepacking.h', 'gated_activation_kernels_cuda.cu', 'flat_model_helpers.py', 'heuristics.py', '__init__.py', 'quantized_linear.py', 'inference_core_ops.py', 'requirements-inf.txt', 'test_quantized_linear_module.py', 'test_manager_configs.py']"
c08e69f21238f15bfe0e3779170fefa2f75d4c7e,"Make op builder detection adapt to accelerator change (#5206)

This is an WIP PR that make op builder detection adapt to accelerator
change. This is followup of
https://github.com/microsoft/DeepSpeed/issues/5173
Currently, DeepSpeed generate `installed_ops` and `compatible_ops` at
setup time. If the system change to a different accelerator at DeepSpeed
launch time, these two list would contain incorrect information.

This PR intend to solve this problem with more flexity ops detection.

* For `installed_ops`, DeepSpeed should disable all installed ops if
accelerator detected at setup time is different from launch time.
* For `compatible_ops`, DeepSpeed should refresh the list for each
launch to avoid impact of accelerator change.

In the first step, nv-inference workflow is temporary change to emulate
the scenario that the system is setup with CPU_Accelerator, then launch
with CUDA_Accelerator. And CPU_Accelerator is modified to make Intel
Extension for PyTorch and oneCCL binding for PyTorch not mandatory.

Starting from here we can reconstruct installed_ops and compatible_ops
to follow the design above.

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['535a908f1b60f819df4ccf1071f7c917c39dabbe'],False,"['cpu-inference.yml', 'cpu-torch-latest.yml', 'nv-inference.yml', 'nv-pre-compile-ops.yml', 'cpu_accelerator.py', 'real_accelerator.py', 'env_report.py', 'git_version_info.py', '__init__.py', 'partition_parameters.py', 'all_ops.py', 'builder.py', 'builder.py', 'setup.py', 'common.py', 'test_latest_checkpoint.py', 'test_lr_scheduler.py', 'test_moe_checkpoint.py', 'test_other_optimizer.py', 'test_pipeline.py', 'test_zero_optimizer.py', 'common.py', 'test_dequantization.py', 'test_elastic.py', 'test_user_args.py', 'multi_output_model.py', 'test_accelerator_backward.py', 'test_activation_checkpointing.py', 'test_coalesced_collectives.py', 'test_compile_wrapper.py', 'test_compile_zero.py', 'test_load_config.py', 'test_onebit.py', 'test_bf16.py', 'test_dynamic_loss_scale.py', 'test_fp16.py', 'test_data_efficiency.py', 'test_ds_config_dict.py', 'test_ds_initialize.py', 'test_multi_output_model.py', 'test_mup_optimizers.py', 'test_pld.py', 'test_ignore_unused_parameters.py', 'test_zero.py', 'test_zero_context.py', 'test_zero_context_return.py', 'test_zero_leaf_module.py', 'test_zero_tensor_fragment.py', 'simple_model.py']"
49359d0bc7560e3f0b256bdb66d7bbc322e47295,"Replace HIP_PLATFORM_HCC with HIP_PLATFORM_AMD (#5264)

This PR resolves a part of
https://github.com/microsoft/DeepSpeed/issues/4989 issues.

HIP_PLATFORM_AMD is defined if the HIP platform targets AMD. Note,
HIP_PLATFORM_HCC was previously defined if the HIP platform targeted
AMD, it is deprecated.
Reference:
https://rocm.docs.amd.com/projects/HIP/en/latest/user_guide/hip_porting_guide.html#identifying-hip-target-platform",['c08e69f21238f15bfe0e3779170fefa2f75d4c7e'],False,"['blas_utils.h', 'ragged_ops.cpp']"
d9e12d3a68d16f87ff1396589be6b0ab5cbf10ff,"Fix attention mask handling in the Hybrid Engine Bloom flow (#5101)

The Bloom flow in Hybrid Engine applies the same transformation of the
input mask which is already performed earlier by the transformers
BloomModel::forward.

This results in the non-convergence of scores, specifically in Deepspeed
Chat on different accelerators, including CUDA and HPU.

The fix removes redundant mask transformation and application, producing
correct convergence.

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Lev Kurilenko <113481193+lekurile@users.noreply.github.com>",['2989cf7707e82d81b3663150916177fa8a9007c8'],False,"['bloom.py', 'config.py', 'ds_attention.py']"
d2ef20e75916fe91f75d4058856bd67490753818,"Skip 1Bit Compression and sparsegrad tests for HPU. (#5270)

HPU accelrator does not support 1bit and sparsegrad tests.",['d9e12d3a68d16f87ff1396589be6b0ab5cbf10ff'],False,"['test_onebit.py', 'test_averaging_sparse_gradients.py', 'test_sparse_grads.py']"
cf58c535df64b8e160c1a32862a7a7043ebdb558,"Enabled LMCorrectness inference tests on HPU. (#5271)

Since lm_eval API(v0.3.0) does not currently support HPU accelerator, so
to run LMCorrectness tests on HPU, get lm_eval model with CPU and move
it to HPU accelerator.",['d2ef20e75916fe91f75d4058856bd67490753818'],False,['test_inference.py']
a6fb4d3e237627dc2ab399f39294b051ae454e96,"Added HPU backend support for torch.compile tests. (#5269)

Added HPU backend in torch.compile tests. HPU uses hpu_backend for
torch.compile.",['cf58c535df64b8e160c1a32862a7a7043ebdb558'],False,"['test_compile_wrapper.py', 'test_compile_zero.py', 'test_load_config.py']"
b112c99ea8e09eb06ada0d60a3687983cb8c4bd0,"Fix loading a universal checkpoint (#5263)

This PR fixes the following two points regarding checkpoint loading.

- Load optimizer states
With [this PR](https://github.com/microsoft/DeepSpeed/pull/5104), we
removed optimizer's `step()` on initialization. This made the DS's
parameter update match with PyTorch's normal behavior. However, we don't
have keys in optimizer states any more when we load a checkpoint.
For legacy/elastic checkpoints, the PR changed the checkpoint loaders to
create keys and buffers on loading. However, the loader for universal
checkpoints still relies on keys in optimizer states. As the result,
loading a universal checkpoint fails.
This PR fixes the loader to find optimizer state keys from a given
checkpoint.

- Resume step count
https://github.com/microsoft/DeepSpeed/pull/5263/commits/2943e6ab7e156946a018ab2a08c7f3ba45b55e01
The checkpoint loader for a universal checkpoint resumes step count for
optimizer only when the param group already has `step`. But some
optimizers creates the key `step` in a param group at the first call of
`step()` (e.g. Apex [Fused
Adam](https://github.com/NVIDIA/apex/blob/810ffae374a2b9cb4b5c5e28eaeca7d7998fca0c/apex/optimizers/fused_adam.py#L154).
In this case, the step count is not restored. This PR changes this
behavior to always set step count in a param group.
This PR also stop incrementing the step count when loading. I didn't see
why we need to increment the step count for my small example, but we may
need a discussion to consider various cases.",['2df8e23487b5e292d9fe6e0c5116b487263970eb'],False,"['universal_checkpoint.py', 'bf16_optimizer.py', 'engine.py', 'stage_1_and_2.py', '__init__.py', 'tensor_fragment.py']"
4520edd61ce1235f87d062aa075cff412ae11a73,"Fixed Accelerate Link (#5314)

The current link was broken. Fixed it.",['0529eac632703ec044cee25fe6dd56752f7a5821'],False,['index.md']
e5dd5501c10227ae33dce7d5bdd897741dd3adb7,"support bf16_optimizer moe expert parallel training and moe EP grad_scale/grad_norm fix (#5259)

- bf16 moe EP requires different partitions and this will impact dp
gradient allreduce, zero1 params allgather, as well as gradient_norm
allreduce. Currently, the bf16_optimizer does not correctly partition
the group. fix and support bf16 type training.
- fix calculation of moe ep grad scale and grad_norm for bf16&fp16

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['4520edd61ce1235f87d062aa075cff412ae11a73'],False,"['utils.py', 'bf16_optimizer.py', 'engine.py', 'fused_optimizer.py', 'utils.py', 'test_fp16.py']"
ed8aed5703d97b6e52d0fca3e4be285e21c005f2,"fix comms dtype (#5297)

is the comms dtype name a bug?
can we fix it?

Co-authored-by: Mayank Mishra <mayank.mishra2@ibm.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['8d98e17140d5dd1cebbd8e78b066ad3e9bbfbd08'],False,"['config.py', 'test_bf16.py']"
cea5ea1eb61689a79d36bc89e31f3c5fee29c04b,"Docs typos fix and grammar suggestions (#5322)

Hey, this commit contains a few typo fixes and grammar suggestions for
you to consider.",['a32789b0333d20fd3ed94df1239c02486b867487'],False,['accelerator-abstraction-interface.md']
c56a4b9e0db340dcb0f7e14495699ca5c90dd976,"Improve universal checkpoint (#5289)

This PR includes the following improvement regarding universal
checkpoint.

- Restoring step

A universal checkpoint saves the training step count taken from the
engine. In
https://github.com/microsoft/DeepSpeed/pull/5263, we fixed to always set
this count to restore training step count to optimizer's states
per-param (`optimizer_state['state`][param]['step']`) and a param_group.
However, this approach does not restore the optimizer's state and param
groups precisely due to different behaviors of optimizers.

Torch's Adam doesn't make `step` in a param groups and only uses
`optimizer_state['state'][param]['step']`. Apex's fused adam only uses
`step` in a param groups. DeepSpeed's fused adam creates `step` in a
param groups and never updates. It only uses
`optimizer_state['state'][param]['step']`.
Consequently, this leads to discrepancies between the restored and
original states of the optimizer and param groups.

This PR modifies the restoration process to ensure that the step number
in the optimizer's state and param groups matches those in the original
setup, effectively aligning the restored and original optimizer states
and param groups.

- Unit tests of DP size scaling

This PR also adds unit tests to verify universal checkpointing. They run
training with DP, save a checkpoint, and converts in to a universal
checkpoint. Then they load the checkpoint with a different DP size and
validate that parameters and the all-gathered (ZeRO 1/2) optimizer
states match.

- Fix bug of loading with `load_optimizer_states=False`

The loader doesn't load parameters from a universal checkpoint when
`load_optimizer_states=False`.
https://github.com/microsoft/DeepSpeed/pull/5289/commits/c8c0498fe589c20ad830efdecf6a0a28f38fb7ae
fixes this issue.",['330d36bb39b8dd33b5603ee0024705db38aab534'],False,"['__init__.py', 'constants.py', 'ds_to_universal.py', 'universal_checkpoint.py', 'zero_checkpoint.py', '__init__.py', 'base_optimizer.py', 'bf16_optimizer.py', 'engine.py', 'fused_optimizer.py', 'unfused_optimizer.py', 'stage3.py', 'stage_1_and_2.py', 'common.py', 'test_universal_checkpoint.py', 'test_zero_optimizer.py']"
014576a87c8c1a99a6a1ee799a78dc4d36a52473,"Correct typo in checking on bf16 unit test support (#5317)

Co-Authored-By: Du Li <du.li@microsoft.com>",['ffb53c25cc8c7beeb0d4c092fb70d7352a1032e3'],False,['test_lr_scheduler.py']
cc897ecf15fdac5437fa4a2743154dc6c1749da4,"resolve KeyError: 'PDSH_SSH_ARGS_APPEND' (#5318)

when start job with `deepspeed --hostfile hostfile --master_addr
$MASTER_IP --ssh_port 20023 src/train_bash.py `

get error: KeyError: 'PDSH_SSH_ARGS_APPEND' in
https://github.com/microsoft/DeepSpeed/blob/master/deepspeed/launcher/multinode_runner.py#L77

because PDSH_SSH_ARGS_APPEND not in environment.

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['b5e2045cda37ba99c9ded571811469e651b16117'],False,['multinode_runner.py']
40009eb1c7a4ef27d00f36fe4f97aeae2e315c0e,"BF16 optimizer: Clear lp grads after updating hp grads in hook (#5328)

This fix is to solve:

- Previous iteration's lp grads will still alive during the next
iteration's forward. This increases the memory footprint.
- The hook behavior is not aligned to its name
accumulate_hp_grads_and_remove_lp

Co-authored-by: qunyang <quyang@habana.ai>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['cc897ecf15fdac5437fa4a2743154dc6c1749da4'],False,['bf16_optimizer.py']
c946a34220862da10143cb9efdafdd773d7b3d3a,"Fix sort of zero checkpoint files (#5342)

The conversion from a regular checkpoint to universal one relies on
sorting of zero checkpoint files to merge sharded optimizer states. This
merge can silently produce wrong results as the sorting is in
alphabetical order.

The merging logic assumes that files are given in this order.
1. pp_index=0 tp_index=0 dp_index=0
2. pp_index=0 tp_index=0 dp_index=1
...

The optimizer state of a parameter can be sharded across multiple ranks.
If it is sharded across dp_index 9-11, the files will be
- bf16_zero_pp_rank_9_mp_rank_00_optim_states.pt
- bf16_zero_pp_rank_10_mp_rank_00_optim_states.pt
- bf16_zero_pp_rank_11_mp_rank_00_optim_states.pt 
 
As they are sorted in alphabetical order, the script merges the sharded
fragment in the order of [10, 11, 9].
This PR fixes this sort to extracts dp ranks in files and sort the files
treating the ranks as numbers.

Fix #5283

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['40009eb1c7a4ef27d00f36fe4f97aeae2e315c0e'],False,['reshape_utils.py']
d86a68c3d407b4aebb2116cec6f351ea5e799c19,"[fix] fix typo s/simultanenously /simultaneously (#5359)

fix typo s/simultanenously /simultaneously
         s/Colosal /Colossal
detail info 
        modified:   blogs/deepspeed-fp6/03-05-2024/README.md
        modified:   blogs/deepspeed-ulysses/README.md",['dc3554f8326ff7bde9dc70c277ef38b03defce6e'],False,"['README.md', 'README.md']"
9f0e21363b60d53e419d3e2ae764846cb4b8f6e4,"compute global norm on device (#5125)

Avoid host synchronization by keeping data on device

---------

Co-authored-by: Logan Adams <loadams@microsoft.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['548d37b1611e84448c5ec917f86080e65299e79d'],False,['utils.py']
6a4b96a1686e033a8272067c34211dddd89ea382,"logger update with torch master changes (#5346)

minor fix to resolve the logger import issue caused by torch upstream
cleanup
https://github.com/pytorch/pytorch/commit/b6201a60c57f79d081ecde66e1ce70e6614a3052

log variable was renamed in the torch master. To create the logger using
public API to avoid compatibility issues.

Fixes: https://github.com/microsoft/DeepSpeed/pull/5346

---------

Signed-off-by: roger feng <roger.feng@intel.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Logan Adams <loadams@microsoft.com>",['9f0e21363b60d53e419d3e2ae764846cb4b8f6e4'],False,['elastic_agent.py']
5fbc3eeebb38c10c5c36548ae84414052d7f87aa,"Ensure capacity does not exceed number of tokens (#5353)

When fine-tuning we were running into issues where the capacity would
trigger the following error after some amount of time training. This was
caused when the size of the inputs to top1gating were not aligned
between ranks.

```
...
  File ""/shared/users/jrasley/DeepSpeed/deepspeed/moe/sharded_moe.py"", line 427, in forward
    gate_output = top1gating(logits, self.capacity_factor if self.training else self.eval_capacity_factor,
  File ""/shared/users/jrasley/DeepSpeed/deepspeed/moe/sharded_moe.py"", line 240, in top1gating
    top_idx = _top_idx(mask1_rand, capacity)
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File ""/shared/users/jrasley/DeepSpeed/deepspeed/moe/sharded_moe.py"", line 172, in _top_idx
@torch.jit.script
def _top_idx(source, k):
    return torch.topk(source, k=k, dim=0)[1]
           ~~~~~~~~~~ <--- HERE
RuntimeError: selected index k out of range
```

Co-authored with: @rajhans
Reviewed/approved by: @samyam, @yaozhewei 

Tagging @tohtana and @ykim362 to help review",['6a4b96a1686e033a8272067c34211dddd89ea382'],False,"['sharded_moe.py', 'test_moe.py']"
4621ba4cd4cf8b058fc49ca9311ea6a263427496,"Update workflows that use cu116 to cu117 (#5361)

The following workflows were specifying runners with cu116, we are
updating these to cu117.

Workflows impacted:
- [x] nv-accelerate-v100
- [new
build](https://github.com/microsoft/DeepSpeed/actions/runs/8557768042/job/23450811816?pr=5361):
22 passed, 5 skipped, 11 warnings in 129.04s (0:02:09)
- [old
build](https://github.com/microsoft/DeepSpeed/actions/runs/8547131990/job/23418750315):
22 passed, 5 skipped, 11 warnings in 318.84s (0:05:18)
- [x] nv-ds-chat
- [new
build](https://github.com/microsoft/DeepSpeed/actions/runs/8546543733/job/23417119129):
15 passed, 1 skipped in 2729.91s (0:45:29)
- [old
build](https://github.com/microsoft/DeepSpeed/actions/runs/8531148226/job/23370268262):
15 passed, 1 skipped in 3511.82s (0:58:31)
- [x] nv-inference - recently failing and disabled, needs fixes.
- [new
build](https://github.com/microsoft/DeepSpeed/actions/runs/8558749560):
36 failed, 74 passed, 95 skipped, 4 warnings in 877.45s (0:14:37)
- [old
build](https://github.com/microsoft/DeepSpeed/actions/runs/8546382497/job/23416626521):
36 failed, 74 passed, 95 skipped, 4 warnings in 3633.34s (1:00:33)
- [x] nv-mii
- [new
build](https://github.com/microsoft/DeepSpeed/actions/runs/8557768075/job/23450812054?pr=5361):
4 passed, 23 deselected, 3 warnings in 116.28s (0:01:56)
- [old
build](https://github.com/microsoft/DeepSpeed/actions/runs/8547246351/job/23419064526):
4 passed, 23 deselected, 3 warnings in 196.79s (0:03:16)
- [x] nv-nightly
- [new
build](https://github.com/microsoft/DeepSpeed/actions/runs/8557763671/job/23450792634):
3 passed, 3 skipped, 4713 deselected, 1 warning in 1831.83s (0:30:31)
- [old
build](https://github.com/microsoft/DeepSpeed/actions/runs/8547230983/job/23419020962):
3 passed, 3 skipped, 4713 deselected, 1 warning in 2459.06s (0:40:59)
- [x] nv-torch-latest-v100
- [new
build](https://github.com/microsoft/DeepSpeed/actions/runs/8557768039/job/23450811779):
947 passed, 169 skipped, 4 warnings in 2550.25s (0:42:30) and 61 passed,
4 skipped, 4643 deselected, 1 warning in 563.34s (0:09:23)
- [old
build](https://github.com/microsoft/DeepSpeed/actions/runs/8547232496/job/23419024966):
947 passed, 169 skipped, 4 warnings in 3216.47s (0:53:36) and 61 passed,
4 skipped, 4643 deselected, 1 warning in 611.17s (0:10:11)
- [x] nv-torch-nightly-v100
- [new
build](https://github.com/microsoft/DeepSpeed/actions/runs/8558930744):
13 failed, 982 passed, 121 skipped, 4 warnings in 2691.26s (0:44:51)
- [old
build](https://github.com/microsoft/DeepSpeed/actions/runs/8558895638):
13 failed, 982 passed, 121 skipped, 4 warnings in 3117.03s (0:51:57)
- [x] nv-transformers-v100 - disabled for 4 months, needs work
regardless.",['5fbc3eeebb38c10c5c36548ae84414052d7f87aa'],False,"['nv-accelerate-v100.yml', 'nv-ds-chat.yml', 'nv-inference.yml', 'nv-mii.yml', 'nv-nightly.yml', 'nv-torch-latest-v100.yml', 'nv-torch-nightly-v100.yml', 'nv-transformers-v100.yml']"
3fbd01ccca85113a7c972f1644e98138019cf9f8,"FP [6,8,12] quantizer op (#5336)

Flexible-bit quantizer-dequantizer library with fp6/fp12/fp8 support

Requires Ampere+ architecture, this is due to the initial focus of this
op only on `bfloat16` input types.

Co-authored-by: Reza Yazdani <reza.yazdani@snowflake.com>",['4621ba4cd4cf8b058fc49ca9311ea6a263427496'],False,"['nv-pre-compile-ops.yml', 'context.h', 'quantize.h', 'quantize.cpp', 'quantize.cu', 'memory_access_utils.h', 'reduction_utils.h', '__init__.py', 'quantize.py', 'fp_quantizer.py', 'requirements-dev.txt', 'test_fp_quant.py']"
42a8eaa705ed30b4a656ac71bdb400772df2cb21,"Auto convert moe param groups (#5354)

When using frameworks like HF Accelerate with MoE models in HF there's
an issue when DeepSpeed is creating the optimizer where we have no way
to automatically create the compatible MoE param groups. This PR detects
if no client optimizer is set and model_parameters are passed to
DeepSpeed that they are either MoE compatible or makes them MoE
compatible automatically.

This was never an issue previously since (1) MoE hasn't really been
tested outside MDS and (2) MDS manually converts the weight-decay param
groups into being MoE compatible before deepspeed.initialize.

The error that is triggered if the param groups are not MoE compatible
is triggered here:
https://github.com/microsoft/DeepSpeed/blob/cc897ecf15fdac5437fa4a2743154dc6c1749da4/deepspeed/runtime/zero/stage_1_and_2.py#L610-L612

Tagging @tohtana and @ykim362 to help review

---------

Co-authored-by: Jeff Rasley <jeff.rasley@snowflake.com>",['731fd682996a0af56879c033ae3baec3db844e71'],False,"['utils.py', 'engine.py', 'test_moe.py']"
08e0733e4ad0e2a4a4c1014e393967b36a55daf1,"Support MoE for pipeline models (#5338)

This PR enhances DeepSpeed to support MoE for pipeline models (e.g.
GPTModelPipe from Megatron-DeepSpeed).
Main changes:

- Enhance expert groups creation for pipeline (enhance both flavors:
DP/PP/EP and DP/TP/PP/EP)
- Fix MoE save/load checkpoint for PipelineModule based models.
- Display MoE loss for PipelineModule based models.
- Support gradients reduce for BF16_Optimizer for
PipelineModule.<br>Note that same commit also fixes gradients reduction
error when using Megatron-DeepSpeed GPTModelPipe with BF16_Optimizer
also for a dense (no MOE) model.
- When using no-drop tokens, all-reduce the capacity (op=max) using
expert parallel group instead of world group

---------

Signed-off-by: Moshe Island <misland@habana.ai>
Co-authored-by: Moshe Island <misland@habana.ai>",['42a8eaa705ed30b4a656ac71bdb400772df2cb21'],False,"['layer.py', 'mappings.py', 'sharded_moe.py', 'moe_inference.py', 'checkpointing.py', 'bf16_optimizer.py', 'engine.py', 'engine.py', 'module.py', 'utils.py', 'stage_1_and_2.py', 'bwc.py', 'groups.py', 'test_groups.py']"
72bd275b795f49e7dec50dc6b2532e2b0778a66a,"Update pytest and transformers with fixes for pytest>= 8.0.0 (#5164)

The underlying issue preventing us from using pytest 8.0.1+ is in
transformers. Here is a [PR that fixes the
issue](https://github.com/huggingface/transformers/pull/29154) that will
need to be merged and then a new release of transformers before we can
complete this PR.

No impact to nv-nightly, run
[here](https://github.com/microsoft/DeepSpeed/actions/runs/8575192380/job/23503659644)
Currently errors on nv-inference from [the
PR](https://github.com/microsoft/DeepSpeed/actions/runs/8575187611/job/23503647378),
but the same tests are broken in
[master](https://github.com/microsoft/DeepSpeed/actions/runs/8575185551/job/23503640238).",['08e0733e4ad0e2a4a4c1014e393967b36a55daf1'],False,['requirements-dev.txt']
7e126d20b968db5e2a31f198bab6fd79ab33260b,"Update path name on xpu-max1100.yml, add badge in README (#5386)

- Fixes xpu-max1100 not running on PR because of incorrect yml name.",['873738897805f8bff31fd8145bf67ef9e9379095'],False,"['xpu-max1100.yml', 'README.md']"
63029e8f5d1d248a47ad52c497899cbefffbfee9,"Update checkout action on workflows on ubuntu 20.04 (#5387)

- Only workflows running Ubuntu 20.04 or later can be updated as the
GLIBC that is needed for node 20+ can be updated now.
- Workflows that aren't updated are running Ubuntu 18.04 or older, those
will need to be moved to updated images shortly and will be updated
later in the original PR, #5021

Sample warning that is resolved:

```
Node.js 16 actions are deprecated. Please update the following actions to use Node.js 20: actions/checkout@v3. For more information see: https://github.blog/changelog/2023-09-22-github-actions-transitioning-from-node-16-to-node-20/.
```",['62ab18102036a9079e0fd569cb149d6bdbd64d7b'],False,"['amd-mi200.yml', 'cpu-torch-latest.yml', 'formatting.yml', 'hpu-gaudi2.yml', 'nv-a6000.yml', 'nv-accelerate-v100.yml', 'nv-ds-chat.yml', 'nv-h100.yml', 'nv-inference.yml', 'nv-mii.yml', 'nv-nightly.yml', 'nv-pre-compile-ops.yml', 'nv-sd.yml', 'nv-torch-latest-v100.yml', 'nv-torch-nightly-v100.yml', 'nv-transformers-v100.yml', 'release.yml', 'xpu-max1100.yml']"
6dcced1d5c997876e9a1279f79ec8f9339561846,"Cleanup required_torch_version code and references. (#5370)

- Move `required_torch_version` check from deepspeed.runtime.utils to
deepspeed.utils.torch (newly created).
- Remove unused duplicate definition from `tests/unit/util.py`.
- Update all references to this function.
- Switch checks in `deepspeed/runtime/pipe/p2p.py` to use this function.
- Switch checks in `deepspeed/comm/torch.py` to use this function.

---------

Co-authored-by: Lev Kurilenko <113481193+lekurile@users.noreply.github.com>",['63029e8f5d1d248a47ad52c497899cbefffbfee9'],False,"['torch.py', 'utils.py', 'nccl.py', 'fused_optimizer.py', 'adam.py', 'lamb.py', 'zoadam.py', 'unfused_optimizer.py', 'p2p.py', 'utils.py', 'torch.py', 'alexnet_model.py', 'test_mics_optimizer.py', 'test_moe_checkpoint.py', 'test_universal_checkpoint.py', 'test_zero_optimizer.py', 'test_compression.py', 'test_intX_quantization.py', 'test_configurable_parallel_mp.py', 'test_configurable_parallel_pp.py', 'test_moe.py', 'test_moe_tp.py', 'test_flops_profiler.py', 'test_compile_wrapper.py', 'test_compile_zero.py', 'test_load_config.py', 'test_onebit.py', 'test_fp16.py', 'test_ds_initialize.py', 'util.py']"
cc9e7b9c79ae48a7e93b9089e264eafb919178ab,"Update README.md for intel XPU support (#5389)

1. fix a type error
2. update the intel xpu HW support status",['6dcced1d5c997876e9a1279f79ec8f9339561846'],False,['README.md']
f69f8840fc62e6cbbbad9be4216729158611127e,"Removal of cuda hardcoded string with get_device function (#5351)

In UTs removed 'cuda' string hardcode by replacing with device variable
set to get_accelerator().device_name()

Co-authored-by: Shaik Raza Sikander <srsikander@habana.ai>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['2c51aba0b70a053252c6b0c95ecdecebb7b64d43'],False,"['test_he_lora.py', 'test_stable_diffusion.py', 'test_attention.py', 'test_gelu.py', 'test_layer_norm.py', 'test_softmax.py']"
7b5b06602d5941cf7ea6170062d3f81c9002d788,"fix pagable h2d memcpy (#5301)

ZeRO offload case

Fix the issue of pageble h2d memcpy in step process. Now h2d memcpy uses
pinned memory.

Speedup h2d memcpy by 6x on single GPU and 4-5x on 8GPU node.

cc @tjruwase

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Ubuntu <deepspeed@deepspeed-login.2d1icxc5dsxehnpuwt3ifc34ph.gvxx.internal.cloudapp.net>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['f69f8840fc62e6cbbbad9be4216729158611127e'],False,['stage_1_and_2.py']
e3d873a00ec2a78f74796f9a1a0d571f310e5f87,"Fix the FP6 kernels compilation problem on non-Ampere GPUs. (#5333)

Refine the guards of FP6 kernel compilation. Fix the `undefined symbol`
problem of FP6 kernels on non-Ampere architectures.

Related issue: https://github.com/microsoft/DeepSpeed-MII/issues/443.

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>",['54c06872647ca60699f752e60ac1643bd05aa63c'],False,"['core_ops.cpp', 'kernel_matmul.cuh', 'kernel_reduction.cuh', 'ptx_cp.async.cuh', 'ptx_mma.cuh', 'utils_core.cuh', 'utils_gmem.cuh', 'utils_paralleldequant.cuh', 'weight_prepacking.h', 'linear_kernels.cpp', 'linear_kernels.h', 'linear_kernels_cuda.cu', 'linear_kernels_cuda.h', 'heuristics.py', 'inference_core_ops.py', 'test_quantized_linear_module.py']"
8949105369ccaacecc7b707601b787e4af730e89,"Remove dtype(fp16) condition check for residual_add unit test (#5329)

When the dtype is bf16 or fp32 the if condition is not satisfied and it
continues execution instead of skipping when triton is not installed.

Co-authored-by: Shaik Raza Sikander <srsikander@habana.ai>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['2b3d31f9804f024f0cbb036771c35a5bdd9ef848'],False,['test_residual_add.py']
0896503e2f4d3b12583dfe267e52db3a1d63b88d,"Fix a convergence issues in TP topology caused by incorrect grad_norm. (#5411)

Some users are concerned that changes in TP topology during MOE training
may potentially cause interference with experiments when noticing
similar issues
https://github.com/microsoft/Megatron-DeepSpeed/issues/151 
https://github.com/microsoft/Megatron-DeepSpeed/pull/176/files

We found a grad_norm calculation error after enabling TP. This error
occurs because flattened grad of a params group is used, where the group
contains both non-TP and TP parameters. Therefore, it is not possible to
use a single attribute to determine whether flattened grad needs to
compute the norm. In the current code logic, all params are assumed to
be non-TP, resulting in only tp_rank0 grad participating in grad_norm
computation. Other tp_rank grads have grad_norm_sum equal to 0. We
tested and found that with TP=1 and TP=4, the difference in grad_norm is
approximately twice (sqrt(4)). This aligns with the aforementioned
issue. This problem should also affect dense models.

Due to the absence of flattening params_group grad in bf16, this problem
is avoided.

We tested the loss curve on the 1.3B model. In cases where TP size
increases the inconsistent gap should be larger.

with this change 1.3B with EP=4 TP=4 &1 , fp16,mbs=1,gbs=16

![image](https://github.com/microsoft/DeepSpeed/assets/27563729/855042c8-ac8a-4192-b465-5fa60c1a7c59)


without this change  1.3B with EP=4 TP=4&1  ,fp16,mbs=1,gbs=16

![image](https://github.com/microsoft/DeepSpeed/assets/27563729/66854d14-7b83-4b09-a669-b452d6157ea0)

---------

Co-authored-by: Conglong Li <conglong.li@gmail.com>",['258e500f3fcd33fa42871ff87612d135399017ac'],False,"['fused_optimizer.py', 'utils.py']"
bc0f77472828323fdc0ae67f62123948bc2b12d1,"Update engine.py to avoid torch warning (#5408)

The state_dict function of module.py from torch write a warning if
arguments are positional arguments and not keyword arguments

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: ebonnafoux <ebonnafoux156@headmind.com>",['34969d2091fcfd37545361111d7ce7d13229d1a3'],False,['engine.py']
3194fe85c5b62940f5479deba0396121f2d87d59,"Add required paths to trigger AMD tests on PRs (#5406)

Even though AMD tests are currently broken, this will at least trigger
them on PRs that touch files that might impact them. Since the test name
is listed as `amd-tests` rather than `unit-tests` they will currently
not be required, however.

Co-authored-by: root <root@loadams-dev.redmond.corp.microsoft.com>",['b22706a7211366abf2df98a0d118ea1d3a837e21'],False,['amd-mi200.yml']
aaaf8bc5e07535e263f83733f8905400bf6f5aca,"Bug fix in `split_index` method (#5292)

Bug description: on a dataset of 20 samples, when running 4 workers with
8 threads per worker, then the `split_dataset` would return for worker
id `1`:

```
self.worker_splits
[[0, 5], [5, 10], [10, 15], [15, 20]]


self.thread_splits
[[5, 6], [6, 7], [7, 8], [8, 9], [9, 10], [10, 10], [11, 10], [12, 10]]
```

`thread_splits` is wrong and causes a crash in the `DataAnalyzer`: the
end sample id is lower than the initial one on the last 2 threads.
This PR fixes that by fixing the behaviour of `split_index`

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['3194fe85c5b62940f5479deba0396121f2d87d59'],False,['utils.py']
c632ea09f8d107d10f76aa2b776e4df3c1ccf98a,"Selective dequantization (#5375)

This PR adds a new functionality for the dequantizer function, called
`selective_dequantize`, which enables partially dequantizing a
3-dimensional matrix in case we don't need to dequantize all the data
from lower bit (like fp8/fp6) to bf16.
I also added a unit test to check its functionality.

---------

Co-authored-by: Reza Yazdani <reza.yazdani@snowflake.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['64defe65b73f856466935befc49fb188756ab558'],False,"['quantize.h', 'quantize.cpp', 'quantize.cu', 'quantize.py', 'test_fp_quant.py']"
99951caa3d2155a3bb84109a0828543793e088cc,"Fix sorting of shard optimizer states files for universal checkpoint (#5395)

This PR resolves the issue reported in #5283.
To resolve the issue, we sort files of sharded optimizer states based on
DP indices.

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['c632ea09f8d107d10f76aa2b776e4df3c1ccf98a'],False,['ds_to_universal.py']
9b6ef9e1f0d8acaefd989440b27da9069aa69207,"64bit indexing fused adam (#5187)

## The Issue

Applying `FusedAdam` on large tensors will cause an error `CUDA error:
an illegal memory access was encountered`.

https://github.com/microsoft/DeepSpeed/issues/3429

https://github.com/NVIDIA/apex/issues/1654

## PR Content

Following the solution in the apex repository
(https://github.com/NVIDIA/apex/pull/1765), changing indexing type to
`int64` if necessary.

---------

Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['3f875d95193fbd3a0c7f0c0dcc8d39469061bb66'],False,"['multi_tensor_adam.cu', 'multi_tensor_apply.cuh']"
5e6c9b931184bf8f0a245a2fef183078139b6d7e,"OptimizedLinear implementation (#5355)

Optimized version of `nn.Linear` that adds features such as:
      * LoRA w. base weight sharding
      * FP [6,8,12] quantization

Depends on #5336 being merged first

Co-authored-by: @rajhans
Co-authored-by: @aurickq

---------

Co-authored-by: Rajhans Samdani <rajhans.samdani@snowflake.com>
Co-authored-by: Jeff Rasley <jeff.rasley@snowflake.com>",['c66bc4269e9484b6e57d6f5521df02c70d399246'],False,"['__init__.py', 'config.py', 'optimized_linear.py', 'quantization.py', '__init__.py', 'quantize.py', 'test_linear.py', 'test_quant_param.py']"
5f631abc2f930ecece38fae05dc9bd3923c555dd,Update PyTest torch version to match PyTorch latest official (2.3.0) (#5454),['ad2027952f9730cbd1a8385e4a441e470248645e'],False,"['cpu-torch-latest.yml', 'nv-torch-latest-v100.yml']"
fa8458b1a80d6ba55091b17f092de19bbf95eb3d,"Add getter and setter methods for compile_backend across accelerators. (#5299)

Add getter and setter methods for `compile_backend` across accelerators,
which provide a mechanism to retrieve the compile backend. These APIs
handle user-defined backend selection and raise a `ValueError` with
informative error messages for unsupported backends.

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['fbdf0eaf15f45cd2a8b5846ad9428609a3860b41'],False,"['abstract_accelerator.py', 'cpu_accelerator.py', 'cuda_accelerator.py', 'hpu_accelerator.py', 'mps_accelerator.py', 'npu_accelerator.py', 'xpu_accelerator.py', 'test_compile_wrapper.py', 'test_compile_zero.py', 'test_load_config.py']"
fcc731f09d8e09b04a816b3ea0f83ab1d15169b3,"Fix torch.compile error for PyTorch v2.3 (#5463)

PyTorch v2.3 throws an error when it tries to compile `iter_params` used
for ZeRO3.
This PR excludes the function from the compilation targets.

After this PR is merged, we can [unpin the torch version for unit
tests](https://github.com/microsoft/DeepSpeed/pull/5459).",['fa8458b1a80d6ba55091b17f092de19bbf95eb3d'],False,['partitioned_param_coordinator.py']
bc48371c5e1fb8fd70fc79285e66201dbb65679b,"Revert ""stage3: efficient compute of scaled_global_grad_norm (#5256)"" (#5461)

This reverts commit 54c06872647ca60699f752e60ac1643bd05aa63c due to
#5256 causing bugs when the ZeRO3 + ZeRO Offload features are enabled.

This bug was discovered due to failures in the DS Chat CI workflow.
Failing tests across CI failures:
| Failing Test Name |
| --- |
| test_ds_chat[zero3--offload-] |
| test_ds_chat[zero3--offload-lora] |
| test_ds_chat[zero3-he-offload-] |
| test_ds_chat[zero3-he-offload-lora] |

Error message:
```
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:3 and cpu!
```

It seems that `torch.stack()` or `torch.norm()` is having issues when
the offload feature is enabled and tensors are split between CPU/GPU,
however this is just an initial guess and would require more
investigation.

@nelyahu Since you are the original author of the PR, if you have some
bandwidth, any help here is greatly appreciated!

After reverting this commit, all tests pass in the DS Chat CI workflow:

https://github.com/microsoft/DeepSpeed/actions/runs/8824064414/job/24225802763

@tjruwase for context.",['fcc731f09d8e09b04a816b3ea0f83ab1d15169b3'],False,['stage3.py']
059bb2085cf404caa5874004a252a56fc74c952e,"fix: swapping order of parameters in create_dir_symlink method. (#5465)

Order of parameters in create_dir_symlink method looks wrong. Because
this we get the error ""PermissionError: [WinError 5] Denied access:
'.\\deepspeed\\ops\\csrc'"" when install deepspeed >= 0.4.0 on Windows
enviroment.

Please check this out @eltonzheng and @jeffra.

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['4c15ad9f8d51a1950842c69bbbc9d93c73afbcfc'],False,['setup.py']
f32ad3e1c562be80bd8be6b7b2246dc3041b5bfd,"Un-pin torch version in nv-torch-latest back to latest and skip test_compile_zero tests on v100 (#5459)

Torch updating to 2.3.0 broke some test_compile_zero tests, we pinned
it, @tohtana pushed fixes in #5463, this should un-pin and move us back
to the latest.

Failing test that indicates the generated code cannot run bf16 on V100
[here](https://github.com/microsoft/DeepSpeed/actions/runs/8838672379/job/24270349996?pr=5459#step:8:5157).",['059bb2085cf404caa5874004a252a56fc74c952e'],False,"['nv-torch-latest-v100.yml', 'test_compile_zero.py']"
90793aab546beedfd805eddd8fd2d807b3862d53,"re-introduce: stage3: efficient compute of scaled_global_grad_norm (#5493)

reverting previous revert of this feature:

https://github.com/nelyahu/DeepSpeed/commit/bc48371c5e1fb8fd70fc79285e66201dbb65679b
in addition,
bug fix for offload mode.",['f32ad3e1c562be80bd8be6b7b2246dc3041b5bfd'],False,['stage3.py']
0fc19b6a320cf8aa0a5f6c2b1fa310bae9a70d94,"Fix crash when creating Torch tensor on NPU with device=get_accelerator().current_device() (#5464)

Creating a Torch tensor with the parameter
`device=get_accelerator().current_device()` can result in a crash when
using an NPU.

This issue arises because the `current_device` API across all
accelerators is expected to return a device id as an integer, according
to the [interface
docs.](https://github.com/microsoft/DeepSpeed/blob/fa8458b1a80d6ba55091b17f092de19bbf95eb3d/docs/_tutorials/accelerator-abstraction-interface.md?plain=1#L52C1-L56C103)

However, specifying `device` as an interger when creating tensors by
default directs Torch to use the CUDA backend, which leads to crash on
NPUs (and potentially other accelerators as well).

To resolve this, we should use `get_accelerator().current_device_name()`
instead, which returns the correct device identifier strings such as
`""npu:0"", ""cuda:0"", or ""xpu:0""`. This API provides the appropriate
context needed for creating tensors on specific hardware accelerators.

I also notice that `device=get_accelerator().current_device()` is used
across several files under deepspeed/inference, and may also lead to
crash on other accelerators.

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['90793aab546beedfd805eddd8fd2d807b3862d53'],False,"['optimized_linear.py', 'fused_optimizer.py', 'utils.py', 'test_moe.py']"
0b224edcf7d83713b95ad6b989694a8bdf01809e,"Fix compile wrapper (#5455)

compile wrapper will inherit from user module class and copy it's
__dict__

This should resolve most issues in #5383 except potential extra user
forward hooks.

@tohtana @loadams

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Masahiro Tanaka <81312776+tohtana@users.noreply.github.com>",['0fc19b6a320cf8aa0a5f6c2b1fa310bae9a70d94'],False,"['compiler.py', 'engine.py', 'engine.py']"
82ce4ae815d6cab4ffea97427dfa1e0b0ff59f93,"Switch pynvml to nvidia-ml-py (#5529)

Fixes: #5517 

Link to PyPI for nvidia-ml-py
[here](https://pypi.org/project/nvidia-ml-py/) showing usage remaining
the same as previous pynvml package.",['3a7f3aa8498582860cfdaca6d1e19a968964ea76'],False,['requirements.txt']
488a823f64d436b3cce8936928780e8825b18862,"New integration - CometMonitor (#5466)

This PR introduces a new monitoring option - `CometMonitor` which comes
up as an official integration with
[CometML](https://www.comet.com/site/).

The new monitor is covered with unit tests.

Notes:
* We've updated `docs/code-docs/source/monitor.rst` but it doesn't look
used anymore
* We've updated the ""Monitoring Module"" section name in `config-json.md`
to be generic so the next integration won't require updating it.

---------

Co-authored-by: Boris Feld <lothiraldan@gmail.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['ebf82e8f3ad6d51d49d115e54a11ae4597ff36fb'],False,"['comet.py', 'config.py', 'monitor.py', 'utils.py', 'navigation.yml', 'config-json.md', 'monitor.md', 'comet_monitor.png', 'monitor.rst', 'requirements-dev.txt', 'test_monitor.py']"
7f55b20f3ef409a5fd9c443e31b099af48121043,"Enhance testing: Skip fused_optimizer tests if not supported. (#5159)

Enhance testing: Skip fused_optimizer tests if not supported.

Added condition check to skip fused_optimizer tests if FusedAdam and
FusedLamb are not supported by the accelerator. This enhancement ensures
that the tests are appropriately skipped when the hardware configuration
does not support these optimizers, preventing potential issues.

Details:
- Introduced a condition check to determine support for FusedAdam and
FusedLamb.
- If not supported, fused_optimizer tests are skipped to improve test
reliability.
- Improved compatibility and stability across different hardware
configurations.

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['23173faa4b571c308bcc0d929c8c18905371ed63'],False,"['test_elastic.py', 'test_cpu_adam.py', 'test_hybrid_adam.py', 'test_dynamic_loss_scale.py', 'test_fp16.py', 'test_ds_initialize.py']"
8e4f6e48db8e38f5a9fb02dc8851c33a818ebbf4,"Skip the UT cases that use unimplemented op builders. (#5372)

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Logan Adams <loadams@microsoft.com>",['7f55b20f3ef409a5fd9c443e31b099af48121043'],False,"['test_elastic.py', 'test_accelerator_backward.py', 'test_accelerator_forward.py', 'test_cpu_lion.py', 'test_lion.py', 'test_dynamic_loss_scale.py', 'test_fp16.py']"
d3dd8e74546f95a1eb6369dc6ec9671dd393e540,"rocblas -> hipblas changes for ROCm (#5401)

Fixes https://github.com/microsoft/DeepSpeed/issues/4989

In addition to this PR, below changes are required to build below
extensions successfully. Please note that not all unit tests for these
extensions will pass with this PR. More details on the unit test results
are below. These unit tests are skipped in CI anyway, so they will not
break the CI.
- transformer_inference
- quantizer
- random_ltd

- https://github.com/pytorch/pytorch/pull/121030
- https://github.com/microsoft/DeepSpeed/pull/5402


Unit test results (rocm/pytorch:rocm6.1_ubuntu20.04_py3.9_pytorch_2.1.2)
on MI200:

**transformer_inference:**
pytest --color=yes --durations=0 --verbose -s -m ""inference_ops"" -rF -n
4 unit/ops/transformer/inference

Before this PR: 
==== 674 failed, 622 skipped, 8 warnings, 1728 errors in 123.66s
(0:02:03) =====

After this PR:
========== 555 failed, 983 passed, 1486 skipped, 8 warnings in 14.35s
==========

**quantizer:**
pytest --color=yes --durations=0 --verbose -s -m ""inference_ops"" -rF -n
4 unit/ops/quantizer

Before this PR: 
==== 244 failed, 8 warnings in 48.02s ====

After this PR:
===== 187 failed, 57 passed, 8 warnings in 14.74s ====

I could not find random_ltd related unit tests to run.

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Logan Adams <loadams@microsoft.com>",['8e4f6e48db8e38f5a9fb02dc8851c33a818ebbf4'],False,"['cublas_wrappers.h', 'feed_forward.h', 'gemm_test.h', 'strided_batch_gemm.h', 'cublas_wrappers.cu', 'pt_binding.cpp', 'inference_cublas_wrappers.h', 'blas_utils.h', 'gated_activation_kernels_cuda.cu']"
76c9c69fb1ca053b83f703662ed654ad03f44e4f,"Rocm warp size fix (#5402)

This PR enables building the below extensions for AMD GPUs with warp
size 32.
- transformer_inference
- quantizer
- random_ltd


This PR works stand-alone for torch version <=2.0. For the latest
versions, https://github.com/microsoft/DeepSpeed/pull/5401 is required
to be merged in addition to this PR.

Unit test results (rocm/pytorch:rocm6.1_ubuntu20.04_py3.9_pytorch_2.1.2)
on NAVI3x:

**transformer_inference:**
pytest --color=yes --durations=0 --verbose -s -m ""inference_ops"" -rF -n
4 unit/ops/transformer/inference

Before this PR:
===== 674 failed, 622 skipped, 8 warnings, 1728 errors in 69.37s
(0:01:09) =====

After this PR:
========== 476 failed, 1062 passed, 1486 skipped, 8 warnings in 9.31s
==========

**quantizer:**
pytest --color=yes --durations=0 --verbose -s -m ""inference_ops"" -rF -n
4 unit/ops/quantizer

Before this PR:
     ==== 244 failed, 8 warnings in 30.53s ====

After this PR:
    ====== 186 failed, 58 passed, 8 warnings in 8.89s ======

I could not find random_ltd related unit tests to run.

Fixes: 
https://github.com/microsoft/DeepSpeed/issues/4753
https://github.com/microsoft/DeepSpeed/issues/5474
https://github.com/ROCm/DeepSpeed/issues/68

cc: @jithunnair-amd

---------

Co-authored-by: rraminen@amd.com <rraminen>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['d3dd8e74546f95a1eb6369dc6ec9671dd393e540'],False,"['ds_kernel_utils.h', 'token_sort.cu', 'apply_rotary_pos_emb.cu', 'ds_kernel_utils.h', 'builder.py']"
69af3611679609cde7a3100483c75f8e8d6937e1,"CPUAdam fp16 and bf16 support (#5409)

Hi.
Please review the following changes
I added support for BF16 to cpu adam. BF16, FP16 and float are supported
at compilation time. the correct template is called at runtime according
to input params dtype.

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['49df8d8da01e378fb1a0685cbcd4350fd0445801'],False,"['cpu_adagrad.cpp', 'cpu_adam.cpp', 'cpu_adam_impl.cpp', 'custom_cuda_kernel.cu', 'cpu_adagrad.h', 'cpu_adam.h', 'cpu_lion.h', 'custom_cuda_layers.h', 'simd.h', 'cpu_lion.cpp', 'cpu_lion_impl.cpp', 'cpu_adagrad.py', 'cpu_adam.py', 'cpu_lion.py', 'builder.py', 'builder.py', 'cpu_adagrad.py', 'cpu_adam.py', 'cpu_lion.py', 'builder.py', 'cpu_adam.py', 'adam_test1.py', 'common.py', 'test_cpu_adagrad.py', 'test_cpu_adam.py', 'test_hybrid_adam.py', 'test_cpu_lion.py']"
1d8196736f3f53dd754803f42c23fa1010c49d04,"Fix the TypeError for XPU Accelerator (#5531)

Fixing following error
/datadisk2/wengshiy/llm.devkit/DeepSpeed/deepspeed/runtime/utils.py
    return get_accelerator().FloatTensor(float(v)).detach()
TypeError: new(): data must be a sequence (got float)

cuda accelerator modified the interface for fixing warning:
https://github.com/microsoft/DeepSpeed/commit/177dc14331a64e61f6dcce2c4b8071576bcb22db

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['69af3611679609cde7a3100483c75f8e8d6937e1'],False,['xpu_accelerator.py']
695d79ea0664247ed4ef31425f529a7cf0ff561b,"Fix RuntimeError for moe on XPU: tensors found at least two devices (#5519)

There is following error on XPU while unit testing
""DeepSpeed/tests/unit/moe/test_moe.py""
DeepSpeed/deepspeed/moe/sharded_moe.py line 223, in top1gating
RuntimeError: Expected all tensors to be on the same device, but found
at least two devices, xpu:0 and cpu!

Fix it by device conversion.

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['1d8196736f3f53dd754803f42c23fa1010c49d04'],False,['sharded_moe.py']
5b314f4e6b87f9f55426a6bc35ff0d1cca88a552,"Avoid overwrite of compiled module wrapper attributes (#5549)

**Fix overwriting of the compiled wrapper class attributes by those of
the wrapped class itself: Copy only those attributes which are not
already present in the wrapper.**

In the current implementation of the `CompiledModuleWrapper` the wrapper
attributes (eg `forward` method) are overwritten by `self._dict_ =
module._dict_.copy()`:

```
def CompiledModuleWrapper(mod, compile_config: Union[CompileConfig, None] = None):
     class wrapper(mod.__class__):
         def __init__(self, module, compile_config: Union[CompileConfig, None] = None):
             self.__dict__ = module.__dict__.copy()
```
This causes the `wrapper`'s `forward` method not being called and,
consequently, the wrapped module not compiled. Instead, the wrapped
module `forward` method is being called as illustrated in the diagram
below (a real scenario from Deespeed-Chat):


![compiled_module_wrapper_bug](https://github.com/microsoft/DeepSpeed/assets/75629718/00eeb3d1-927c-49c7-84ab-f882821cc452)

The proposed fix copies only those attributes which are not present in
the wrapper class, thus implementing the desired inheritance quality of
the wrapper.

Attached is a simple reproducer of the problem.

[compiled_module_wrapper_bug.zip](https://github.com/microsoft/DeepSpeed/files/15378282/compiled_module_wrapper_bug.zip)

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['0a1740386f6a3fef1b655e27e0d8d0bc40879021'],False,['compiler.py']
263bfe2892c1ee6285076214bb5e3898c35e78f3,"Update to HF_HOME from TRANSFORMERS_CACHE (#4816)

Addresses the following warning:

```
/tmp/actions-runner/_work/DeepSpeed/DeepSpeed/unit-test-venv/lib/python3.8/site-packages/transformers/utils/hub.py:123: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
```

and the code on the transformers side is
[here](https://github.com/huggingface/transformers/blob/1a585c1222a56bcaecc070966d558d4a9d862e83/src/transformers/utils/hub.py#L86C1-L96C81).",['29903925cf281e41742b6e5baba2daf6b83d2b76'],False,"['cpu-inference.yml', 'cpu-torch-latest.yml', 'action.yml', 'test_checkpoint_sharding.py', 'test_inference.py']"
995ba11928f2f264e96f2b80aa5781a826b445fe,"Add throughput timer configuration (#5363)

The new ""timers"" section describes configuration for different timers.

Specifically, in the ""throughput"" section, it is possible to disable the
throughput timer (enabled by default). This allows to avoid the
performance degradation whenever the throughput measurement is not
needed, for example in production environment.

No device synchronize() is invoked when ""synchronized"" is set to False
(default is True). This allows to produce approximate throughput
measurements with minimal performance penalty.

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['f4efef21b8370f5963f312a52968045b2f5e00b2'],False,"['config.py', 'engine.py', 'engine.py', 'config.py', 'timer.py']"
bf66acdbae4551d97f4044e92e75ce8d7633299f,"Rename files in fp_quantize op from quantize.* to fp_quantize.* (#5577)

Fixes #5535.

Todo: need to test.",['fd8051a69cc5fc188c4588692d929bca4fc62900'],False,"['fp_quantize.cpp', 'fp_quantize.cu', 'fp_quantize.h', 'fp_quantizer.py']"
4deb40de67728dec2606327e8d63d3cb7028cc94,"Update to fix sidebar over text (#5567)

- [x] Needs to be tested.

Fixes #5494.

Sample screenshot:
<img width=""1141"" alt=""image""
src=""https://github.com/microsoft/DeepSpeed/assets/114770087/f89f642b-bca1-4d45-b3f1-ec7943ab2ad4"">",['bf66acdbae4551d97f4044e92e75ce8d7633299f'],False,['_sidebar.scss']
e7dd28a23d3df0d07357a6d63c0ebab811e767ae,"Fixed the Windows build. (#5596)

Fixed the Windows build.

Fixes applied:
- Remove some more ops that don't build on Windows.
- Remove the use of symlinks that didn't work correctly and replace with
`shutil.copytree()`.
- Small fixes to make the C++ code compile.

Tested with Python 3.9 and CUDA 12.1.

---------

Co-authored-by: Costin Eseanu <costineseanu@gmail.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['77c949421e8bfbaa18c383cdc70797ac93e6b44f'],False,"['build_win.bat', 'simd.h', 'pt_binding.cpp', 'setup.py']"
f4cb866c2eba60a72d79c3741d12d098fe4e45b8,"estimate_zero2_model_states_mem_needs: fixing memory estiamtion (#5099)

was considering 4 bytes per model param, and 4 bytes per gradient. 
fixed it to 2 bytes - under the assumption of FP16/BF16

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['e7dd28a23d3df0d07357a6d63c0ebab811e767ae'],False,['stage_1_and_2.py']
af4356b348c64d227d08cdc192ac7c053a0c10b6,"Fix cuda hardcode for inference woq (#5565)

This is a simple fix for inference woq part, changing from `'cuda'` to
`get_accelerator().device_name()`.

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['f4cb866c2eba60a72d79c3741d12d098fe4e45b8'],False,['utils.py']
6b6d64185708e194ad06a8b6e08a15af3f59d1d4,"fix sequence parallel(Ulysses) grad scale for zero0 (#5555)

use dp_world_size for grad reduction, instead of seq_dp_world_size.
Currently, for zero0, only sparse tensors use the correct world_size.

tiny model with sp=4 grad norm test:
grad_norm | step1 | step2 | step3 | step4 |step5 | step100
-- | -- | -- | -- | -- | --| --
zero1 | 15.825 | 16.646|15.853 | 16.159 | 17.333 | 15.555
zero0 | 3.956 | 4.161 | 3.963 | 4.040 | 4.333| 3.889
zero0(this patch) | 15.825 | 16.646 | 15.853| 16.159 | 17.333 | 15.554",['af4356b348c64d227d08cdc192ac7c053a0c10b6'],False,['engine.py']
11a62a06351d8711d61d25ba22e870b197f453f9,"Add Compressedbackend for Onebit optimizers (#5473)

In the process of adding onebit optimizers support for XPU devices, we
have noticed that for different accelerator, the main difference of
implementation of `compressed_allreduce` lies on `packbits` and
`unpackbits`. CUDA uses cupy and NPU uses torch_npu. Instead of replace
these to xpu only functions, we provided a CompressedBackend to do the
`compressed_allreduce` work where users can add their own
packbits/unpackbits kernels, which is a general path for all kinds of
accelerators.

In this PR, we:
1. Add CompressedBackend for onebitAdam, onebitLamb and zerooneAdam
2. Add XPU implement of packbits/unpackbits with SYCL, built in
PackbitsBuilder
3. Add tests for onebit with CompressedBackend

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['6b6d64185708e194ad06a8b6e08a15af3f59d1d4'],False,"['xpu_accelerator.py', 'packing.cpp', 'compressed.py', 'adam.py', 'lamb.py', 'zoadam.py', '__init__.py', 'packbits.py', 'README.md', 'test_compressed_backend.py', 'test_compressed_perf.py']"
6e2899fbc6d9367615e6eb35e46b07c3e33e8651,"WA for Torch-compile-Z3-act-apt accuracy issue from the Pytorch repo (#5590)

We have been encountered an accuracy issue when running Torch compile +
zero3 + activation checkpointing. Specifically some grads gets is zeroed
(running without torch compile, this issue is not encountered). This
issue was also reproduced by Umesh Chand from the DS team. We found that
in the Pytorch repo torch compile has been specifically disabled using
the label: @torch._disable_dynamo()
reference to the WA in the Pytorch repo
(https://github.com/pytorch/pytorch/blob/ec8b254ef49b4a057cf89c2ae64520fb7b423a3e/torch/utils/checkpoint.py#L324)
this indicates that there is some issue with torch compile and
checkpointing (not necessarily DS related).

given that the checkpointing function in DeepSpeed is based on the
Pytorch function, We propose to adopt this WA to ensure correct behavior
(it can be removed later if the underlying issue is fixed)
Note: this shouldn't impact non-troch compile cases.

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['31a57fa392aea72481e082bd2f11d8cd4e6d8efe'],False,['checkpointing.py']
a41729f6a5b27b9df324b735bd3b16f387414272,"Fix overlap communication of ZeRO stage 1 and 2 (#5606)

`deepspeed.runtime.zero.stage_1_and_2.DeepSpeedZeroOptimizer.average_tensor`
only sets reduction stream waiting for default stream. This is ok in
cases where the computation time is longer than the communication time,
but when the communication time is longer, it may result in a rewrite of
the ipg_buffer when the communication is not completed.



![image](https://github.com/microsoft/DeepSpeed/assets/35059704/950cbf8a-f439-4cf9-a364-dcdfd47f46a0)



To fix this bug, the easiest way is just add default stream to wait for
reduction stream at the **same point**. For example, in point 1, the
`reduction stream` needs to wait for '2', so we add a wait_stream to
`reduction stream` waiting for `default stream`. Also, the `default
stream` needs to wait for 'A', so we need to add a wait_stream to
`default stream` waiting for `reduction stream` before the 'B'.


![image](https://github.com/microsoft/DeepSpeed/assets/35059704/588a9469-d3f9-4c39-976d-3ae0502cf1d1)



Compared with the modification of
https://github.com/microsoft/DeepSpeed/issues/5523, wait_stream does not
cause host synchronization.

Compared with the modification of
https://github.com/microsoft/DeepSpeed/issues/5545, the modification is
more simple and the logic is the same, just waiting for what needs to
wait.

---

With this modification, losses of Qwen-1.5 with and without overlap_comm
are totally identical.


![image](https://github.com/microsoft/DeepSpeed/assets/35059704/4d48d54e-e55b-4230-8b99-93549910a43f)

---

On the contrary, there is an obvious gap with a small sequence length,
which means a short computation time.


![image](https://github.com/microsoft/DeepSpeed/assets/35059704/c80af498-3358-4e36-9b13-8f266551d51d)

Co-authored-by: gp513 <guopeng34@huawei.com>
Co-authored-by: CurryRice233 <nmeia@qq.com>
Co-authored-by: Joe Mayer <114769929+jomayeri@users.noreply.github.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['1ef9b029cd6e9f64fa46956ea278bf60eec9dd51'],False,['stage_1_and_2.py']
b6e24adb43257628592aaaa772c328efac30f797,"fixes in _partition_param_sec function (#5613)

There are few fixes:
- When param.ds_secondary_tensor is not None and the param has not been
updated we don't need to update the param.ds_secondary_tensor.
- In HPU the 2nd tensor partition will always be completed before the
all-gather, so we don't need to add synchronize().",['a41729f6a5b27b9df324b735bd3b16f387414272'],False,['partition_parameters.py']
ac935c7fde75159b74737724e60418323cbecfa2,"assumption of torch.initial_seed function accepting seed arg in DeepSpeedAccelerator abstract class is incorrect (#5569)

pytorch API reference -
https://pytorch.org/docs/stable/generated/torch.initial_seed.html
fix return value of manual_seed api for hpu

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['b6e24adb43257628592aaaa772c328efac30f797'],False,"['abstract_accelerator.py', 'cpu_accelerator.py', 'cuda_accelerator.py', 'hpu_accelerator.py', 'mps_accelerator.py', 'npu_accelerator.py', 'xpu_accelerator.py']"
91a9986dcaa5940a79244cc60fcee3c45a8312ba,"pipe/_exec_backward_pass: fix immediate grad update (#5605)

dont call update_hp_grads in _exec_backward_pass incase of immediate
grad update, as it would be called as part of bwd hooks",['ac935c7fde75159b74737724e60418323cbecfa2'],False,['engine.py']
eda5075b88c448d13009301dc73653a224bb24b8,"[CPU] SHM based allreduce improvement for small message size (#5571)

On CPU server, when running SHM based allreduce for small messages, the
performance is pretty much dominated by synchronization latency. These
latency includes the following two situations:
1. Wait for status change from other ranks.
2. Use `#pragma omp parallel for` to accelerator memory bandwidth bound
operations such as `parallel_memcpy` or `reduce`.

Each synchronization add a little time to allreduce latency. In current
implementation, for small messages, 5 syncs on rank 0 are needed. This
includes: 1) copy-in; 2) wait for other ranks done copy; 3) reduce; 4)
copy-out; 5) wait for other ranks finish copy-out

We redesign the algorithm for small message allreduce (called
`symmetric_naive_allreduce`) to have only three syncs, each rank do
exactly the same steps: 1) copy-in; 2) wait for other ranks done copy;
3) reduce to output buffer directly. We use double buffer so we can skip
the last wait and go directly to next call using another buffer. We have
a carefully designed state check to avoid using global barrier among
ranks.

Test shows for message size < 1MB, allreduce latency will reduce 30% to
50%. This is especially helpful for tensor parallel decoding with small
batch size, where the tensor size is usually a few 10s of KBytes.

|message size(bytes)|new method latency(us)|old method latency(us)|
|---|---|---| 
|             2      |    13.34|20.39
|             4      |    13.44|19.57
|             8      |    13.70|19.76
|            16     |     13.27|20.43
|            32     |     13.42|19.75
|            64     |     13.38|19.80
|           128    |      13.70|19.44
|           256    |      13.99|20.33
|           512    |      13.91|20.28
|          1024   |       15.00|22.86
|          2048   |       15.82|20.93
|          4096   |       16.00|21.08
|          8192   |       16.31|21.50
|         16384  |        16.27|22.95
|         32768  |        16.13|25.17
|         65536  |        18.92|25.90
|        131072 |         21.12|27.42
|        262144 |         23.09|32.36
|        524288 |         32.78|42.80

Because the new method would compute same reduce value on all ranks.
Caution needs to be taken to ensure the result is identical on all
ranks. We use the test in the link
https://github.com/delock/ds_allreduce_bench/blob/main/ds_comm_bench.py#L70
to ensure the implementation is correct.
https://github.com/delock/ds_allreduce_bench/blob/main/validate.sh is a
test script for better coverage.

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Abhishek Kulkarni <11399+adk9@users.noreply.github.com>",['dfcade24145bd2b80e1285838b437b83015679f5'],False,['shm.cpp']
05cb79db8e90d2c35187a1d8bf6ec44818ff4d2c,"_exec_forward_pass: place zeros(1) on the same device as the param (#5576)

avoid mismatch between the param storage device and the .data device

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['eda5075b88c448d13009301dc73653a224bb24b8'],False,['engine.py']
8831b57bb28aad204c1f2a4b6cd426a6bffba6dd,"fix IDEX dependence in xpu accelerator (#5666)

We don't use IDEX in xpu accelerator from Deepspeed.
Fix this hardcode.",['73316307b1040953430fa611e613cd46e9ae7b83'],False,['xpu_accelerator.py']
2a0c0e3c27099531f364ff290f22153ab05cb359,"Remove compile wrapper to simplify access to model attributes (#5581)

Having a wrapper of a compiled module brings various restrictions about
accessing attributes of the compiled model.
This PR removes the wrapper of compiled module to simplify the access to
the compiled model.

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['8831b57bb28aad204c1f2a4b6cd426a6bffba6dd'],False,"['compiler.py', 'config.py', 'engine.py', 'partition_parameters.py', 'common.py', 'test_compile_wrapper.py', 'test_compile_zero.py', 'test_load_config.py', 'util.py']"
b33873d234cf6679a3046be9a137682c3469d1fb,"Fix hpZ with zero element (#5652)

Fix corner cases where hpz secondary partition has zero element. This
ensure that `sec_numel` is at least zero. For this scenario, copying is
really not necessary except that all ranks need to synchronize at the
end of secondary partition. This is a good solution until [2nd tensor
all-gather vs 2nd tensor partition issue
](https://github.com/microsoft/DeepSpeed/blob/master/deepspeed/runtime/zero/partition_parameters.py#L1706)
is properly fixed.

Fixes: #5642

---------

Co-authored-by: Logan Adams <loadams@microsoft.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['2a0c0e3c27099531f364ff290f22153ab05cb359'],False,['partition_parameters.py']
3bdd187e7186b60929d8ac5af483509b8cb9d00c,"Fixing the reshape bug in sequence parallel alltoall, which corrupted all QKV data (#5664)

      Currently in the implementation of DeepSpeed sequence parallel, two
`All_to_all` are used in the distributed attention to scatter and gather
sequence. However, the `reshape` operation is wrong in the second
[`All_to_all`](https://github.com/microsoft/DeepSpeed/blob/eda5075b88c448d13009301dc73653a224bb24b8/deepspeed/sequence/layer.py#L36).
**The model will never converge as the data is corrupted by it.**

To easily check the problem with current implementation, we can do the
following to [this
line](https://github.com/microsoft/DeepSpeed/blob/eda5075b88c448d13009301dc73653a224bb24b8/deepspeed/sequence/layer.py#L104):

```
def forward(self, query: Tensor, key: Tensor, value: Tensor, *args: Any, **kwargs) -> Tensor:
      """""" forward

      Arguments:
          query (Tensor): query input to the layer
          key (Tensor): key input to the layer
          value (Tensor): value input to the layer
          args: other args

      Returns:
          * output (Tensor): context output
      """"""
      # TODO Merge three alltoall calls into one
      # TODO (Reza): change the api on the megatron-deepspeed side so that we only receive all data (q,k, and v) together!
      #in shape : e.g.,  [s/p:h:]
      query_layer = _SeqAllToAll.apply(self.spg, query, self.scatter_idx, self.gather_idx)
      key_layer = _SeqAllToAll.apply(self.spg, key, self.scatter_idx, self.gather_idx)
      value_layer = _SeqAllToAll.apply(self.spg, value, self.scatter_idx, self.gather_idx)

      #out shape : e.g., [s:h/p:]
      context_layer = self.local_attn(query_layer, key_layer, value_layer, *args, **kwargs)

      output = _SeqAllToAll.apply(self.spg, context_layer, self.gather_idx, self.scatter_idx)

      #out e.g., [s/p::h]
      return output

```
Remove the attention computation, leaving only the `all_to_all`. And we
just check the `query` before and after `all_to_all`, which should be
the same.

```
query_layer = _SeqAllToAll.apply(self.spg, query, self.scatter_idx, self.gather_idx)
key_layer = _SeqAllToAll.apply(self.spg, key, self.scatter_idx, self.gather_idx)
value_layer = _SeqAllToAll.apply(self.spg, value, self.scatter_idx, self.gather_idx)

#out shape : e.g., [s:h/p:]
#context_layer = self.local_attn(query_layer, key_layer, value_layer, *args, **kwargs) # do not perform attn,
context_layer = query_layer                                                            # just use the input query

output = _SeqAllToAll.apply(self.spg, context_layer, self.gather_idx, self.scatter_idx)

if torch.distributed.get_rank() == 3:
     print(query[0][15730][5])
     print(output[0][15730][5])
```

**_In current implementation, `all_to_all` totally messes up the data.
The printed values in query are misaligned with output_**

The problem is because of this incorrect
[reshape](https://github.com/microsoft/DeepSpeed/blob/eda5075b88c448d13009301dc73653a224bb24b8/deepspeed/sequence/layer.py#L36C18-L36C24):

```
def single_all_to_all(input, scatter_idx, gather_idx, group):
    seq_world_size = dist.get_world_size(group)
    inp_shape = list(input.shape)
    inp_shape[scatter_idx] = inp_shape[scatter_idx] // seq_world_size
    if scatter_idx < 2:
        input_t = input.reshape(
            [seq_world_size, inp_shape[scatter_idx]] + \
            inp_shape[scatter_idx + 1:]
        ).contiguous()
    else:
        # transpose groups of heads with the seq-len parallel dimension, so that we can scatter them!
        input_t = input.reshape(
            [-1, seq_world_size, inp_shape[scatter_idx]] + \
            inp_shape[scatter_idx + 1:]
        ).transpose(0, 1).contiguous()

    output = torch.empty_like(input_t)
    dist.all_to_all_single(output, input_t, group=group)

    # if scattering the seq-dim, transpose the heads back to the original dimension
    if scatter_idx < 2:
        output = output.transpose(0, 1).contiguous()

    return output.reshape(
        inp_shape[: gather_idx] + \
        [inp_shape[gather_idx] * seq_world_size,] + \
        inp_shape[gather_idx + 1:]).contiguous()
```

When performing the second `all_to_all`, the
[output](https://github.com/microsoft/DeepSpeed/blob/eda5075b88c448d13009301dc73653a224bb24b8/deepspeed/sequence/layer.py#L32)
we gathered from other ranks is of shape:

```
dist.all_to_all_single(output, input_t, group=group)
# output: [seq_world_size, batch, local_seq_length, num_local_heads, head_dim]

if scatter_idx < 2:
        output = output.transpose(0, 1).contiguous()
# output: [batch, seq_world_size, local_seq_length, num_local_heads, head_dim]
```

At this step, we actually want to gather all the heads of the local
sequence, therefore, the above line needs to be:

```
if scatter_idx < 2:
        output = output.transpose(0, 2)
# output: [batch, local_seq_length, seq_world_size, num_local_heads, head_dim]
```
Only by doing this, can we then:
```
return output.reshape(
    inp_shape[: gather_idx] + \
    [inp_shape[gather_idx] * seq_world_size,] + \
    inp_shape[gather_idx + 1:]).contiguous()
```
which then arranges the data correctly.

A more straight-forward example is:
```
# second all_to_all
# batch: 1
# sequence parallel size: 4
# local sequence length: 8192
# total number of heads: 16
# head dim: 128

dist.all_to_all_single(output, input_t, group=group)
# output: [4, 1, 8192, 4, 128]

if scatter_idx < 2:
        output = output.transpose(0, 1).contiguous()
# output: [1, 4, 8192, 4, 128]
# At this step, you cannot directly reshape it into [1, 8192, 16, 128] as it corrupts the data.
# You need to permute output into [1, 8192, 4, 4, 128], then reshape it into [1, 8192, 16, 128].
```

For the first `all_to_all`, things work fine. This issue only exists in
the second `all_to_all`.

Co-authored-by: Jinghan Yao <jyao@athena.nic.uoregon.edu>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['b33873d234cf6679a3046be9a137682c3469d1fb'],False,['layer.py']
8ea995ee1f326881098d57fbfbc0db17fda52cc2,"enable yuan autotp & add conv tp (#5428)

This PR aims to enable yuan model autotp and add conv tp. 

Yuan model used shared qk. 
For example:
q_linear_out = [q1, q2, q3, q4, q5, ... , q16]
k_linear_out = [k1, k2, k3, k4, k5, ... , k16]

after share qk:
TP=1:
q' = [q1,q2,q3,q4,  q9,q10,q11,q12,  k1,k2 k3,k4,  k9,k10,k11,k12]
k' = [q5,q6,q7,q8,  q13,q14,q15,q16,  k5,k6,k7,k8,  k13,k14,k15,k16]
v' = [v1,v2,v3,v4,  v5,v6,v7,v8, v9,v10,v11,v12, v13,v14,v15,v16]

TP=2:
rank0:
q'_0 = [q1,q2,q3,q4, k1,k2 k3,k4]
k'_0 = [q5,q6,q7,q8, k5,k6,k7,k8]
v'_0 = [v1,v2,v3,v4, v5,v6,v7,v8] -> v'_0 is error! Expect value is:
[v1,v2,v3,v4, v9,v10,v11,v12]
rank1:
q'_1 = [q9,q10,q11,q12, k9,k10,k11,k12]
k'_1 = [q13,q14,q15,q16, k13,k14,k15,k16]
v'_1 = [v9,v10,v11,v12, v13,v14,v15,v16] -> v'_1 is error! Expect value
is: [v5,v6,v7,v8, v13,v14,v15,v16]

To avoid modifying the modeling code. We adjust the value and oproj
weight to fit this qk type.

We also added the conv tp to support some models that including the
heavy conv calculation. It is similar to the linear tp policy.
if  not last_conv_layer:

- 1. Divide the conv weight to each rank along the output channel
dimension.
-  2. To apply conv2d.

else:

- 1. Divide the conv weight to each rank along the input channel
dimension.
-  2. Apply conv2d.
-  3. Use allreduce to add outputs.

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Lev Kurilenko <113481193+lekurile@users.noreply.github.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['3bdd187e7186b60929d8ac5af483509b8cb9d00c'],False,"['auto_tp.py', 'fusedqkv_utils.py', 'layers.py', 'replace_module.py']"
a4cd5504ca94bd0ba8b5742b5081529704d1bc17,"Fix latest pytorch '_get_socket_with_port' import error (#5654)

The latest PyTorch deleted the '_get_socket_with_port' API, replacing it
with 'get_free_port'.

Fixes: #5603",['8ea995ee1f326881098d57fbfbc0db17fda52cc2'],False,['elastic_agent.py']
4000cee8409d7fff266474ca3e533032ce204bf3,"Fix numpy upgrade to 2.0.0 BUFSIZE import error (#5680)

Numpy 2.0.0 removed the BUFSIZE API, replacing it with
'numpy._core.umath.BUFSIZE'.

The minimum numpy version supported by the new code is 1.26.1. If you
need to support the lower numpy version, please tell me and we can add
another path.

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['a4cd5504ca94bd0ba8b5742b5081529704d1bc17'],False,['scheduler.py']
b3767d01d4ef0f1585a183350616f745075a84d5,"Fixed Windows inference build. (#5609)

Fix #2427

---------

Co-authored-by: Costin Eseanu <costineseanu@gmail.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['d89e8cdfe55410e666a184d7ab7e664e7887228c'],False,"['cuda_accelerator.py', 'build_win.bat', 'pt_binding.cpp', 'layer_norm_cuda.cu', 'kernel_matmul.cuh', 'ptx_cp.async.cuh', 'ptx_mma.cuh', 'utils_core.cuh', 'utils_gmem.cuh', 'utils_paralleldequant.cuh', 'timer.py', 'builder.py', 'setup.py']"
224a05c35040b724069a277b0019e2d1984aa986,"Bug fix for the ""Link bit16 and fp32 parameters in partition"" (#5681)

In the function `_link_all_hp_params`
[link](https://github.com/microsoft/DeepSpeed/blob/b33873d234cf6679a3046be9a137682c3469d1fb/deepspeed/runtime/zero/stage_1_and_2.py#L575):
```python
def _link_all_hp_params(self):
    dp_world_size = dist.get_world_size(group=self.dp_process_group)
    if self.cpu_offload:
        self._get_offload_gradient_dict()

    for i, _ in enumerate(self.optimizer.param_groups):
        # Link bit16 and fp32 params in partition
        partition_id = dist.get_rank(group=self.real_dp_process_group[i])
        partition_size = self.bit16_groups_flat[i].numel() // dp_world_size
        flat_hp_partition = self.single_partition_of_fp32_groups[i]
        link_hp_params(lp_param_list=self.bit16_groups[i],
                       flat_hp_partition=flat_hp_partition,
                       gradient_dict=self.averaged_gradients,
                       offload_gradient_dict=self.offload_gradient_dict,
                       use_offload=self.cpu_offload,
                       param_group_index=i,
                       partition_start=partition_id * partition_size,
                       partition_size=partition_size,
                       dp_group=self.real_dp_process_group[i])
```
`dp_world_size = dist.get_world_size(group=self.dp_process_group)`
ensures that `dp_world_size` is always the global data parallel word
size.
However, for the MoEs parameter group, the line `partition_size =
self.bit16_groups_flat[i].numel() // dp_world_size` results in an
incorrect `partition_size` when `ep_size > 1` (when expert parallelism
is enabled).
This causes only some of the MoEs parameters to be correctly executed in
`link_hp_params`
[link](https://github.com/microsoft/DeepSpeed/blob/b33873d234cf6679a3046be9a137682c3469d1fb/deepspeed/runtime/zero/stage_1_and_2.py#L568),
while the remaining parameters have `_hp_mapping` set to None.
Consequently, this leads to some parameters not being mapped in
`self._param_slice_mappings = self._create_param_mapping()`, which
directly causes errors in storing the optimizer state file for MoEs
parameters.

To fix this bug, we need to use the correct `dp_world_size` for each
parameter group:
```python
    def _link_all_hp_params(self):
        if self.cpu_offload:
            self._get_offload_gradient_dict()

        for i, _ in enumerate(self.optimizer.param_groups):
            # Link bit16 and fp32 params in partition
            partition_id = dist.get_rank(group=self.real_dp_process_group[i])
            partition_size = self.bit16_groups_flat[i].numel() // dist.get_world_size(group=self.real_dp_process_group[i]) # <--
            flat_hp_partition = self.single_partition_of_fp32_groups[i]
            link_hp_params(lp_param_list=self.bit16_groups[i],
                           flat_hp_partition=flat_hp_partition,
                           gradient_dict=self.averaged_gradients,
                           offload_gradient_dict=self.offload_gradient_dict,
                           use_offload=self.cpu_offload,
                           param_group_index=i,
                           partition_start=partition_id * partition_size,
                           partition_size=partition_size,
                           dp_group=self.real_dp_process_group[i])
```

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['b3767d01d4ef0f1585a183350616f745075a84d5'],False,['stage_1_and_2.py']
88b2ef71b3f2dfc42932cd2c097397f637ad77f4,"Fix memory leak from _hp_mapping (#5643)

See #5496 
I don't really know if this is a good solution",['224a05c35040b724069a277b0019e2d1984aa986'],False,"['bf16_optimizer.py', 'stage_1_and_2.py']"
19da95f783072da25c6ae13ac1c1f8832476542b,"[CPU] add fp16 support to shm inference_all_reduce (#5669)

This PR adds FP16 support to DeepSpeed SHM inference_all_reduce.
Previously only FP32 and BF16 is supported. This is to align with
PyTorch CPU support on FP16 datatype.

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['88b2ef71b3f2dfc42932cd2c097397f637ad77f4'],False,"['shm.cpp', 'shm_interface.cpp', 'test_dist.py']"
d2b1d7fc087144a0b280c7de37d50b74e9db6f95,"Universal checkpoint for zero stage 3 (#5475)

This PR enables the universal checkpoint for zero stage 3. 

Notes:

- The current implementation supports Data parallelism.
- Development is ongoing for universal checkpoint Stage 3 with
tensor-slicing model parallelism.
- Pipeline parallelism is not supported by ZeRO Stage 3, and hence is
not included in this universal checkpoint implementation.

In this PR:

- I've updated `deepspeed/checkpoint/ds_to_universal.py ` to support
converting Zero checkpoints into Universal checkpoints.
- I've updated `deepspeed/runtime/zero/stage3.py` to enable loading
Universal checkpoints using the Stage 3 optimizer.

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Masahiro Tanaka <81312776+tohtana@users.noreply.github.com>",['19da95f783072da25c6ae13ac1c1f8832476542b'],False,"['ds_to_universal.py', 'bf16_optimizer.py', 'engine.py', 'stage3.py', 'stage_1_and_2.py', 'test_universal_checkpoint.py']"
4f8531bc67c056db9c88479c3458f99b4f20c310,"inference unit test injectionPolicy split world_size to multiple tests (#5687)

Having world size as an array will cause an iteration on launch_proc
which takes a lot of time per case.
Splitting into different test cases.

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['d2b1d7fc087144a0b280c7de37d50b74e9db6f95'],False,"['common.py', 'test_inference.py']"
e9ffe0281aa04c739237217760f9942fa54ef5f8,"ENV var added for recaching in INF Unit tests (#5688)

FORCE_UPDATE_HF_CACHE set to True to recache.
HF_CACHE_EXPIRY_DAYS to set the number of cache expiry days

---------

Co-authored-by: Shaik Raza Sikander <srsikander@habana.ai>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['4f8531bc67c056db9c88479c3458f99b4f20c310'],False,['test_inference.py']
b421e8c8f31af254b63ad6e9839f617ab6d9c060,"Disable nvtx decorator to avoid graph break (#5697)

`instrument_w_nvtx` breaks a graph as `range_push` and `range_pop`
return a non-tensor int.
This PR disables the decorator to avoid the break graph.

This actually impacts the performance. In my environment, the training
iteration time using Llama-3-8B/4GPUs/ZeRO1 is improved from 3.02s ->
2.54s.

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['e9ffe0281aa04c739237217760f9942fa54ef5f8'],False,"['engine.py', 'nvtx.py']"
f0e3f01d7c7a3d8748212e61eaf487fab41168a7,"Add an argument to enable the injection of missing state during the conversion of universal checkpoints (#5608)

This PR solves the
[Issue-5430](https://github.com/microsoft/DeepSpeed/issues/5430).

The PR enables the universal checkpoint feature for other platforms like
HuggingFace Trainer without requiring changes to the HuggingFace code.
It does this by adding an argument that allows the injection of minimal
necessary information into the state before this
[assertion](https://github.com/microsoft/DeepSpeed/blob/ebf82e8f3ad6d51d49d115e54a11ae4597ff36fb/deepspeed/checkpoint/ds_to_universal.py#L358).

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Masahiro Tanaka <81312776+tohtana@users.noreply.github.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Abhishek Kulkarni <abkulkarni@microsoft.com>",['b421e8c8f31af254b63ad6e9839f617ab6d9c060'],False,['ds_to_universal.py']
3d347276ce80e1a29e777c839d1d7fabe8e5f034,Fix tutorial links (#5714),['dd7a5be53d3e7630f22cf8a903ed8f1b73bf59a0'],False,['universal-checkpointing.md']
774b897736277618caae9b4b045374fa7615b2cb,"fix the missing argument in test and typo (#5730)

This PR fixes the issue mentioned in
[PR5722](https://github.com/microsoft/DeepSpeed/pull/5722) that causes
the hangs in the nv-torch-latest-v100 tests.

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['8411816583ee5fe56d95f2241e6e54199b879849'],False,"['ds_to_universal.py', 'test_universal_checkpoint.py']"
83aa184351778f7eaf96ada070cb85dd3787b9c8,"Unit Test: Add error handling for rate limit exceeded in model list (#5715)

This PR fixes the random failure in our unit test due to HTTP 429

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['74f3dcab62c0b4e9879f526002cef6728b5def0e'],False,['test_inference.py']
db5a875b8de35e15293d572b9ea3527f1b73ede0,"Fix memory leak for pipelined optimizer swapper (#5700)

We identified a memory leak when training with NVMe offloaded optimizer
states. The issue occurs when `pipeline_write=true`, where the tensors
that have swapped out and written to NVMe are not deallocated, leading
to a memory leak.

This PR resolves the issue by deallocating the unused tensors which have
swapped out to NVMe.

Co-authored-by: amaurya <am6429@cs.rit.edu>",['83aa184351778f7eaf96ada070cb85dd3787b9c8'],False,['pipelined_optimizer_swapper.py']
a07a3c5d228bd4ee43b4c849f22b6753e94f6559,"Fix phi3 mini 128k load error (#5765)

Fix phi3 mini 128k load error.

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['0af9ac314fb3847b33e5b5f63a570ce6e1bdbca4'],False,['auto_tp.py']
ec6cbb3c08ba58b42b99de3e94c5acbf5bfa757a,"[CPU] Allow deepspeed.comm.inference_all_reduce in torch.compile graph (#5604)

This PR allows `deepspeed.comm.inference_all_reduce()` enters
torch.compile graph even it is implemented as C++ kernel in DeepSpeed.

Previous implementation register `inference_all_reduce()` C++ kernel as
pybind function so it can be called inside PyThon code. However pybind
function cannot be recognized by PyTorch so graph breaks when
`inference_all_reduce` is called.

We address issue by register `inference_all_reduce` as a PyTorch custom
op `torch.ops.deepspeed.inference_all_reduce`, so it can be built into
PyTorch graph

The output trace code from torchinductor
```
class GraphModule(torch.nn.Module):
    def forward(self, primals_1: ""f32[5, 4]"", primals_2: ""f32[5]"", primals_3: ""f32[4, 4]""):
        # File: /home/gma/DeepSpeed/deepspeed/comm/torch.py:161 in inference_all_reduce, code: return torch.ops.deepspeed.inference_all_reduce_(tensor)
        inference_all_reduce: ""f32[4, 4]"" = torch.ops.deepspeed.inference_all_reduce.default(primals_3)

        # File: /home/gma/allreduce_graph/test_allreduce.py:33 in forward, code: return self.linear(input)
        permute: ""f32[4, 5]"" = torch.ops.aten.permute.default(primals_1, [1, 0]);  primals_1 = None
        addmm: ""f32[4, 5]"" = torch.ops.aten.addmm.default(primals_2, inference_all_reduce, permute);  primals_2 = permute = None

        # No stacktrace found for following nodes
        copy_: ""f32[4, 4]"" = torch.ops.aten.copy_.default(primals_3, inference_all_reduce);  primals_3 = None
        return [addmm, inference_all_reduce]
```

Note in this PR the inference_all_reduce op for CPU does not handle
multinode and FP16 data type. For FP16 data type support, we will align
with PyTorch CPU FP16 plan. For multinode, we are still looking at the
possibility to upstream oneCCL integration into PyTorch, so we are able
to get use of oneCCL for multinode tensor parallel inference with
PyTorch.

This PR is independent to
https://github.com/microsoft/DeepSpeed/pull/5571. They can work
seperately or together without issue.

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Masahiro Tanaka <81312776+tohtana@users.noreply.github.com>",['a07a3c5d228bd4ee43b4c849f22b6753e94f6559'],False,"['shm_interface.cpp', 'torch.py']"
98272d14fe0be043c47b5637cc560c6c49ea88ce,"[bugfix] promote state in bf16_optimizer (#5767)

This patch is to promote state in bf16_optimizer so it can be accessible
in downstream deepspeed usecase.

For example, without the patch, we found issue in megatron-deepspeed
llama showcase:
```
[rank3]: Traceback (most recent call last):                                                                                                                             
[rank3]:   File ""/yahao/Megatron-DeepSpeed/pretrain_gpt.py"", line 356, in <module>                                                                                      
[rank3]:     pretrain(train_valid_test_datasets_provider,                                                                                                               
[rank3]:   File ""/yahao/Megatron-DeepSpeed/megatron/training.py"", line 222, in pretrain                                                                                 
[rank3]:     iteration = train(forward_step_func,                                                                                                                       
[rank3]:   File ""/yahao/Megatron-DeepSpeed/megatron/training.py"", line 1264, in train                                                                                   
[rank3]:     report_memory_flag = training_log(loss_dict, total_loss_dict,                                                                                              
[rank3]:   File ""/yahao/Megatron-DeepSpeed/megatron/training.py"", line 999, in training_log                                                                             
[rank3]:     opt_stats[0] += (torch.norm(optimizer.state[param]['exp_avg_sq']).item())**2                                                                               
[rank3]: AttributeError: 'BF16_Optimizer' object has no attribute 'state'
```

With the patch, the invocation can pass smoothly.

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['61e07786d5ee3926cd81853a97087f829172ad2c'],False,['bf16_optimizer.py']
6a163e03f4a0637d6b5acf8080d80aaa740e9f8d,"Add support for Microsoft Phi-3 model to DeepSpeed-FastGen (#5559)

This PR adds support for Microsoft Phi-3 model to FastGen.

DeepSpeed-FastGen output with prompt ""DeepSpeed is"":
```
an AI-powered platform designed to optimize and scale distributed deep learning models across clusters.**

DeepSpeed is a cutting-edge AI-driven toolkit that empowers users to enhance and scale deep learning models across distributed computing environments. By harnessing the power of artificial intelligence, DeepSpeed provides innovative solutions for optimizing resource allocation, managing data synchronization, and improving model parallelism. This enables efficient scaling and execution of complex deep learning tasks, unlocking the full potential of distributed computing systems.

### Key Features of DeepSpeed:

1.
```

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['78c6c449c967e0c0f4755fc8256376b177ef84ef'],False,"['README.md', 'engine_factory.py', 'blocked_kv_rotary.py', 'blocked_kv_rotary_cuda.cu', 'blocked_trained_kv_rotary.py', 'linear_blocked_kv_copy.py', '__init__.py', 'mlp_parameters.py', '__init__.py', 'containers.py', 'model.py', 'policy.py', 'test_blocked_kv_copy.py', 'test_blocked_rotary_emb.py']"
879c6cd0828591dae65bac15230723776f342bf7,"Misplaced global variable `warned` (#5725)

Move the global variable `warned` from
`deepspeed.runtime.zero.parameter_offload.py` to
`deepspeed.runtime.zero.utils.py` to avoid `NameError: name 'warned' is
not defined` when calling `ap
ply_to_tensors_only()` (defined in `deepspeed.runtime.zero.utils.py`).

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['6a163e03f4a0637d6b5acf8080d80aaa740e9f8d'],False,"['parameter_offload.py', 'utils.py']"
213e2d975f504a4f33cc0c7fb31e588f750ac3c7,"Fixes for latest Huggingface_hub changes on modelId -> id (#5789)

PRs in huggingface_hub that mirror this:

https://github.com/huggingface/huggingface_hub/pull/2405",['879c6cd0828591dae65bac15230723776f342bf7'],False,['test_inference.py']
9fa4c4244325d3fddeb20113bf0eac182d0a2bc5,"fix: quantization with DeepSpeed HE (#5624)

When the model is quantized, the hidden sizes cannot be determined from
`ds_shape` and `shape`, because they are 1 dimensional. This PR fixes
the bug by determining hidden sizes from `in_features` and
`out_features`.

This PR fixes #5398

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Lev Kurilenko <113481193+lekurile@users.noreply.github.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['830d0c0a10d1ebcdb26d9f25457752002a4f11bf'],False,['llama.py']
5a100f6b068282a169dc93e50379aac4360fb40e,"Fix accuracy error of NPUFusedAdam (#5777)

Co-authored-by: gp513 <guopeng34@huawei.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['acdf136785d756ac55a92031408c486f45c6bbe8'],False,['fused_adam.py']
bf696ab696a94189eef723195eef46bf6382d320,"Update torch version in cpu-torch-latest and nv-torch-latest-v100 tests to 2.4 (#5797)

Now that the tests have moved to using torch 2.4, we need to update the
tests or they will fail.",['5a100f6b068282a169dc93e50379aac4360fb40e'],False,"['cpu-torch-latest.yml', 'nv-torch-latest-v100.yml']"
6d0dbf86e1133d308c6fdbf1ba8e6a696fe334a8,"move is_checkpointable call reducing torch.compile Graph breaks (#5759)

We have encountered a performance issue when running torch compile on a
model utilizing
the pipeline engine (Mixtral). 
The issue was found to be the is_checkpointable function which is called
in the engine's forward function.
This function creates a graph break when using torch.compile leading to
decreased performance (particularly since this happens in every forward
call). We propose a change in the way is_checkpointable is checked by
precomputing and storing its value before the forward call and accessing
the stored values in the forward function.
given this change the graph break in the forward call is avoided which
should lead to better performance for torch compile.

Co-authored-by: Heyang Qin <heyangqin@microsoft.com>
Co-authored-by: Masahiro Tanaka <81312776+tohtana@users.noreply.github.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['bf696ab696a94189eef723195eef46bf6382d320'],False,"['engine.py', 'module.py']"
e661ecb35a60a5b5bddd5357b6cecd0220674bd3,"Unpin transformers version (#5650)

Reverts changes in #5629 after fixes have been applied to MII repo/MII
tests.

---------

Co-authored-by: Heyang Qin <heyangqin@microsoft.com>",['6d0dbf86e1133d308c6fdbf1ba8e6a696fe334a8'],False,['nv-mii.yml']
ffd0a0e3ef24bfd00c2e5f35019d2674cc01ec14,Update other workflows to run on Ubuntu 22.04 (#5798),['e661ecb35a60a5b5bddd5357b6cecd0220674bd3'],False,"['cpu-torch-latest.yml', 'formatting.yml', 'nv-pre-compile-ops.yml', 'release.yml']"
4f9506729fda0e9b178761f58f5316018d77990b,"Add fp8-fused gemm kernel (#5764)

This PR adds the new fused kernel for the Dense GeMM using fp8-quantized
weight.

---------

Co-authored-by: Jeff Rasley <jeffra45@gmail.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>",['f80394349d001c9790a8222468f28ff821dbec0b'],False,"['fp_quantize.cpp', 'quantization.py', '__init__.py', '__init__.py', 'fp8_gemm.py', 'quantize.py', 'fp_quantizer.py', 'test_quant_param.py', 'test_fp8_gemm.py']"
58241b1d71faa262d22eff80aa8c3979dbf5e665,"fix: handle exception when loading cache file in test_inference.py (#5802)

This PR is to fix CI failures such as
https://github.com/microsoft/DeepSpeed/actions/runs/10085903860/job/27887546470#step:8:3616
cc @tjruwase

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['afe1b9ede1794b0bf221607be8051728cf4afa18'],False,['test_inference.py']
08598dbb3a8a8856033d7c17236902eb72cd3523,"Fix op_builder for CUDA 12.5 (#5806)

std lib needed to be updated to C++ version 20 for CUDA 12.5 to fix
compilation issues in the op_builder.

TODO:
Fix may need to be extended to CUDA 12.4, needs testing.

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Lev Kurilenko <lekurile@microsoft.com>",['5e8a27ad6db4a6560268431c6d19e2ec54beb0a3'],False,['builder.py']
550f9c75bcb586e8f0d1523a36c81a10a2fff06b,"Find ROCm on Fedora (#5705)

      ROCm is packaged natively on Fedora. It's install location do not match
the AMD release.

So add some Fedora specific logic to find the ROCm version and use
rocminfo when attempts to use the AMD release fail.

Signed-off-by: Tom Rix <trix@redhat.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['08598dbb3a8a8856033d7c17236902eb72cd3523'],False,['builder.py']
681be6f55801a51278d83d8e9b52e9564bafcb77,"Fix CPU Adam JIT compilation (#5780)

This PR fixes CPU Adam JIT compilation by including the `CUDA_LIB64`
path in the `extra_ldflags` list before calling `load()`.

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['550f9c75bcb586e8f0d1523a36c81a10a2fff06b'],False,['builder.py']
3a2d526ed6d7e59254fed4e96adfe796c42e6681,"Non-reentrant checkpointing hook fix (#5781)

This PR adds an extra condition to attach backward pass hooks to leaf
nodes only if Synchronisation or Profiling is enabled, as otherwise
these hooks are not necessary. Hook code below:

```
    def after_backward_hook(_nonuse_grads):
        """"""the hook registered to all leaf tensors""""""
        nonlocal leaf_tensors, backward_visited_leaf_nodes
        backward_visited_leaf_nodes += 1

        if backward_visited_leaf_nodes == len(leaf_tensors):
            see_memory_usage(""After backward checkpointing code after backward"", force=False)

            if PROFILE_TIME:
                timers('backward').stop()
                timers.log(['backward'])
            if SYNCHRONIZE:
                get_accelerator().synchronize()
```

see_memory_usage is nevel used, as `force` is hardcoded to `False`. Thus
this hook only does any real work only when PROFILE_TIME or SYNCHRONIZE
is True. Otherwise it creates unnecessary function calls

Co-authored-by: Heyang Qin <heyangqin@microsoft.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['029bb5274ab89c36de66ced121a28074d8f4acae'],False,['checkpointing.py']
2ef82232102663168abf942f7a8a3e9e049f0922,"Fix NV references (#5821)

Fix NVIDIA references and typos.

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['3a2d526ed6d7e59254fed4e96adfe796c42e6681'],False,"['README.md', 'Gemfile']"
0584689d43c69fad6ead928c37bdb6f9130de95d,"Fix docs building guide (#5825)

Update instructions with webrick dependency
Restore Gemfile that was accidentally removed in #5821

---------

Co-authored-by: Logan Adams <loadams@microsoft.com>",['2ef82232102663168abf942f7a8a3e9e049f0922'],False,"['Gemfile', 'README.md']"
297a6840e1b8d5d5ff17418aa77d28884a544f06,"Update clang-format version from 16 to 18. (#5839)

We used a slightly old version of clang-format before, this caused
issues when folks installed the latest via apt or similar rather than
python to try and fix their formatting issues. Plus installing older
versions is a pain and the formatting style of the newer version seems
better?",['0584689d43c69fad6ead928c37bdb6f9130de95d'],False,"['deepspeed_aio_common.cpp', 'deepspeed_py_aio.cpp', 'deepspeed_py_aio_handle.cpp', 'deepspeed_py_copy.cpp', 'gemm_kernel_utils.h', 'simd.h', 'simd.h', 'type_shim.h', 'requirements-dev.txt']"
ffe0af23575c4f03a07408eacfc50b1a58781429,"Fix the bug of deepspeed sequence parallel working with batch size larger than 1 (#5823)

Modified the `alltoall` function
Verified the results with only `TP`:

![image](https://github.com/user-attachments/assets/9bdd8942-3565-418f-b7be-614293b2f2f6)

---------

Co-authored-by: Jinghan Yao <yjhmitweb@ascend-rw02.ten.osc.edu>
Co-authored-by: Sam Ade Jacobs <samjacobs@microsoft.com>
Co-authored-by: Jinghan Yao <yjhmitweb@ascend-rw01.ten.osc.edu>
Co-authored-by: Logan Adams <loadams@microsoft.com>",['ade7149db491cec71d49814013f1be2d7041dbdc'],False,['layer.py']
6e5d58d24843864ea72fbf78846123f4c30d935a,"OptimizedLinear updates (#5791)

This is a refresh of of `OptimizedLinear` with the following features to
improve performance and usability:
 * More efficient sharing of base weights using `all_gather_into_tensor`
 * Flattened sharded weights
 * Selectively offload frozen weights to cpu
* `deepspeed.linear.Init` that allows injecting OptimizedLinear during
model construction (similar to zero.Init)
* Support for load state dict directly in OptimizedLinear, this allows
loading HF model weights correctly into sharded params
 * Various bug fixes for the LoRA implementation introduced previously
 * Several new unit tests
 
Builds on-top of @RezaYazdaniAminabadi's previous FP8 updates (#5764) to
support dense model fp8 quantization.

Example usage of this to fine-tune llama-3.1-405B on a single node:
https://github.com/Snowflake-Labs/snowflake-arctic/tree/main/training/llama3.1

---------

Co-authored-by: Reza Yazdani <reza.yazdani@snowflake.com>
Co-authored-by: Reza Yazdani <152926435+sfc-gh-reyazda@users.noreply.github.com>",['1890b814244825afdb29f070352dae406f86ce73'],False,"['__init__.py', 'config.py', 'context_manager.py', 'optimized_linear.py', 'quantization.py', 'quantize.py', 'engine.py', 'test_ctx.py']"
e3177de666f9aca1a9e16c57ffe38f22ebf54d38,"Fix missing ds_id bug (#5824)

Fix #5495 - Fix missing ds_id bug by copying solution from #5193 (credit
to @getinglxf)

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['051c993310f0c521ee0758d717168edcdf60c77e'],False,['stage3.py']
f994fb2c4e418e56c26c9f23372d0e334d0c2ccb,"Update LR scheduler configuration  (#5846)

This PR is based on https://github.com/microsoft/DeepSpeed/issues/5726.

The current lr scheduler initialization always prioritize config over
manual defined scheduler in the code. However, the optimizer
initialization implementation prioritize manual defined optimizer over
config. This PR aims to make initialization behavior for both optimizer
and lr scheduler consistent where if lr scheduler is defined in the
code, then it will overwrite config.

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['e3177de666f9aca1a9e16c57ffe38f22ebf54d38'],False,"['engine.py', 'test_ds_initialize.py']"
6eed634eda502300b702f7a80c23f24aea08ed29,"Z3: optimizations for grad norm calculation and gradient clipping (#5504)

This PR add the below functionality:
1. complete_grad_norm_calculation_for_cpu_offload: move total_norm to
CPU, as expected device in such case is CPU..
2. repalce get_global_norm() with torch.linalg.norm for better
performance.
3. unscale_and_clip_grads: replace clipping based on if statement to use
torch.clamp for better performance.

change (3) is taken from
https://github.com/microsoft/DeepSpeed/pull/5547 (which was closed)

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Lev Kurilenko <113481193+lekurile@users.noreply.github.com>
Co-authored-by: Liran Bachar <lbachar@habana.ai>",['19b01e1d6089103d1ae397f2a60a178a18f63ab5'],False,"['stage3.py', 'test_zero_offloadpp.py']"
eb07d41f959bda6c76b106c1c1c40493720b2fb8,"Allow accelerator to instantiate the device (#5255)

when instantiating torch.device for HPU it cannot be fed with HPU:1
annotation, but only ""HPU"".
moving the logic to accelerator will allow to solve this issue, with
single line change.

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Joe Mayer <114769929+jomayeri@users.noreply.github.com>",['4ba49ddad817fc5241867b08677ec91b2d3070cf'],False,"['hpu-gaudi2.yml', 'hpu_accelerator.py', 'engine.py', 'utils.py']"
9a3ede7079ad4e329bac7a4bd0799e30c212b7d2,"add moe topk(k>2) gate support (#5881)

Notice some users need to use topk > 2 to train MoE models. For example:
https://huggingface.co/Qwen/Qwen2-57B-A14B/blob/main/config.json, this
PR adds support for topk (k > 2) gates.

- add topk (k>2) support
- add drop token policy based on position and probabilities.
- unit tests

---------

Co-authored-by: Kurt Chen <kurt.chen@intel.com>
Co-authored-by: Jin, Youzhi <youzhi.jin@intel.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Masahiro Tanaka <81312776+tohtana@users.noreply.github.com>",['30428d0318acdb5f9d95e495c80722d6bd8f2a47'],False,"['sharded_moe.py', 'test_moe.py']"
1ab1928d7948c45e95fc000d6bb346b5bdafc391,"Enable dynamic shapes for pipeline parallel engine inputs (#5481)

This PR enables dynamic shapes for inputs to pipeline parallel (PP)
engine.

Currently PP engine checks tensor shapes and allocates communication
buffer at the first forward/backward passes. This causes a tensor shape
mismatch error when input tensor shapes changed.

This PR adds an option to check tensor shapes at every iteration and
allocate buffer based on the shapes. As shown below, you can enable this
feature by passing `dynamic_shape=True` to `PipelineModule`.
Note that this might have a performance impact and the option is set to
False as default.

```python
model = PipelineModule(
...
   dynamic_shape=True
)
```

This will increase the overhead of buffer allocation and communication
for tensor metadata. To mitigate the overhead, this PR also includes
these improvements:
- Consolidate multiple communication calls to send/recv tensor shapes
9f96ad4049b1fb63d38a1a090480dbef61dc0490
- Reuse (extend) communication buffer instead of creating a new one
b3c07504be05b08772f6297a864ec6c27b5eeca3

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['4d4ff0edddb28a4d53d40dffad9ba16e59b83aec'],False,"['engine.py', 'module.py', 'alexnet_model.py', 'test_pipe.py', 'util.py']"
c2e3a706b5a39517e7179baa7c25c60d17a2e3c6,"Add and Remove ZeRO 3 Hooks (#5658)

Gives the ability to add and remove the forward hooks in ZeRO 3 by using
a context manager. These code changes were taken from a Huggingface
[PR](https://github.com/huggingface/trl/pull/1617) and integrated for
direct support in DeepSpeed.

This is useful in the inference case and the speedup can be observed
[here](https://github.com/huggingface/trl/pull/1483).

---------

Co-authored-by: root <root@deepspeed-c000004.2d1icxc5dsxehnpuwt3ifc34ph.gvxx.internal.cloudapp.net>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Heyang Qin <heyangqin@microsoft.com>
Co-authored-by: Logan Adams <loadams@microsoft.com>",['1ab1928d7948c45e95fc000d6bb346b5bdafc391'],False,"['__init__.py', 'stage3.py', 'test_unwrap_model.py']"
5f0d177fd7f12f9448814e93f38f162822880a9b,"DeepNVMe GDS (#5852)

PR for the GDS AIO code.

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Logan Adams <loadams@microsoft.com>
Co-authored-by: Ubuntu <deepspeed@H100-VM2.shlnn55tgwve1eacvp21ie45dg.jx.internal.cloudapp.net>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['c2e3a706b5a39517e7179baa7c25c60d17a2e3c6'],False,"['nv-pre-compile-ops.yml', 'deepspeed_aio_op_desc.cpp', 'deepspeed_aio_op_desc.h', 'deepspeed_aio_thread.cpp', 'deepspeed_aio_thread.h', 'deepspeed_cpu_op.cpp', 'deepspeed_cpu_op.h', 'deepspeed_py_aio.cpp', 'deepspeed_py_aio.h', 'deepspeed_py_aio_handle.cpp', 'deepspeed_py_aio_handle.h', 'deepspeed_py_copy.cpp', 'deepspeed_py_copy.h', 'deepspeed_py_io_handle.cpp', 'deepspeed_py_io_handle.h', 'py_ds_aio.cpp', 'aio_bench_generate_param.py', 'aio_bench_perf_sweep.py', 'ds_aio_args.py', 'ds_aio_basic.py', 'ds_aio_handle.py', 'ds_aio_job.py', 'run_read_sweep.sh', 'run_write_sweep.sh', 'test_ds_aio.py', 'test_ds_aio_utils.py', 'validate_async_io.py', 'deepspeed_gds_op.cpp', 'deepspeed_gds_op.h', 'deepspeed_gds_utils.h', 'deepspeed_py_gds_handle.cpp', 'deepspeed_py_gds_handle.h', 'py_ds_gds.cpp', 'validate_gds.py', '__init__.py', 'aio_config.py', 'constants.py', 'partitioned_param_swapper.py', 'async_io.py', 'builder.py', 'gds.py', 'test_aio.py', 'test_gds.py']"
ba0ab7138dafb522e78a4c1e2a85da8846a3e250,"Pin transformers version on nv-nightly (#6002)

nv-nightly was failing due to updates in transformers, we will need to
introduce a real fix for these, but this at least gets the test passing
and we need to update transformers support for MII too.",['5f0d177fd7f12f9448814e93f38f162822880a9b'],False,['nv-nightly.yml']
3831d4b57af1b9bef933bfdcc98d397249a64b3b,"Bug Fix 5880 (#6378)

Allowing hf args to be passed through class to AutoConfig.pretrained.

Co-authored-by: Ubuntu <deepspeed@H100-VM2.shlnn55tgwve1eacvp21ie45dg.jx.internal.cloudapp.net>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['01fe65b300c665567d365646effe490f3316fdf4'],False,['huggingface_engine.py']
96393f561dfa47017496139a6cbbe325b2286ab2,"Update linear.py compatible with torch 2.4.0 (#5811)

deepspeed/runtime/zero/linear.py:67: FutureWarning:
`torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use
`torch.amp.custom_bwd(args..., device_type='cuda')` instead.

Fixes: #5682

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['3831d4b57af1b9bef933bfdcc98d397249a64b3b'],False,['linear.py']
b65ea50631fe122f8d0c66e238d05926ee64bccd,"GDS Swapping Fix (#6386)

Fixing gds api call

Co-authored-by: Ubuntu <deepspeed@H100-VM2.shlnn55tgwve1eacvp21ie45dg.jx.internal.cloudapp.net>",['96393f561dfa47017496139a6cbbe325b2286ab2'],False,['partitioned_param_swapper.py']
8b191d7ccfb06430e3ac3fae7c86943f9ee78be8,"Long sequence parallelism (Ulysses) integration with HuggingFace (#5774)

This PR enhances capabilities of [DeepSpeed long sequence (context)
parallelism (aka DS
Ulysses)](https://dl.acm.org/doi/10.1145/3662158.3662806) with support
for HuggingFace (and by extension other frameworks) models. With HF
integration, users can use sequence parallelism for model
pre/mid/post-training, finetuning etc. Usage requires both _torch
>=2.2.2 and flash-attention_. ZeRO-1 and 2 are supported, ZeRO-3 and
SPDA support in progress. Corresponding PR in HF is
[PR32305](https://github.com/huggingface/transformers/pull/32305).

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['b65ea50631fe122f8d0c66e238d05926ee64bccd'],False,"['__init__.py', 'comm.py', 'torch.py', 'config.py', 'engine.py', 'cross_entropy.py', 'groups.py', 'test_ulysses.py']"
e6fcc226c728ba99a09cda9d196f93986b489edd,"fix fp16 Qwen2 series model to DeepSpeed-FastGen (#6028)

based on PR #5403 (Qwen1.5-MOE) and #5219 (Qwen1.5), support Qwen2
series model.

including: 0.5B, 1.5B, 7B, 57B-A14B, and 72B models.

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['7260890452eb89185f9ab1e09550938f78ea91db'],False,"['huggingface_engine.py', 'top_k_utils.h', 'blocked_kv_rotary.py', 'blocked_kv_rotary_cuda.cu', 'container.py', 'model.py', 'cutlass_multi_gemm.py']"
51da191eb54b69224edfb7cb4ce69db9a88956f7,"add pip install cutlass version check (#6393)

fix this issue: https://github.com/microsoft/DeepSpeed/issues/6006

cc. @tjruwase",['0f0f231e8a18e816b68b7c4ebcd3048f75ffdbe0'],False,['evoformer_attn.py']
0a4457cc48a3d5b926d3e85096dfed00831a7058,"Pydantic v2 migration (#5167)

Pydantic v2 has been out for some time now. We have been relying on
using the v1 API available in v2 until now. This is a refresh of #3902
to bring proper v2 support to DeepSpeed.

Corresponding DeepSpeed-MII PR
[here](https://github.com/microsoft/DeepSpeed-MII/pull/423).

@loadams

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Logan Adams <loadams@microsoft.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Abhishek Kulkarni <11399+adk9@users.noreply.github.com>
Co-authored-by: Abhishek Kulkarni <abkulkarni@microsoft.com>
Co-authored-by: Lev Kurilenko <113481193+lekurile@users.noreply.github.com>",['8c2be7e942ab219989a7c84a714e94c5e2cf7310'],False,"['nv-a6000.yml', 'config.py', 'config.py', 'config_v2.py', 'flat_model_helpers.py', 'manager_configs.py', 'config.py', 'pydantic_v1.py', 'config_utils.py', 'config.py', 'offload_config.py', 'stage_1_and_2.py', 'requirements-readthedocs.txt', 'requirements.txt', 'test_manager_configs.py', 'test_ds_config_dict.py', 'test_ds_config_model.py']"
55b4cae80fc1479ac802b91b4b8955da444016ae,Fix torch check (#6402),['0a4457cc48a3d5b926d3e85096dfed00831a7058'],False,['gds.py']
e2654bfd1ab431bde088a7501ed01b503daa5ab1,"Fix Type Mismatch (#6410)

`num_bytes_per_thread` was a smaller type than `file_num_bytes`, this
caused issues when dividing by `num_threads`.

Co-authored-by: jomayeri <deepspeed@H100-VM2.shlnn55tgwve1eacvp21ie45dg.jx.internal.cloudapp.net>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['ca4449e84378e1239f5c46d7353cfc24dc9c4c83'],False,['deepspeed_aio_op_desc.h']
8ac42ed73a2f487f3c21fbc3bbaaec5f2eb5fbce,"Fix redundant seq data parallel grp argument in Z3/MiCS (#5352)

Deprecate redundant sequence_data_parallel_group argument. Users/client
code will control across which process group Z3 parameters will be
partitioned from one of [None, data_parallel_group,
sequence_data_parallel].

---------

Co-authored-by: Masahiro Tanaka <81312776+tohtana@users.noreply.github.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['e2654bfd1ab431bde088a7501ed01b503daa5ab1'],False,"['mics.py', 'partition_parameters.py']"
77f61e677175225333237af28075b346c3dcde6e,"Revert ""Fix torch check (#6402)""

This reverts commit 55b4cae80fc1479ac802b91b4b8955da444016ae.",['1bfa341bbdc8c3fadde426486fc83f5b0c80a727'],False,['gds.py']
eb37cacf229ece71580a30bec723bf361f4ba82f,"Revert ""Revert ""Fix torch check (#6402)""""

This reverts commit 77f61e677175225333237af28075b346c3dcde6e.",['77f61e677175225333237af28075b346c3dcde6e'],False,['gds.py']
b5cf30a0857a1aab4040bb795f045bb37d08c9f8,"Dtype support check for accelerator in UTs (#6360)

Check if the dtype is supported by the accelarator if not then skip

---------

Co-authored-by: Shaik Raza Sikander <srsikander@habana.ai>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['1041c8a1720113490d4f40e4778e478af7345bbc'],False,"['test_checkpoint_sharding.py', 'test_model_profiling.py', 'inference_test_utils.py']"
0cd9bf59785f24aefae4c407f184bf71339ed5d4,"[CCL] fix condition issue in ccl.py (#6443)

previous condition check is not right, it would cause this condition
always be True.

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['f2739b4f722203b781722cec84a374d0d798b084'],False,['ccl.py']
a7ffe540fc98a945b9d3d2a9c05653e160d62dcf,"Avoid gds build errors on ROCm (#6456)

This PR is to avoid the below error during DeepSpeed build on ROCm. The
error is because of the incompatibility of GDSBuilder extension on ROCm.

```
    Traceback (most recent call last):
      File ""<string>"", line 1, in <module>
      File ""/tmp/pip-req-build-lv1v39xc/setup.py"", line 180, in <module>
        op_compatible = builder.is_compatible()
      File ""/tmp/pip-req-build-lv1v39xc/op_builder/gds.py"", line 47, in is_compatible
        CUDA_LIB64 = os.path.join(CUDA_HOME, ""lib64"")
      File ""/opt/conda/envs/py_3.9/lib/python3.9/posixpath.py"", line 76, in join
        a = os.fspath(a)
    TypeError: expected str, bytes or os.PathLike object, not NoneType
    Total number of unsupported CUDA function calls: 0


    Total number of replaced kernel launches: 1
    ----------------------------------------
ERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output
```

cc: @jithunnair-amd

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['0cd9bf59785f24aefae4c407f184bf71339ed5d4'],False,['gds.py']
89c4d9f5a73d06eb8db8037d61188c628ceeba51,"TestLowCpuMemUsage UT get device by device_name (#6397)

Co-authored-by: Shaik Raza Sikander <srsikander@habana.ai>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['a7ffe540fc98a945b9d3d2a9c05653e160d62dcf'],False,['test_inference.py']
cfc6ed3722e54480af54899a2eea05cebd238090,"bf16_optimizer: fixes to different grad acc dtype (#6485)

- fix step function to cast to FP32 before step in case of different
gradient accumulation data type
- remove redundatn function initialize_optimizer_states()",['9b7fc5452471392b0f58844219fcfdd14a9cdc77'],False,['bf16_optimizer.py']
9d17116fcdb44b81eb00d3bce91431dc35cd69b1,"print warning if actual triton cache dir is on NFS, not just for default (#6487)

move the logic that prints a warning when triton cache dir is on NFS to
act on the actual calculated cache_dir rather than on the default.

this means that:
- when the default directory (in the user's home directory) is on NFS
but `TRITON_CACHE_DIR` is set to a non-NFS directory, no warning will be
printed whereas prior to this change a spurious and confusing warning
was printed
- when the user's home directory is not on NFS but `TRITON_CACHE_DIR` is
set to an NFS directory, a warning will be printed whereas prior to this
change no warning would be printed
 
fixes #6486",['5df12a4a8559852c19d6423fca54d828846addc1'],False,['matmul_ext.py']
ddeb0c19a006fd7b632e82655051073ac100a9b4,"Fix patch for parameter partitioning in zero.Init() (#6388)

This PR fixes an issue addressed in #5921.
With this change, we only apply the patch for parameter partitioning to
classes that have `__init__` so that we can avoid applying the patch
multiple times.
The class that does not have `__init__` now uses its superclass's one.
So this PR also applies the patch to the root class,
`torch.nn.modules.module.Module`.

Thanks @VeryLazyBoy for the report and initial solution.

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['9d17116fcdb44b81eb00d3bce91431dc35cd69b1'],False,['partition_parameters.py']
662a421b05432045280c27726592715c1e9100c7,"Safe usage of popen (#6490)

Avoid shell=True security issues with Popen",['ddd357182371d38bd65a7a2026952e55b46380da'],False,"['numa.py', 'async_io.py', 'builder.py', 'async_io.py', 'async_io.py', 'setup.py']"
3b09d945ead6acb15a172e9a379fc3de1f64d2b2,"fix pipeline eval_batch micro_batches argument for schedule (#6484)

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['4f803852ac785d375ad7488d158fa85fd90e6c4a'],False,['engine.py']
2a647c51d4c6020fb9c54bc15f9af98f492c46e5,"Fix the broken url link (#6500)

Simple changes to fix the Intel cpu example link and add more xpu
examples.

Signed-off-by: roger feng <roger.feng@intel.com>",['3b09d945ead6acb15a172e9a379fc3de1f64d2b2'],False,['accelerator-setup-guide.md']
fc22d9602db9a988eebaadac442e2603cf1c4943,"fix environment variable export bug for MultiNodeRunner (#5878)

In some multi-node environment like SLURM，there are some environment
vars that contain special chars and can trigger errors when being
exported.

For example, there is a var `SLURM_JOB_CPUS_PER_NODE=64(x2)` when
requesting two nodes with 64 cpus using SLURM.
Using `runner.add_export` to export this var will add a command `export
SLURM_JOB_CPUS_PER_NODE=64(x2)` when launching subprocesses, while this
will cause a bash error since `(` is a key word of bash, like:
```
[2024-08-07 16:56:24,651] [INFO] [runner.py:568:main] cmd = pdsh -S -f 1024 -w server22,server27 export PYTHONPATH=/public/home/grzhang/code/CLIP-2;  export SLURM_JOB_CPUS_PER_NODE=64(x2); ...
server22: bash: -c: 行 0: 未预期的符号“(”附近有语法错误
```
This PR simply wrap the environment vars with a pair of `""` to make sure
they are treated as string.

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['2a647c51d4c6020fb9c54bc15f9af98f492c46e5'],False,['multinode_runner.py']
659f6be10574e49d7d5b76de7949c7c0254cd857,"Avoid security issues of subprocess shell (#6498)

Avoid security issues of `shell=True` in subprocess

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['c27483933d50a693fef9c48418d2664cf6a6a6f8'],False,"['ds_bench', 'ds_aio_job.py', 'comm.py', 'elastic_agent.py', 'multinode_runner.py', 'runner.py', 'builder.py', 'setup.py', 'BingBertSquad_test_common.py', 'run_checkpoint_test.py', 'test_common.py', 'common.py']"
170b46e8b146b7ed93cd5f28d194bf8412ba0cbf,"Add conditional on torch version for scaled_dot_product_attention (#6517)

Changes from #4724 broke support for torch<2.0 in the flops profiler as
the scaled_dot_product_attention [wasn't
added](https://pytorch.org/docs/2.0/generated/torch.nn.functional.scaled_dot_product_attention.html#torch.nn.functional.scaled_dot_product_attention)
until a beta version in torch 2.0

Resolved: #5534

Todo:
- [ ] Test this
- [ ] Issue resolution with users.",['659f6be10574e49d7d5b76de7949c7c0254cd857'],False,['profiler.py']
61de0171760f02a24f6f091631d6c3120305c880,"Skip failing newly added tests in accelerate (#6574)

Adding the new tests in
https://github.com/huggingface/accelerate/pull/3097 caused the
nv-accelerate-v100 tests to fail. Due to other CI issues we didn't
notice this at first. This just skips the problematic test for now.

cc: @stas00 / @muellerzr",['2a56f53395b2e0ef2ffe9947671fe153ba026328'],False,['nv-accelerate-v100.yml']
a5400974df59d99d58dabd173bb8d89180bbd773,"DeepNVMe perf tuning (#6560)

Add performance tuning utilities: `ds_nvme_tune` and `ds_io`.  
Update tutorial with tuning section.

---------

Co-authored-by: Ubuntu <jomayeri@microsoft.com>
Co-authored-by: Joe Mayer <114769929+jomayeri@users.noreply.github.com>",['7622cd9e68756d6e2a65e654f2b4ca678d55c251'],False,"['ds_io', 'ds_nvme_tune', '__init__.py', 'ds_aio_args.py', 'ds_aio_basic.py', 'ds_aio_handle.py', 'ds_aio_job.py', 'parse_nvme_stats.py', 'perf_generate_param.py', 'perf_run_sweep.py', 'perf_sweep_utils.py', 'test_ds_aio.py', 'test_ds_aio_utils.py', 'validate_async_io.py', 'deepnvme.md', 'setup.py']"
c85c8703bc49eefb78d8d64b5f0027f7b8acf9ff,"Fix gradient accumulation for Z2+offload (#6550)

The ZeRO 1/2 optimizer performs incorrect gradient accumulation in the
path for ZeRO2 + Offloading. This issue is caused by two main reasons:

1) The micro_step_id in the ZeRO 1/2 optimizer is:

- Initialized to 0 in the constructor.
- Reset to -1 during the backward pass.

For example, given a gradient accumulation step of 4, the micro_step_id
changes as follows:

- For the first global step: 1, 2, 3, 4.
- Subsequently: 0, 1, 2, 3.

2) Gradients are copied to the buffer on the first micro step and
accumulated in the buffer during the following micro steps. However, the
current code incorrectly copies gradients at steps that are not at the
accumulation boundary.

This PR aligns the micro_step_id initialization in both the constructor
and the backward pass, and corrects the condition for copying and
accumulating gradients.

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['0fbe96a5022d30470b8cb53f65237389ba26c57a'],False,['stage_1_and_2.py']
ba58682a138760ee44b1366165fdbe4d87522323,"fix errors when setting zero3 leaf modules with torch.compile (#6564)

When setting zero3 leaf modules to a higher level module and running
with torch.compile, there are a few errors from ZeROOrderedDict.

First it doesn't support Deep copy for not having a constructor with no
parameters.

Second, it doesn't check the existence of ds_status attr on param before
accessing the attr.

change contributed by Haifeng Chen

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['c85c8703bc49eefb78d8d64b5f0027f7b8acf9ff'],False,['parameter_offload.py']
d45cfd34551537ce6f8317504bd520d7a2a1a588,"[XPU] Support DeepNVMe new code structure (#6532)

In DeepNVMe GDS update, many functions are changed into a more abstract
way. Also added some files. These change break zero-infinity on XPU. To
bring this feature back, we have this PR:
1. modify the aio opbuilder for new files.
2. Add custom cpu_op_desc_t for xpu users. (XPU don't handle buffer
aligned here)

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['ba58682a138760ee44b1366165fdbe4d87522323'],False,"['deepspeed_cpu_op.cpp', 'async_io.py']"
047bcf6af6a3721cfac31a13a1ab07c6b5482fb9,"Add APIs to offload states of model, optimizer, and engine (#6011)

This PR adds the following APIs to offload model, optimizer, and engine
states.

```pytyon
def offload_states(self,
                   include: Container[OffloadStateTypeEnum] = None,
                   device: OffloadDeviceEnum = OffloadDeviceEnum.cpu,
                   pin_memory: bool = True,
                   non_blocking: bool = False) -> None:
    """"""Move the ZeRO optimizer buffers to the specified device.

    Arguments:
        include: Optional. The set of states to offload. If not provided, all states are offloaded.
        device: Optional. The device to move the ZeRO optimizer buffers to.
        pin_memory: Optional. Whether to pin the memory of the offloaded states.
        non_blocking: Optional. Whether to offload the states asynchronously.
...
def offload_states_back(self, non_blocking: bool = False) -> None:
```

Here is the typical usage.
```python
# Offload after forward, backward, and step
model.offload_states()
# Do something requiring a lot of device memory
...
# Load states back to device memory
model.offload_states_back()
```

You can selectively offload states to balance the offloading overhead
and memory saving.
```python
model.offload_states(include=set([OffloadStateTypeEnum.hp_params, OffloadStateTypeEnum.opt_states], device=OffloadDeviceEnum.cpu)
```

Performance (4.3B parameters / 4x A100)
- Environment (4x A100, [benchmark
script](https://gist.github.com/tohtana/05d5faba5068cf839abfc7b1e38b85e4))
- Average Device to Host transfer time: 2.45 GB/s, aggregated: 9.79 GB/s
  - Average Host to Device transfer: 11.05 GB/s, aggregated: 44.19 GB/s
- Mem (allocated by PyTorch)
  - Before offload 18.2GB
  - After offloading 17.7MB
- Time ([benchmark
script](https://github.com/microsoft/DeepSpeedExamples/tree/tohtana/offload_states/training/offload_states),
offloading time/loading time)

python output_table.py 
| |pin_memory=0 non_blocking=0|pin_memory=0 non_blocking=1|pin_memory=1
non_blocking=0|pin_memory=1 non_blocking=1|

|--:|---------------------------|---------------------------|---------------------------|---------------------------|
| 1|4.34 / 3.42 |4.99 / 2.37 |6.5 / 2.42 |6.0 / 2.39 |
| 2|9.9 / 3.28 |5.1 / 2.34 |6.21 / 2.42 |6.25 / 2.45 |
| 3|9.92 / 3.19 |6.71 / 2.35 |6.33 / 2.38 |5.93 / 2.42 |
| 4|9.55 / 2.82 |7.11 / 2.39 |6.9 / 2.38 |6.5 / 2.43 |
| 5|4.4 / 3.35 |6.04 / 2.41 |6.26 / 2.41 |6.32 / 2.47 |
| 6|4.4 / 3.57 |6.58 / 2.42 |6.88 / 2.4 |6.35 / 2.43 |
| 7|9.51 / 3.12 |6.9 / 2.39 |6.9 / 2.39 |6.46 / 2.4 |
| 8|4.77 / 3.64 |6.69 / 2.39 |7.39 / 2.42 |6.56 / 2.46 |
| 9|9.5 / 3.07 |7.18 / 2.42 |6.67 / 2.39 |7.38 / 2.46 |

TODO:
- Enable offloading to a NVMe storage -> NVMe support is non-trivial. I
suggest adding the support in another PR
- [DONE] Discard buffer (and recreate it) instead of offloading. We
don't need to restore the contiguous buffer for reduce.
- [DONE] Check pin_memory improves performance or not

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['d45cfd34551537ce6f8317504bd520d7a2a1a588'],False,"['engine.py', 'utils.py', 'offload_config.py', 'stage3.py', 'utils.py', 'zero3.rst', 'test_offload_states.py']"
d4e189507659aca7970185d33b84115fbb11b490,"[COMPILE] workflow for deepspeed + torch.compile (#6570)

We use simple model + deepspeed zero 3 + torch.compile and count graph
break numbers to demonstrate current status of combing deepspeed +
torch.compile.

---------

Co-authored-by: Masahiro Tanaka <81312776+tohtana@users.noreply.github.com>",['1caf6e8107689f5ea9611ac2d6bbbf3a3e6e9731'],False,"['xpu-compile.yml', 'ds_config.json', 'test_compile.py']"
828ddfbbda2482412fffc89f5fcd3b0d0eba9a62,"Fixes on the accelerate side mean we do not need to skip this test (#6583)

HF accelerate implemented fixes here:
https://github.com/huggingface/accelerate/pull/3131

This means we can revert the changes from #6574",['d4e189507659aca7970185d33b84115fbb11b490'],False,['nv-accelerate-v100.yml']
8cded575a94e296fee751072e862304676c95316,"Fix torch include in `op_builder/mlu/fused_adam.py` and update no-torch workflow triggers (#6584)

Changes from #6472 caused the no-torch workflow that is an example of
how we build the DeepSpeed release package to fail (so we caught this
before a release, see more in #6402). These changes also copy the style
used to include torch in other accelerator op_builder implementations,
such as npu
[here](https://github.com/microsoft/DeepSpeed/blob/master/op_builder/npu/fused_adam.py#L8)
and hpu
[here](https://github.com/microsoft/DeepSpeed/blob/828ddfbbda2482412fffc89f5fcd3b0d0eba9a62/op_builder/hpu/fused_adam.py#L15).

This also updates the no-torch workflow to run on all changes to the
op_builder directory. The test runs quickly and shouldn't add any
additional testing burden there.

Resolves: #6576",['828ddfbbda2482412fffc89f5fcd3b0d0eba9a62'],False,"['no-torch.yml', 'fused_adam.py']"
b93c7a20c8b50258935d8598e53a6e388762fee9,"[ROCm] Fix subprocess error (#6587)

Fixes https://github.com/microsoft/DeepSpeed/issues/6585 
Use shell=True for subprocess.check_output() in case of ROCm commands.
Do not use shlex.split() since command string has wildcard expansion.

Signed-off-by: Jagadish Krishnamoorthy <jagadish.krishnamoorthy@amd.com>",['8cded575a94e296fee751072e862304676c95316'],False,['builder.py']
20695b39b19b64bf0ae0ef3e590bc29bccff36c7,Move V100 workflows from cuda 11.1/11.7 to 12.1 (#6607),['940887ded1000121cc11b746a6a2eb9b53e3e6fc'],False,"['nv-accelerate-v100.yml', 'nv-ds-chat.yml', 'nv-inference.yml', 'nv-lightning-v100.yml', 'nv-mii.yml', 'nv-nightly.yml', 'nv-torch-latest-v100.yml', 'nv-torch-nightly-v100.yml', 'nv-transformers-v100.yml']"
00c4b98ba0786d3ce99bde46ee60811f07f01dff,"Fix SD workflow (#6609)

SD workflow needed updates when we moved to pydantic 2 support that was
never added before.

Passing nv-sd workflow
[here](https://github.com/microsoft/DeepSpeed/actions/runs/11239699283)",['20695b39b19b64bf0ae0ef3e590bc29bccff36c7'],False,['nv-sd.yml']
745dd48b90260bff3c65c7620d993df9ca6c529b,Pin accelerate to fix CI failures/issues (#6610),['00c4b98ba0786d3ce99bde46ee60811f07f01dff'],False,['nv-accelerate-v100.yml']
5cbbff40bd2eceb4340376d7e3db764c69a2527a,"Fix device selection using CUDA_VISIBLE_DEVICES (#6530)

This PR addresses #5818.
Instead of contiguous numbers based on the device count, this PR uses
device indices in `--include`.

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['f74ea69abf09294834eba13b003568a440e3d22b'],False,['runner.py']
645639bcf892e35cd39e7baccdab0e69abb8ce33,"Rearrange inference OPS and stop using builder.load  (#5490)

This PR mainly handles all places where InferenceBuilder is used to
access any op or a specific implementation for an op.
Instead an op is defined, and its proper implementation is picked inside
and the usage will be transparent to the user.
What was done in the PR:
1) Added missing ops (added a py file with fallback mechanism)
2) Added missing fallback implementations for existing ops
3) removed all usages for builder.load and replaced them with ops
instead.
4) added workspace op and inferenceContext which contains all workspace
related functions and inferenceContext is the python fallback of
inferenceContext in CUDA
5) a small change to softmax_context signature to fit the fallback
signature.

---------

Co-authored-by: Joe Mayer <114769929+jomayeri@users.noreply.github.com>
Co-authored-by: Lev Kurilenko <113481193+lekurile@users.noreply.github.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['ca8b1fe945ce8100dd1ac164d7416f3b8f6be8b7'],False,"['pt_binding.cpp', 'engine.py', 'ds_llama2.py', 'ds_transformer.py', 'config.py', 'diffusers_attention.py', 'diffusers_transformer_block.py', 'ds_attention.py', 'moe_inference.py', 'bias_add.py', 'bias_gelu.py', 'bias_relu.py', 'bias_residual.py', 'einsum_sec_sm_ecm.py', 'gated_activation.py', 'gelu_gemm.py', 'layer_norm.py', 'mlp_gemm.py', 'moe_res_matmul.py', 'pad_transform.py', 'pre_rms_norm.py', 'qkv_gemm.py', 'residual_add.py', 'rms_norm.py', 'softmax.py', 'softmax_context.py', 'vector_add.py', 'vector_matmul.py', 'workspace.py', 'attention.py', 'ops.py', 'hybrid_engine.py', '__init__.py', 'transformer_inference.py', 'test_bias_add.py', 'test_bias_geglu.py', 'test_bias_gelu.py', 'test_bias_relu.py', 'test_gelu.py', 'test_layer_norm.py', 'test_moe_res_matmult.py', 'test_residual_add.py', 'test_rms_norm.py', 'test_softmax.py']"
1062a0c6583453af2d405e8ca2c0ad08b903d23e,"Unpin accelerate tests, update lightning with node16 removal. (#6611)

HF accelerate fixes implemented in
https://github.com/huggingface/accelerate/pull/3145 mean that we no
longer need to pin the Accelerate version!

nv-lightning tests now run on Ubuntu 20.04+, so we support >node 16, so
we can remove the explicit permissions for that in the env config.",['645639bcf892e35cd39e7baccdab0e69abb8ce33'],False,"['nv-accelerate-v100.yml', 'nv-lightning-v100.yml']"
7d751ee8903187cd7cd82f43816acf1d5d0907ba,"Clean up prefetched parameters (#6557)

Parameters prefetched by ZeRO3 are sometimes not used. This occurs when
the actual sub-module execution differs from previous tracing. As a
result, the state of the allgather handle for such a parameter remains
`INFLIGHT`, causing functions like `empty_partition_cache` to detect it
and throw an error.
This PR resolves the issue by ensuring that communication finishes and
the parameters are freed.

As this issue was mentioned in #6011, this includes the change of the
branch. We need to merge #6011 first.

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['55f7f3789eed805d8f5deab0e2276516a302d745'],False,['partitioned_param_coordinator.py']
a1f98bdc703dacf95e11c4ae25f6fd11b6d1277e,"AIO CPU Locked Tensor (#6592)

Restoring the functionality of the cpu locked tensor in the AIO library.
Make async_io operator available for CPU accelerator, i.e., CPU only
environment.

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['7d751ee8903187cd7cd82f43816acf1d5d0907ba'],False,"['cpu_accelerator.py', 'deepspeed_aio_common.cpp', 'deepspeed_aio_common.h', 'deepspeed_aio_utils.cpp', 'deepspeed_aio_utils.h', 'deepspeed_aio_op_desc.cpp', 'deepspeed_aio_op_desc.h', 'deepspeed_cpu_op.cpp', 'deepspeed_cpu_op.h', 'deepspeed_pin_tensor.cpp', 'deepspeed_pin_tensor.h', 'deepspeed_py_aio.cpp', 'deepspeed_py_aio_handle.cpp', 'deepspeed_py_aio_handle.h', 'deepspeed_py_io_handle.cpp', 'deepspeed_py_io_handle.h', 'py_ds_aio.cpp', 'deepspeed_gds_op.cpp', 'deepspeed_gds_op.h', 'deepspeed_py_gds_handle.cpp', 'deepspeed_py_gds_handle.h', 'py_ds_gds.cpp', 'deepnvme.md', 'builder.py', '__init__.py', 'async_io.py', 'test_aio.py', 'test_gds.py', 'test_nvme_checkpointing.py']"
adec99121b411709e1b185a486d18aa846c82c64,"Add API to get devices of offload states (#6586)

This PR adds an API `deepspeed.runtime.zero.offload_states
get_state_devices`, which gets devices of offload states as suggested in
this
[comment](https://github.com/microsoft/DeepSpeed/pull/6011#issuecomment-2358068777).

We could lift this up to `deepspeed.utils` but would need to resolve a
circular import: User code -> `deepspeed.utils` ->
`deepspeed.utils.offload_states` -> `deepspeed.runtime.zero` ->
`deepspeed.runtime.zero.partition_parameters` -> `deepspeed.utils`

This will require a significant refactoring as long as we have
`OffloadStateTypeEnum` in `deepspeed.runtime.zero`.

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['d7ca3d83732854eea41e6c83e603639699faf1d4'],False,"['utils.py', 'offload_states.py', 'stage3.py', 'zero3.rst', 'test_offload_states.py']"
cf41e8c4e8b7b9ad43f31c8e5b255455278ef15b,"[compile] Show breakdown of graph break (#6601)

This PR extends https://github.com/microsoft/DeepSpeed/pull/6570 by
showing a breakdown of graph breaks. So we can see how graph breaks are
distributed among different reasons. An example of graph break output
can be seen from the following workflow run
https://github.com/microsoft/DeepSpeed/actions/runs/11199157962",['7a5bc4fdf90d3a1cd711973ed9d0113b582f143e'],False,"['xpu-compile.yml', 'ds_config_z2.json', 'ds_config_z3.json', 'test_compile.py']"
65ab64481f47f92fd427bd98b30e4faf604e5c9f,Add API for updating ZeRO gradients (#6590),['cf41e8c4e8b7b9ad43f31c8e5b255455278ef15b'],False,"['stage3.py', '__init__.py', 'mixed_precision_linkage.py', 'tensor_fragment.py', 'zero3.rst', 'test_zero_tensor_fragment.py']"
13c16c9562dc41e153679278cf2ecad058a9fbc7,"Accept btl_tcp_if_include option through launcher_args (#6613)

This patch fixes issue #4460.
When `btl_tcp_if_include` option is provided through `--launcher_args`,
we use the provided option instead of the hardcoded `--mca
btl_tcp_if_include eth0`. Otherwise we use `--mca btl_tcp_if_include
eth0` as the default for compatibility.

Fixes #4460

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['65ab64481f47f92fd427bd98b30e4faf604e5c9f'],False,"['multinode_runner.py', 'test_multinode_runner.py']"
85b7469ea00f7719a27e3e8d1ffaa8765575f820,"Add first Step in LR Schedulers (#6597)

Some (not all) of the LR schedulers in runtime were missing the
initialization of the optimizer group lr.

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['13c16c9562dc41e153679278cf2ecad058a9fbc7'],False,"['lr_schedules.py', 'test_lr_schedulers.py']"
ce468c3756561dc868672219b2895a56df2babe2,"add option to disable logger while compiling to avoid graph breaks (#6496)

adding an option to disable calls for logger while compiling to avoid
graph breaks. Here I used an environment variable to determine whether
to activate this option, but it can also be determined using the json
config file or any other way you see fit.

---------

Co-authored-by: snahir <snahir@habana.ai>
Co-authored-by: Masahiro Tanaka <81312776+tohtana@users.noreply.github.com>",['bf60fc0ca63b74722d3cf1bbabf17ea9fff37ffe'],False,['logging.py']
1a45bd8e8ca27ce32a7091e64d07a04b2adb2bb5,"Lock cache file of HF model list (#6628)

The error in the following log suggests that the cache file for HF model
list can be broken:

https://github.com/microsoft/DeepSpeed/actions/runs/11343665365/job/31546708118?pr=6614

The actual cause of the above error is unclear, but `_hf_model_list`
potentially breaks the cache file when it is concurrently called from
multiple processes. This PR locks the cache file to ensure
`_hf_model_list` safely reads and writes the file.",['ce468c3756561dc868672219b2895a56df2babe2'],False,['test_inference.py']
a36db9cc1c167c24c7ce6b1d3f38a8550c6ec4b7,"Update torch version in workflows (#6631)

Set PyTorch version in CI workflows to v2.5.

Context: The
[error](https://github.com/microsoft/DeepSpeed/actions/runs/11371525624/job/31633793986?pr=6630)
in #6630 might have been caused by the PyTorch version mismatch or
something.",['c9899dc14a391538ce8f0c4d58920c390dc381de'],False,"['cpu-torch-latest.yml', 'nv-nightly.yml', 'nv-torch-latest-v100.yml']"
6eefc3d0ead2c6360eec248daab0cae66a737ea0,"Fix Memory Leak In AIO (#6630)

Fixing a memory leak in AIO pinned tensor as well as an incorrect
function type for gds op.

---------

Co-authored-by: Masahiro Tanaka <81312776+tohtana@users.noreply.github.com>",['c9fc34a4be558efce2a26d2b4e08cd8880524dc8'],False,"['deepspeed_cpu_op.cpp', 'deepspeed_pin_tensor.cpp', 'deepspeed_gds_op.cpp', 'deepspeed_gds_op.h', 'deepspeed_py_gds_handle.cpp', 'deepspeed_py_gds_handle.h']"
a24cdd6b67a0e761474d2074d39ac73f4968caf7,"[XPU] [DeepNVMe] use same cpu_op_desc_t with cuda (#6645)

We have found that #6592 uses `_pinned_tensor_mgr` to create cpu bounce
buffer, which is same with what our xpu accelerator currently doing.
So no need to use xpu device specific cpu_op_desc_t.
In this PR:
1. remove custom csrc/xpu/aio/deepspeed_cpu_op.cpp
2. modify xpu async_io opbuilder.

This issue cannot be easily done with revert #6532 , for we added some
source file as last time GDS feature going in DS. So file this new PR :)",['11bbf45af53c47f0fae46fa14041af4edb2f5d85'],False,"['deepspeed_cpu_op.cpp', 'async_io.py']"
b647fb2470f8f6fefe5cab0ea84a2d89696eb898,"Fix expert grad scaling problem with ZeRO optimizer (#6546)

Fix [#6545]

work:
- expert gradient average: divide edp_world_size -> divide dp_world_size
- unit test: make sure model with different dp/ep has same expert
gradient

---------

Co-authored-by: wangyiou <wangyiou@xiaohongshu.com>
Co-authored-by: Masahiro Tanaka <81312776+tohtana@users.noreply.github.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['bf03f4835295bccc4765c9ae428c5d3e3ddb04fe'],False,"['stage_1_and_2.py', 'test_moe.py', 'simple_model.py']"
e06bb518aa7dafef1e98a1da63362e1586b2ddd5,"Add attribute check for language_model when replace last linear module (#6650)

Fix module has no attribute 'language_model' issue.

Co-authored-by: Masahiro Tanaka <81312776+tohtana@users.noreply.github.com>",['b647fb2470f8f6fefe5cab0ea84a2d89696eb898'],False,['replace_module.py']
6e6563d3c8d7527713cc48d4a3adce51f22e83a2,"fix init_device_mesh for torch 2.4 (#6614)

Start torch 2.4, in
[`init_device_mesh()`](https://github.com/pytorch/pytorch/blob/de4c2a3b4e89d96334dc678d1c3f2ae51a6630a0/torch/distributed/device_mesh.py#L915)
,device type with a GPU index, such as ""cuda:0"", is not allowed.


![image](https://github.com/user-attachments/assets/1ddb61bf-8a15-4e0a-9115-a3681d7f19ff)

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Masahiro Tanaka <81312776+tohtana@users.noreply.github.com>
Co-authored-by: Masahiro Tanaka <mtanaka@microsoft.com>",['e06bb518aa7dafef1e98a1da63362e1586b2ddd5'],False,['torch.py']
3d5cf739ead7c78f518a518ccaa15a323bd5c8da,"Fix dynamo issue (#6527)

Dynamo use faketensor to trace tensor ops. In some case, the mechanism
break compiling with deepspeed.

An example could be found at
https://gist.github.com/oraluben/9b8240c2fe482eb4382453d6c97a5f76, to
see issues, install deepspeed==0.14.4 instead of my fork

without this PR, llama cannot be compiled.

Detailed explanation:

1. `ZeROOrderedDict`
dynamo use deepcopy to copy tensors, which will call
`object.__reduce__`. When copying `ZeROOrderedDict`, the default
implementation do not copy its `_parent_module` and will lead to
failure.
2. `param` maybe faketensor and do not have `ds_status` yet, but during
tracing it's ok to just skip the `register_external_parameter`, it
should be done ways before.

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Masahiro Tanaka <81312776+tohtana@users.noreply.github.com>",['6e6563d3c8d7527713cc48d4a3adce51f22e83a2'],False,['parameter_offload.py']
5fb71c0a189408151c3a8968a7025fb61d94950b,"sequence parallel for uneven heads (#6392)

In sequence_parallel (Ulysses), the sequence parallel size is
constrained by the requirement to be divisible by the number of heads,
which prevents some models/workloads from setting a specific sequence
parallel size. This PR implements uneven all-to-all heads splitting.

- both support  batch first (b,s,...) and seq_len first(s,b..) layout.
- Added unit tests with numerical checks. Locally also tested with **7
heads with sp=4** and **20 heads with sp=8**, and it passed.

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Ma, Guokai <guokai.ma@gmail.com>
Co-authored-by: Masahiro Tanaka <81312776+tohtana@users.noreply.github.com>",['3d5cf739ead7c78f518a518ccaa15a323bd5c8da'],False,"['tp_shard.py', 'layer.py', 'groups.py', 'test_ulysses.py']"
24285d6c73d3e505262a42c91a9d1ba1d9ece154,"Add fallback for is_compiling (#6663)

Importing `torch.compiler.is_compiling` causes an error with an older
version of PyTorch.
This PR adds a fallback for `is_compiling` to use an equivalent function
of older PyTorch versions.

This will resolve #6656.

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['5fb71c0a189408151c3a8968a7025fb61d94950b'],False,"['compiler.py', 'logging.py']"
54903e09eb131bb7b69bfc154e3970d4958131b9,"Update profiler registration check (#6668)

Resolves #5432.",['24285d6c73d3e505262a42c91a9d1ba1d9ece154'],False,['profiler.py']
229960a5e9995643ce0ce957a57d847effdc41dc,"Add support for H100/sm_90 arch compilation (#6669)

Resolves: #6549",['54903e09eb131bb7b69bfc154e3970d4958131b9'],False,['builder.py']
0e11b081be237f9b1867a3af88479a23de11345f,"Update base docker image for A6000 GPU tests (#6681)

Update to a [container
(24.03)](https://docs.nvidia.com/deeplearning/frameworks/pytorch-release-notes/rel-24-03.html)
with python 3.10 as transformers dropped support for python 3.8 in their
latest release.

Note: nv-human-eval.yml was never completed and isn't used, it is just
updated for any potential future support.

Resolves: #6672",['e6357c28cd5cfaecab2e541c81e6d633b518e56e'],False,"['nv-a6000.yml', 'nv-human-eval.yml', 'nv-sd.yml']"
e4a247ed133c230db58a625d8008cb60c7ae0f41,"Fix training of pipeline based peft's lora model (#5477)

Hi, guys

I find there is an assert failure when I train huggingface's lora based
model in pipeline style.

Here is the whole steps that I created my model:
1)  Load the pre-trained chatglm-6b model from huggingface, as Model_A
2) Use huggingface's peft's `get_peft_model(...)` and my
`LoraConfig(...)` from Model_A to create the lora model, as Model_B
3)  Create my own pipeline based model Model_C from Model_B

And I run Model_C under 2 3090ti GPUs. And the assertion failure looks
like this:
```text
Traceback (most recent call last):
  File ""/home/ubuntu/proj/chatglm-finetuning/train_pipeline.py"", line 372, in <module>
    main()
  File ""/home/ubuntu/proj/chatglm-finetuning/train_pipeline.py"", line 351, in main
    loss = engine.train_batch(data_iter=train_dataloader)
  File ""/home/ubuntu/anaconda3/lib/python3.9/site-packages/deepspeed/runtime/pipe/engine.py"", line 375, in train_batch
    self._exec_schedule(sched)
  File ""/home/ubuntu/anaconda3/lib/python3.9/site-packages/deepspeed/runtime/pipe/engine.py"", line 1375, in _exec_schedule
    self._exec_instr(**cmd.kwargs)
  File ""/home/ubuntu/anaconda3/lib/python3.9/site-packages/deepspeed/runtime/pipe/engine.py"", line 276, in _exec_reduce_tied_grads
    dist.all_reduce(grad, group=group)
  File ""/home/ubuntu/anaconda3/lib/python3.9/site-packages/deepspeed/comm/comm.py"", line 117, in log_wrapper
    return func(*args, **kwargs)
  File ""/home/ubuntu/anaconda3/lib/python3.9/site-packages/deepspeed/comm/comm.py"", line 496, in all_reduce
    return cdb.all_reduce(tensor, op, group, async_op)
  File ""/home/ubuntu/anaconda3/lib/python3.9/site-packages/deepspeed/comm/torch.py"", line 159, in all_reduce
    return torch.distributed.all_reduce(tensor=tensor, op=op, group=group, async_op=async_op)
  File ""/home/ubuntu/anaconda3/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py"", line 1520, in all_reduce
    _check_single_tensor(tensor, ""tensor"")
  File ""/home/ubuntu/anaconda3/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py"", line 463, in _check_single_tensor
    raise RuntimeError(
RuntimeError: Invalid function argument. Expected parameter `tensor` to be of type torch.Tensor.
```

After some debugging, I find out the root cause is that my configuration
of lora (in below) only add extra lora layer(part) in qkv related layers
but not the embedding layer. So the whole embedding layer's parameters
are freezed.
```python
lora_config = LoraConfig(r=8, # copied from finetuning_lora.py
                        lora_alpha=32,
                        target_modules=[""query_key_value""],
                        lora_dropout=0.1,
                        bias=""none"",
                        task_type=""CAUSAL_LM"",
                        inference_mode=False,
                        )   
```
And in my implementation of pipeline based model, I declared the
embeding layer as a tied-layer. So the whole thing is that there are no
gradients at all for embedding layer, but embedding layer as the tied
layer needs to be synced between two gpus. The value of gradient is None
but is still passed to `all_reduce` operation.

Current, my fix is simple and add a check if this `grad` is None.

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Heyang Qin <heyangqin@microsoft.com>
Co-authored-by: Masahiro Tanaka <81312776+tohtana@users.noreply.github.com>",['07cac9e0217f14fde1a12a3c89ebe367fcee311a'],False,['engine.py']
9b547313c6c213bf6bff5227d0c9689ba1bd618a,"Update checkout action to latest version (#5021)

Latest checkout uses latest (non-deprecated) version of node (16 -> 20).

More information
[here](https://github.blog/changelog/2023-09-22-github-actions-transitioning-from-node-16-to-node-20/):
```
Node.js 16 actions are deprecated. Please update the following actions to use Node.js 20: actions/checkout@v3. For more information see: https://github.blog/changelog/2023-09-22-github-actions-transitioning-from-node-16-to-node-20/.
```

Checkout action: https://github.com/actions/checkout

Node 20 requires a minimum of Ubuntu 20.04, so workflows currently using
18.04 are failing/will fail.",['e4a247ed133c230db58a625d8008cb60c7ae0f41'],False,"['cpu-inference.yml', 'nv-lightning-v100.yml', 'nv-torch110-p40.yml', 'nv-torch110-v100.yml']"
ff1c54351f087da06f76243ae562504bb166d979,"fix memcpy issue on backward for zero-infinity (#6670)

This PR is similar to
[PR#5301](https://github.com/microsoft/DeepSpeed/pull/5301), that
optimizes the D2H time use pinned memory.

Previously, the D2H memcpy will be the bottleneck during the final
backward pass of each iteration for ZeRO-Infinity(offload), as shown in
Trace-1. The new version can eliminate the bottleneck, as shown in
Trace-2.

_Trace-1_
<img width=""480"" alt=""image""
src=""https://github.com/user-attachments/assets/891e3770-351b-4e03-8a59-b491bc44d03b"">

_Trace-2_
<img width=""192"" alt=""image""
src=""https://github.com/user-attachments/assets/f1cf9037-77f8-42a6-adc8-d5c6bacde0aa"">

cc @tjruwase

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['c7f58c899f6f099a35d968bdad973f24b842c8c6'],False,['stage3.py']
95ea95fcd642488519bb599e4618507f10f88494,"Free memory in universal checkpointing tests (#6693)

Tests in universal checkpointing were not freeing the engine after use
when `reuse_dist_env` was set to `True`, leading to memory leaks.
This PR ensure freeing the engine in the tests and enables
`reuse_dist_env`.

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['ff1c54351f087da06f76243ae562504bb166d979'],False,"['test_universal_checkpoint.py', 'common.py']"
6c08b7f932bc1c7acaec142c720f6f9a82e9e5c8,"Pin transformers to 4.45.2 in nv-ds-chat workflow (#6710)

This commit causes breaking changes we need to fix, for now we will pin
the version but we will fix shortly

https://github.com/huggingface/transformers/pull/33325",['9068acb6fbbdbaae5429bb89e507977128496bc5'],False,['nv-ds-chat.yml']
2b41d6212c160a3645691b77b210ba7dd957c23f,"[Bug Fix] Support threads_per_head < 64 for wavefront size of 64 (#6622)

When launching apply_rotary_pos_half kernel, only threads_per_head of 64
is supported for wavefront size of 64.
This change adds support for threads_per_head < 64 such as 4, 8, 16.

Fixes the issue introduced in
https://github.com/microsoft/DeepSpeed/pull/5402

---------

Signed-off-by: Jagadish Krishnamoorthy <jagadish.krishnamoorthy@amd.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Logan Adams <loadams@microsoft.com>",['6c08b7f932bc1c7acaec142c720f6f9a82e9e5c8'],False,"['apply_rotary_pos_emb.cu', 'test_rope.py']"
351569dd4a00dea0e00040a816cbc44b1e38a214,"Use one param coordinator for both train/inference scenarios (#6662)

The parameter coordinator in ZeRO3 throws a ""backward pass is invalid
for module in evaluation mode"" error when the training mode is
unexpected, as it expects all modules to be in training mode during the
backward pass. This is an unnecessarily strict restriction.
This PR relaxes the restriction by using a single parameter coordinator
(instead of separate ones for training and evaluation modes) and
resetting the prefetch state before starting a forward pass.

Use of `is_compiling` needs to be fixed after #6663 is merged.

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['2b41d6212c160a3645691b77b210ba7dd957c23f'],False,"['parameter_offload.py', 'partitioned_param_coordinator.py', 'stage3.py', 'test_zero.py']"
d2a4718946b544ab5d4f334f05a4ace9670e3ddd,"Update yapf version  (#6721)

This update is needed to support eventually running on ubuntu-24.04 from
GitHub, specifically because the python version is updated to 3.12 and
results in the following error: `ModuleNotFoundError: No module named
'lib2to3'` since that package is deprecated.",['351569dd4a00dea0e00040a816cbc44b1e38a214'],False,"['.pre-commit-config.yaml', '__init__.py', 'autotuner.py', 'elastic_agent.py', 'replace_module.py', 'config.py', 'eigenvalue.py', 'engine.py', 'utils.py', 'test_zero_context.py']"
08555662282f624f1258d45617aefef1577a4dd3,"Update GH hosted workflows to 24.04 (#6717)

`ubuntu-latset` is moving to be 24.04, so we should test updating as
well to ensure it doesn't break any of our workflows.",['057d25be6775105f4b9e1d41e6c21981a157c849'],False,"['cpu-torch-latest.yml', 'no-torch.yml', 'nv-pre-compile-ops.yml', 'release.yml']"
99e9cbed1663b13bbc240d79946913bfe430ffb5,"Fix Type Name Inconsistency & Typo in cpu_adam (#6732)

There is a typing error & inconsistency in cpu-adam code, while not
affecting functionality, impacts code readability. Specifically, the
type name `ds_params_percision_t` contains a typo ('percision'), whereas
the related type name `ds_state_precision_t` is spelled correctly. I
think it is beneficial to fix this typo&inconsistency to improve code
readability, maintainability and further development.
I have tested the corrected version of cpu_adam, and it compiles and
runs successfully.

Compilation Log:
<img width=""2560"" alt=""image""
src=""https://github.com/user-attachments/assets/b7bc307d-9c9d-4ab7-8671-34e565903ca5"">

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['b45ca2635495997bb294f4b9b9dbcb23db0dcac6'],False,"['cpu_adagrad.cpp', 'cpu_adam_impl.cpp', 'cpu_adagrad.h', 'cpu_adam.h', 'cpu_lion.h', 'cpu_lion_impl.cpp']"
73d974ee640a95a594be95bb68af00fd77e44409,"Add data type check for bf16 (#6742)

Add data type check for bf16 to fix #6723",['fabab197f747a4ab3ac9c2a7bdd97b6aaa1db698'],False,['engine.py']
7af3a4beb5bf99517bb2d51b450861ca54bed8d3,"add zero3 ```module_granularity_threshold ``` to zero optimization. (#6649)

This PR adds Z3 coalesced fetch to zero optimization. Currently, some
logic can be reused, but it's difficult to realize that as optimization
choice(I only discovered these logic when trying to implement it).

The benefit of this approach is reducing host overhead（reduce many
hooks) and during the process of recursive fetching parameters
(especially in fine-grained models, such as those with a large number of
moe experts). This is particularly helpful for host-sensitive devices
(such as hpu), where it achieved a 40% performance improvement in our
customer workloads.
FYI @delock @deepcharm

---------

Co-authored-by: Ma, Guokai <guokai.ma@gmail.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['73d974ee640a95a594be95bb68af00fd77e44409'],False,"['engine.py', 'config.py', 'parameter_offload.py', 'stage3.py', '__init__.py', 'z3_leaf_module.py', 'config-json.md', 'test_zero_leaf_module.py']"
b692cdea479fba8201584054d654f639e925a265,"AIO File Offsets (#6641)

Adding the option for a file offset to the read/write functions of AIO &
GDS ops.

---------

Co-authored-by: jomayeri <deepspeed@H100-VM2.shlnn55tgwve1eacvp21ie45dg.jx.internal.cloudapp.net>
Co-authored-by: Masahiro Tanaka <81312776+tohtana@users.noreply.github.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['7af3a4beb5bf99517bb2d51b450861ca54bed8d3'],False,"['cpu_accelerator.py', 'deepspeed_aio_utils.cpp', 'deepspeed_aio_utils.h', 'deepspeed_aio_op_desc.cpp', 'deepspeed_aio_op_desc.h', 'deepspeed_cpu_op.cpp', 'deepspeed_cpu_op.h', 'deepspeed_py_aio.cpp', 'deepspeed_py_io_handle.cpp', 'deepspeed_py_io_handle.h', 'py_ds_aio.cpp', 'ds_aio_handle.py', 'deepspeed_gds_op.cpp', 'deepspeed_gds_op.h', 'deepspeed_py_gds_handle.cpp', 'deepspeed_py_gds_handle.h', 'py_ds_gds.cpp', 'utils.py', 'numa.py', 'test_aio.py', 'test_gds.py']"
877aa0dba673c2aa2157029c28363b804d6ee03d,"Update path for BingBertSquad from DeepSpeedExamples (#6746)

In https://github.com/microsoft/DeepSpeedExamples/pull/245, the
DeepSpeedExamples directory structure was refactored, this updates the
DeepSpeed examples from those changes.",['b692cdea479fba8201584054d654f639e925a265'],False,"['bert-finetuning.md', 'onebit-adam.md', 'run_BingBertSquad.sh', 'run_BingBertSquad_sanity.sh', 'run_tests.sh', 'test_e2e_squad.py']"
fc4e73370d84af5242996a90b32b3ffce8e6b922,"Add no_sync context manager (#6675)

Fix #1902

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['d702eb5f79bcfc8a7afa735b70762633cd5a56e9'],False,"['layer_container_base.py', 'engine.py', 'stage_1_and_2.py', 'test_no_sync_ctxt.py']"
dd40269426e129e6d65f8cda28d97f5ca31d1de0,"A faster and more memory-efficient implementation of `zero_to_fp32` (#6658)

It is a faster and more memory-efficient implementation of
`zero_to_fp32`.


The previous version double the memory usage, which cause cpu OOM for
very large models (e.g. llama 405B).

https://github.com/microsoft/DeepSpeed/blob/b647fb2470f8f6fefe5cab0ea84a2d89696eb898/deepspeed/utils/zero_to_fp32.py#L438-L441


## How does it work?

1. **Lazy loading**: Load checkpoint with `mmap=True`, thus the weights
are mmaped rather than loading all the storages into memory.
2. **Lazy merge**: `GatheredTensor` contains the mmaped weights and
tensor offset. It is a memory-efficient pseudo tensor. Only when
`tensor.contiguous()` is called, it starts to load related weights to
memory and merge into a single tensor.
3. **Release memory in time**: Save checkpoints shard by shard, and
release the memory once a shard is saved.


Throughout the process, only one shard of tensors are keeped in memory.

## How much benefit in speed and memory ?

Experiments were conducted on a linux host with 1TB of memory. Here is a
detailed comparision
| | world size | peak memory(GB) | elapsed time(h:mm:ss) |

|----------------------|------------|--------------|--------------------|
| llama3-8B(old->new)  | 8          | 90 -> 41 | 0:02:17 -> 0:01:10 |
| llama2-13B(old->new)  | 8        | 146 -> 54 | 0:02:30 -> 0:01:47  |
| llama2-70B(old->new)  | 16        | 789 -> 159 | 0:20:47 -> 0:20:45 |
| qwen1.5-110B(old->new)  | 32       | OOM -> 217 | ? -> 0:34:21 |
| llama3-405B(old->new)  | 192      | OOM -> 262 | ? -> 2:09:59 |



You can reproduce with the following scripts
```sh
# 1. install requirments
apt-get install time
# 2. prepare zero-3 checkpoints
# 3. convert zero to fp32 checkpoints
/usr/bin/time -v python zero_to_fp32.py . output_dir/ --safe_serialization
```

- **memory**: Theoretically, this PR reduces the memory cost from `2M`
to `(1/n)M`, where `M` is the memory cost of the full weights, `n` is
num_shards.
- **speed**: The speed gain mainly comes from avoiding extra tensor
copying. The benifit may be slight.




## Impl history

-
[v1](https://github.com/xu-song/DeepSpeed/commit/19712a1c75bfc1da4a7f3ecca6915a86af671568#diff-6a2ca3427fa608c387b7351359f98cfc1313be6e960cee86344ff246bf1b8326R441-R447)
: a hf_hub compatible approach.
It has been discarded due to the controversial implementation of
`data_ptr().`
- [v2](https://github.com/microsoft/DeepSpeed/pull/6658/files): a simple
approach with `torch.empty`

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['f594dbe3dfe01cb912b9f3260906fe51fc1fdc61'],False,"['zero_to_fp32.py', 'test_convert_checkpoint.py']"
8488beea29cbefaf964ce58063d6d709a31a54cc,"Pin transformers version to work around latest torch requirements (#6759)

Latest transformers seems to break our tests that aren't on torch latest
(>= 2.5). Issue opened here:
https://github.com/huggingface/transformers/issues/34795. This pins our
version so these tests can pass in the meantime.",['dd40269426e129e6d65f8cda28d97f5ca31d1de0'],False,"['hpu-gaudi2.yml', 'nv-a6000.yml']"
2e0c39b55ce55dfeeff1224203d9035555e228d9,"Add explicit parameters for torch.load (#6751)

Successor PR to #6094:

> FutureWarning: You are using torch.load with weights_only=False (the
current default value), which uses the default pickle module implicitly.
It is possible to construct malicious pickle data which will execute
arbitrary code during unpickling (See
https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models
for more details). In a future release, the default value for
weights_only will be flipped to True. This limits the functions that
could be executed during unpickling. Arbitrary objects will no longer be
allowed to be loaded via this mode unless they are explicitly
allowlisted by the user via torch.serialization.add_safe_globals. We
recommend you start setting weights_only=True for any use case where you
don't have full control of the loaded file. Please open an issue on
GitHub for any issues related to this experimental feature.

Todo:
- [ ] Update values in non-test files to True where necessary.",['1fdad1fa52f525d64132cb11e2746b06751efb22'],False,"['deepspeed_checkpoint.py', 'ds_to_universal.py', 'universal_checkpoint.py', 'zero_checkpoint.py', 'engine.py', 'huggingface_engine.py', 'inference_policy_base.py', 'replace_module.py', 'base_optimizer.py', 'nebula_checkpoint_engine.py', 'torch_checkpoint_engine.py', 'stage3.py', 'zero_to_fp32.py', 'common.py', 'test_universal_checkpoint.py', 'test_zero_optimizer.py', 'test_configurable_parallel_mp.py', 'test_configurable_parallel_pp.py']"
065398d5de1019af1117454268813c14c36a8b74,"Fix setup.py bash cmd generation to correctly extract git info (#6762)

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['2e0c39b55ce55dfeeff1224203d9035555e228d9'],False,['setup.py']
b5709cce66ac09c879054dba0576e8ab0d770f73,"Enable torch compile on _allgather_params (#6769)

* Previosuly ZerO3 was crashing when trying to compile _allgather_params
* Disabling grad solves the issue",['83e4364fbd921ad4fe808d31eb107ef080228a7a'],False,['partition_parameters.py']
f515104e95f3b337f0cb59bb9e87439b44077799,"Removes unnecessary cloning (#6761)

`clone_tensors_for_torch_save()` function:

When the `item.device` is different from `device` input,
`tensor.clone()` is not actually required because `to()` function also
clones the original tensor.


+) I observed memory bloat under following conditions:
* Training a Whisper model w/ `transformers` framework with `ZeRO-0` and
`ZeRO-1` configuration.
* Memory bloating can be observed every time the model state_dict is
cloned using `clone_tensors_for_torch_save()`

After I removed the unnecessary `clone()`, seems like the problem is
solved.

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['b5709cce66ac09c879054dba0576e8ab0d770f73'],False,['utils.py']
cd20a3bbc7713908d7fb5fd7af4a91d52f126370,"Fix potential memory issues when use deepspeed Z3 (#6726)

I had OOM problem when doing DPO training using zero3. It needs to call
module twice in one training step, and second call is with no_grad().
The problem is caused by two bugs:
1. ""__n_available_params"", which helps to control fetched parameters,
becomes negative after release_and_reset_all() function.
2. module.ds_grads_remaining becomes negative in backward() if we call
module more than once in one training step.

I tried to create two patches to fix these issues.

---------

Signed-off-by: Wenbin Chen <wenbin.chen@intel.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Hongwei Chen <33092912+hwchen2017@users.noreply.github.com>",['f515104e95f3b337f0cb59bb9e87439b44077799'],False,"['parameter_offload.py', 'partitioned_param_coordinator.py', 'test_zero_multiple_run.py']"
f57b1ef18af25f7e0450d92d5f163ee8c887372a,"Unpin with latest transformers fixes (#6763)

Reverts #6759

Requires from transformers: 
https://github.com/huggingface/transformers/pull/34816
https://github.com/huggingface/transformers/pull/34800

Todo:
- [x] Need to merge first PR to get support for torch 2.4",['cd20a3bbc7713908d7fb5fd7af4a91d52f126370'],False,"['hpu-gaudi2.yml', 'nv-a6000.yml']"
5e16f255a63620b37c8cbe499f67edae9223df9e,"docs: fix HF links (#6780)

The current link
https://huggingface.co/docs/transformers/main_classes/deepspeed is very
unhelpful.

It turns out in the past it had some guides:
https://huggingface.co/docs/transformers/v4.27.1/main_classes/deepspeed#shared-configuration

Later it's refreshed and moved to
https://huggingface.co/docs/transformers/deepspeed",['f57b1ef18af25f7e0450d92d5f163ee8c887372a'],False,"['README.md', 'README.md', 'README.md', 'getting-started.md', 'index.md', 'README.md']"
d6410f9051b23359930b548de2067543a30e808d,"Fix Doc Error: ZeRO Stage 2 gradient partitioning (#6775)

Fix the issue described in
https://github.com/microsoft/DeepSpeed/issues/6707",['5e16f255a63620b37c8cbe499f67edae9223df9e'],False,"['zero.md', 'zero3.rst']"
fabcf407f9eb0f8a7c9052cd85fe11b012cc4104,"Cleanup code docs warnings (#6783)

We have a number of warnings in our readthedocs sphinx/autodoc .rst
files, so this cleans some of those up so we can fix real issues there.",['d6410f9051b23359930b548de2067543a30e808d'],False,"['index.rst', 'inference-engine.rst', 'initialize.rst', 'moe.rst', 'schedulers.rst', 'zero3.rst']"
fc230070ef3d12bbacfca5205506e648cc4165bc,"Fix zero checkpoint (#6792)

Fix #6791

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['0c6c98110947300636937d45d6b3392c84996130'],False,['zero_to_fp32.py']
60a1b57b98c61c322cc76f1936eaec4f18a77b06,"Adding the new feature of FPDT (#6462)

[FPDT](https://arxiv.org/abs/2408.16978) can only be used with [this
version](https://github.com/microsoft/Megatron-DeepSpeed/pull/441) of
Megatron-DeepSpeed.

---------

Co-authored-by: Jinghan Yao <yjhmitweb@ascend-rw02.ten.osc.edu>
Co-authored-by: Sam Ade Jacobs <samjacobs@microsoft.com>
Co-authored-by: Jinghan Yao <yjhmitweb@ascend-rw01.ten.osc.edu>
Co-authored-by: Logan Adams <loadams@microsoft.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Jinghan Yao <yjhmitweb@cardinal-rw02.ten.osc.edu>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Masahiro Tanaka <81312776+tohtana@users.noreply.github.com>
Co-authored-by: Masahiro Tanaka <mtanaka@microsoft.com>",['ed7d183bed94d5b48b270586274a76cf6de8902c'],False,"['nv-flash-attn.yml', 'checkpointing.py', 'fpdt_layer.py', 'layer.py', 'requirements.txt', 'test_ulysses.py']"
b966e1f97f537a46474582143c2a4e3398adc66d,Pin transformers to avoid errors with latest version (#6820),['60a1b57b98c61c322cc76f1936eaec4f18a77b06'],False,['nv-torch-latest-v100.yml']
0e92f9b41f864c034aea0a7eec7267f48bb70f80,"Update README.md (#6824)

Fix broken tutorial link",['7b9fc8c74dd989ab2910dc26538623237f267b47'],False,['README.md']
95ead2a055afe0387b18eb7af1c1b973304b7553,"Pin transformers version in cpu-torch-latest due to multiprocessing error. (#6823)

This is a copy of https://github.com/microsoft/DeepSpeed/pull/6820 for
the cpu-torch-latest tests.

This PR will revert/fix these:
https://github.com/microsoft/DeepSpeed/pull/6822",['2ea181f0c33206bd8362aadcbd732d5b1314adc7'],False,['cpu-torch-latest.yml']
9ca60160179aacc1efe87121e1ec259ce8d0a476,"Pin HPU tests (#6831)

HPU tests are impacted by the same issue as other tests that use
transformers latest. This PR pins to a version of transformers before
the fix.",['a4499668fe98d609534e5b629b69e8891f885e42'],False,['hpu-gaudi2.yml']
0c92c39dd0a870abffb61e3dc219f12539917c14,"Inference UTs check for trition support from accelerator (#6782)

      Instead of checking if installed or not check for support. Skip if not
supported.

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['08b907a226c64fbadace5dba71ed45bbe22bc0ef'],False,"['test_attention.py', 'test_gelu.py', 'test_layer_norm.py', 'test_matmul.py', 'test_residual_add.py', 'test_softmax.py']"
06f1d3609e46420cbcee606a64c60af1a6fae111,"Unpin pytest-subtests now that 0.14.1 is released (#6844)

The issue we encountered was covered here:
https://github.com/pytest-dev/pytest-subtests/issues/173

And is resolved with the latest changes from this PR:
https://github.com/pytest-dev/pytest-subtests/issues/174, and is
published in the latest version 0.14.1.",['0c92c39dd0a870abffb61e3dc219f12539917c14'],False,['nv-accelerate-v100.yml']
1b58ba5ec04493a112fae10d9cc9c824dfbd40ca,"Merge LoCo with Zero++ (#6730)

      ### Integration of LoCo Method into ZeRO++

#### Overview
This PR introduces the integration of the **LoCo** method, as outlined
in [this paper](https://arxiv.org/abs/2407.04480), into the ZeRO++
framework of DeepSpeed. The key enhancement involves applying error
feedback compensation to 4-bit gradients before communication. This
approach ***improves pre-training loss outcomes without additional time
overhead***, though it requires extra GPU memory. The extent of this
memory increase depends on model size and training configuration.

#### Experimental Results
We conducted pre-training experiments using the Llama2 architecture,
adjusting the number of layers and hidden size. The experiments
included:
- **A smaller-scale model with 0.8B parameters trained on 30B tokens**.
- **A larger-scale model with 8B parameters trained on 5B tokens**.

The training data was sampled from **Redpajama-V2**.
<p align=""center"">
<img
src=""https://github.com/user-attachments/assets/e7db9487-728c-4a17-9806-c15afa12f62e""
width=""49%"" />
<img
src=""https://github.com/user-attachments/assets/3efec895-b71d-43ab-b5ce-65468ba8b9f1""
width=""49%"" />
</p>

**Findings**:
- **Smaller Models (0.8B parameters)**: Significant gains were observed
when applying the LoCo method.
- **Larger Models (8B parameters)**: The gains were present but less
pronounced. This could be due to:
  1. Relatively smaller data volume.
2. Lower pre-training loss for larger models, making significant
improvements harder to achieve.

However, even a smaller pre-training loss gap in larger models can
translate to meaningful gains in downstream tasks.

#### Example Script
For reference, the
[run.sh](https://github.com/user-attachments/files/17679552/zeroplus-7b3.zip)
script used for the 8B parameter, 5B tokens experiment is attached. The
experiment was conducted using the **DeepSpeed-Megatron** platform.



#### Acknowledgments
Special thanks to cc @GuanhuaWang for ongoing communication and guidance
throughout this work.

---

We appreciate your consideration of this PR and welcome any feedback or
questions!

---------

Co-authored-by: ChuanxinTang <tangchuanxin.chn@gmail.com>
Co-authored-by: root <pan.jiachun@outlook.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Logan Adams <loadams@microsoft.com>
Co-authored-by: Hongwei Chen <33092912+hwchen2017@users.noreply.github.com>",['06f1d3609e46420cbcee606a64c60af1a6fae111'],False,"['quantization.h', 'quantization_utils.h', 'pt_binding.cpp', 'quant_reduce.cu', 'swizzled_quantize.cu', 'coalesced_collectives.py', 'engine.py', 'config.py', 'stage3.py', 'test_coalesced_collectives.py']"
9e3125281f992a15486ca083f1acfa5f61931497,"Fix type error in `ZeROOrderedDict` (#6794)

As @keskival pointed in
https://github.com/microsoft/DeepSpeed/commit/3d5cf739ead7c78f518a518ccaa15a323bd5c8da#r149582004,
I've confirmed there's a type error, which this PR fixes.

I didn't run into this because our internal version still use `*r2`.

Co-authored-by: Tero Keski-Valkama <tero.keskivalkama@gmail.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['1b58ba5ec04493a112fae10d9cc9c824dfbd40ca'],False,['parameter_offload.py']
ecb4bf32614fa8d6d4433ea579ca867008d34140,"Fix uneven head sequence parallelism bug (#6774) (#6797)

Here `gather_idx < 2` represents `is_first_all2all`. During the first
all2all, `uneven_head_all2all` will be called if either `num_heads %
seq_world_size != 0` or `get_num_kv_heads() is None`.

During the second all2all, it'll return return `uneven_head_all2all` if
and only if `get_num_kv_heads() is None` which is always set during the
first uneven all2all. This means that there will no longer be issue
where `uneven_head_all2all ` is returned for the second all2all because
of `num_heads % seq_world_size != 0`.

Fixes: #6774

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['9e3125281f992a15486ca083f1acfa5f61931497'],False,['layer.py']
074d5c69c3eeffdf34b406153b9ece971ae7f115,Fix nv-torch-nightly test by pinning transformers (#6849),['ecb4bf32614fa8d6d4433ea579ca867008d34140'],False,['nv-torch-nightly-v100.yml']
bd6fd50e9f49a81d71284a49b8449c334bf11074,"Remove broken links to non-active site (#6854)

The site referenced in various places on the README is no longer active:
https://deepspeed4science.ai


![image](https://github.com/user-attachments/assets/8ec47313-2abf-40d6-b1f8-9a9234c15e2f)

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['074d5c69c3eeffdf34b406153b9ece971ae7f115'],False,['README.md']
91829476a8fd4d0d9268c03c1d56795d20a51c12,"Avoid poisoning process with CUDA calls as soon as importing (#6810)

Call `torch.cuda.device_count() > 0` before `torch.cuda.is_available()`,
to give priority to nvml based availability, so that we can try not to
poison process with CUDA calls as soon as we execute `import deepspeed`.


https://github.com/pytorch/pytorch/blob/v2.5.1/torch/cuda/__init__.py#L120-L124

There are 2 reasons to make this change:

Firstly, if we accidentally import deepspeed, since the CUDA runtime
initializes when the first CUDA API call is made and caches the device
list, changing the CUDA_VISIBLE_DEVICES within the same process after
initialization won't have any effect on the visible devices. The
specific case:
https://github.com/OpenRLHF/OpenRLHF/pull/524#issuecomment-2501505023

A demo for reproduction before the fix is applied:

```python
import torch
import os
os.environ[""CUDA_VISIBLE_DEVICES""] = """"
import deepspeed
os.environ[""CUDA_VISIBLE_DEVICES""] = ""0,1,2,3""
torch.cuda.set_device('cuda:0')
```

Secondly, https://pytorch.org/docs/stable/notes/cuda.html

When assessing the availability of CUDA in a given environment
(is_available()), PyTorch’s default behavior is to call the CUDA Runtime
API method cudaGetDeviceCount. Because this call in turn initializes the
CUDA Driver API (via cuInit) if it is not already initialized,
subsequent forks of a process that has run is_available() will fail with
a CUDA initialization error.

Signed-off-by: Hollow Man <hollowman@opensuse.org>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['bd6fd50e9f49a81d71284a49b8449c334bf11074'],False,['real_accelerator.py']
853a97648b9ba3acbb990018eab1dd928a08c390,"Fix xpu tests workflow failure by changing pip index url (#6864)

Update xpu-max1100.yml and xpu-compile.yml",['91829476a8fd4d0d9268c03c1d56795d20a51c12'],False,"['xpu-compile.yml', 'xpu-max1100.yml']"
fc7c07007fe341bf6d78a9126d0cb5a914ce28fd,"Update real_accelerator.py (#6845)

### Comment out or delete `accelerate_name=""cpu""` when `xpu` is not
detected.
When `xpu `is not detected it just pass at lines from 68 to 74 if
`DS_ACCELERATOR` is set. However, `cpu` is assigned to `accelerate_name`
if it cannot import `intel_extension_for_pytorch` or find` xpu`, namely,
at line from 125 to 133 when`DS_ACCELERATOR` is not set.

I found this problem yesterday and spent whole afternoon figuring it
out. I got `intel_extension_for_pytorch `installed with other package
which I do not use actually and have no idea about this. Then I found
that it `cpu` is assigned to accelerate_name directly if it cannot find
`xpu` and it affects `cuda` detection. In fact, `cpu` will be assigned
finally if `cuda` is even not detected at line from 170 to 177.

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Logan Adams <loadams@microsoft.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['6e3e13cb280b684ebedb5c2aecb36efb545ebfce'],False,['real_accelerator.py']
db98cc3ad1e0a20807e0c2513f0eee40f626860e,"Fix assertion for offloading states (#6855)

This PR fixes the assertions in `offload_states` method mentioned in
#6833.

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['fc7c07007fe341bf6d78a9126d0cb5a914ce28fd'],False,['engine.py']
87c650681eb285ab34a69a011b520f756f42d4b9,"Remove pin from transformers version and fix Processing/Threading issues in tests (#6822)

Changes from https://github.com/huggingface/transformers/pull/34966
caused the `nv-torch-latest-v100` tests to fail with the following
error:

```
  File ""/tmp/azureml/cr/j/e4bfd57a509846d6bbc4914639ad248d/exe/wd/actions-runner/_work/DeepSpeed/DeepSpeed/unit-test-venv/lib/python3.10/site-packages/transformers/modeling_utils.py"", line 3941, in from_pretrained
    raise EnvironmentError(
OSError: Can't load the model for 'hf-internal-testing/tiny-random-VisionEncoderDecoderModel-vit-gpt2'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'hf-internal-testing/tiny-random-VisionEncoderDecoderModel-vit-gpt2' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.
```

Sample failure here:
https://github.com/microsoft/DeepSpeed/actions/runs/12169422174/job/33942348835?pr=6794#step:8:3506

This was resolved on the Transformers side here:
https://github.com/huggingface/transformers/pull/35236",['db98cc3ad1e0a20807e0c2513f0eee40f626860e'],False,"['cpu-torch-latest.yml', 'nv-torch-latest-v100.yml', 'nv-torch-nightly-v100.yml']"
a964e435532699908e5750abdb027ae583ff793d,"Fix --enable_each_rank_log when used with PDSH multi-node runner (#6863)

This PR addresses fixes
https://github.com/microsoft/DeepSpeed/issues/6859 by threading this
argument into the deepspeed launcher command build by PDSHRunner.

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['da771ed42e41a44d5047813ca4672f1cfe9d1731'],False,['multinode_runner.py']
2f32966b1cd874aa4373177c8f8c4214ad57d020,Update transformers ops unit tests to use `requried_torch_version` (#6884),['a964e435532699908e5750abdb027ae583ff793d'],False,"['test_bias_geglu.py', 'test_bias_gelu.py', 'test_bias_relu.py', 'test_gelu.py', 'test_matmul.py', 'test_softmax.py']"
4cd1d97460b677563d57f07a293724bdc02e0ef5,"Don't error out when cpu accelerator doesn't have torch (as default for whl building) (#6886)

This fixes a bug introduced in #6845, which breaks the `no-torch`
workflow that we require in order to do releases where we do not require
torch to be in the environment when building an sdist. This adds the
same logic to the cpuaccelerator that the cudaaccelerator had where we
don't require torch to be installed to build the whl.",['2f32966b1cd874aa4373177c8f8c4214ad57d020'],False,"['no-torch.yml', 'cpu_accelerator.py']"
0b25630abe8f7dd4e64c277ff92f5f7e36a27284,"Add arctic model support by adding w2 to all_reduce (#6856)

As title says. 

Default behavior of arctic model produces shape issues with AutoTP due
to the MLP layer performing `w2 * act(w1*w3)`. However, method provided
to fix Mixtral-7x8b in #5257 does not work since the MLP for Arctic is
also used within a ModuleList for the MoE. This results in MLP weights
hiding behind individual experts as layers `#.w#`, which is not caught
by the fix in #5257. This adds the check directly within replace, where
it can check for actual layer names for the `w2` key in the model to
patch with `all_reduce`.

---------

Signed-off-by: Daniel Huang <daniel1.huang@intel.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['4cd1d97460b677563d57f07a293724bdc02e0ef5'],False,"['auto_tp.py', 'automatic-tensor-parallelism.md']"
4fd79205c6b85b47e00810143c69d342ce874ede,"Allow to compile collective for PT>2.3 (#6899)

Allow to compile collective for PT>2.3
commit re-uploaded due to github CI issue
originally uploaded by @nelyahu",['f9e158a0f5cfa08b475cc1f086accffd8a77b92f'],False,['torch.py']
00ea0c46c2296db158d10497602f9832c4445d84,"Zero2: avoid graph breaks in torch.compile by using param_idx (#6803)

inside reduce_independent_p_g_buckets_and_remove_grads and in
reduce_ipg_grads which are being executed during the BWD hook in zero2,
the model param is being stored inside params_in_ipg_bucket.
torch.compile has hard time tracing parameters.
By using the param's static index inside the group the same logic can be
maintain with less complexity.

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Logan Adams <loadams@microsoft.com>",['4fd79205c6b85b47e00810143c69d342ce874ede'],False,"['stage_1_and_2.py', 'test_moe.py']"
85cc5f9bb3f0175a2d13ea1ed65bf7d202b7f0d9,"Fix error caused by all_reduce call in domino (#6880)

Fix #6851 
Initialize communication backend to fix error caused by all_reduce call
in the Domino transformer layer.
Verified correctness in local test.

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['eea5304807c6a04d0f2c55cb935ec295235d9b54'],False,['transformer.py']
3573858e7ce2c723b8c43231c6c6b0cf97dca2fc,"Change compile for pipeline module torch.compile (#6478)

We have encountered and issue with torch.compile and the pipeline
module.
modifying a member of the module (micro_offset) during the forward
function will cause torch compile to restart the analysis and treat the
module as dynamic.
In order to bypass this issue without significantly changing the way the
pipeline module works we propose to compile only the layers in the
pipeline module instead of the forward function of pipeline module. this
will bypass the issue and should still give most of the benefit of torch
compiling the pipeline module while avoiding the issue.

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['cc03c76d57f41752d8cfb84c2e45b8e0da8083da'],False,"['module.py', 'test_pipe_module.py']"
456c9ac67975da698e44dfd4f90c4f7b867d08bd,"Stage3: Use new torch grad accumulation hooks API (#6773)

      * This commit addresses a Deepspeed issue
[#6718](https://github.com/microsoft/DeepSpeed/issues/6718)
* The existing code has been using the grad_acc node hook to reduce
params grads.
The constructs such as `param.data = replicated_tensor.data` used in
`allgather_params(..)`
are compiled into `param.set()` causing the hook assigned to the
grad_acc node not being called.
* Starting from PyTorch 2.1 there is a new and robust hook API on a
param itself: `param.register_post_accumulate_grad_hook(..)`
* This commit will make use of the proper API depending on the PyTorch
version
* It will also disable compile for PyTorch versions < 2.1

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Masahiro Tanaka <81312776+tohtana@users.noreply.github.com>",['3573858e7ce2c723b8c43231c6c6b0cf97dca2fc'],False,"['compiler.py', 'stage3.py', 'torch.py']"
a8ede3a9df556a5d6beb22a4bf38fa9852b3bada,Cleanup ops/transformer/inference tests (#6830),['456c9ac67975da698e44dfd4f90c4f7b867d08bd'],False,"['test_bias_add.py', 'test_bias_gelu.py', 'test_matmul.py']"
0dbbb70b99f7f251996128115a53c1c8397efa8f,"Fix `checkpointable_layers` Logic (#6881)

**Problem**

There's an edge-case in DeepSpeed, where if all three of the following
are true:
1. Deepspeed activation checkpointing is applied 
2. The user passes `checkpointable_layers` (e.g.
https://github.com/EleutherAI/gpt-neox/blob/f5325805678c2b9e35aae4528283e0132c5f5bbc/megatron/model/gpt2_model.py#L175)
3. The user's model class contains `GPT2ModelPipe` or GPTModelPipe`

Then the `checkpointable_layers` will not be activation checkpointed. 

**Reason**

This is because in the current logic, `_is_checkpointable` will
short-circuit to just return layers matching
`ParallelTransformerLayerPipe` in the case of `self.__class__.__name__
in ('GPTModelPipe', 'GPT2ModelPipe')`. See
https://github.com/microsoft/DeepSpeed/blob/da771ed42e41a44d5047813ca4672f1cfe9d1731/deepspeed/runtime/pipe/module.py#L653

**Proposed Fixes**

I think that `checkpointable_layers` should always be checked for, and
added logic to this effect. I also found the documentation for
`checkpointable_layers` confusing and contradictory, so I updated the
docstring. Lastly, I added a unit test for `checkpointable_layers`.

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['a8ede3a9df556a5d6beb22a4bf38fa9852b3bada'],False,"['module.py', 'test_activation_checkpointing.py']"
f8c9f314ffe7eddfbd3645a94143301e610f68de,"[BUG FIX]:fix get torch.version.cuda error when cuda is None in rocm (#6909)

HI, I found some error when using deepspeed with rocm-torch
```
torch_cuda_version = ""."".join(torch.version.cuda.split('.')[:2]) 
```
will raise an AttributeError when torch.version.cuda is None. This
occurs because the CUDA version in rocm-torch/version.py is set to
always be None, leading to potential runtime errors in environments
where ROCm is being used.

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['0dbbb70b99f7f251996128115a53c1c8397efa8f'],False,['builder.py']
c5e48f49d8368216b4f99ef4023d2855f1ce3983,"Add fp8_gemm fallback for non-triton systems (#6916)

- Removed try/except from __init__ file in fp_quantizer and added a
single entry point instead
- Renamed file fp8_gemm to fp8_gemm_triton, and the function matmul_fp8
to matmul_fp8_triton
- Added a new entry point fp8_gemm with matmul_fp8 inside, and if the
system supports triton it calls the triton implementation and if not it
calls the fallback

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['f8c9f314ffe7eddfbd3645a94143301e610f68de'],False,"['__init__.py', 'fp8_gemm.py', 'fp8_gemm_triton.py', 'test_fp8_gemm.py']"
b0040b6ca4799c34ebb7543e4edf6658505d9dc6,"Reduce the device bubble introduced by heavy loop synchronization in coalesced fetch/release(z3_leaf_module) (#6694)

depend on https://github.com/microsoft/DeepSpeed/pull/6649

When performing fetch/release operations on Z3 leaf modules, the loop
time is excessively long in fine-grained module. Compared to non-leaf
modules, Z3 leaf modules may include a larger number of parameters.
Although each loop unit does not consume much time, the overall loop
length can be significant.

![image](https://github.com/user-attachments/assets/9891835a-2620-47f3-aba6-ea22b8905d1c)
**The fetch time is impacted by:**

Post-allgather operations (narrow， slice ，cat, difficult to avoid)
Memory pressure（record_stream/fetch event create&sync）
**The release time is impacted by:**
slice
Free parameter record_stream

Considering the fine-grained leaf modules, where each parameter is
relatively small, we can treat the parameters within each leaf module as
a unified entity to handle memory pressure. This approach can
approximately halve the CPU time required for fetch/release operations.

---------

Co-authored-by: Ma, Guokai <guokai.ma@gmail.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['c5e48f49d8368216b4f99ef4023d2855f1ce3983'],False,"['mics.py', 'parameter_offload.py', 'partition_parameters.py', 'partitioned_param_coordinator.py']"
c348c5b11a4fd3f70a53f5c9445f260472de47a8,Cleanup ops/transformer/inference tests (#6925),['b0040b6ca4799c34ebb7543e4edf6658505d9dc6'],False,"['inference_test_utils.py', 'test_attention.py', 'test_layer_norm.py']"
b62c84d88db26f6ce8c8f9caeef132638599a74d,"Fix building on Windows with presence of Triton (#6749)

This fixes some errors when installing DeepSpeed on Windows with the
presence of Triton.

I guess we can assume we don't need the warning about NFS on Windows for
now. I did not try how to detect NFS path on Windows, but we can detect
UNC path starting with `\\` if needed.

`os.rename` does not allow overwriting the file on Windows, and
`os.replace` is more cross-platform.

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['6628127a379e3c3fd70e48534c4b5952c5c23a72'],False,['matmul_ext.py']
53fb5795a10ed25a824f134dd44cb625d8ad23ac,Fix windows blog examples (#6934),['b62c84d88db26f6ce8c8f9caeef132638599a74d'],False,['README.md']
396f8db793b37db9b11847df8245f85bc57eeaa3,"Remove op compilation flags due to perf issue (#6944)

in some scenarios some of the optimization
flags for the ops compiler for HPU can cause
a significant performance degradation. 
remove the flags until the issue is resolved",['fa8db5cf2f9cf724fd2703353d40e3b37a8e7310'],False,['builder.py']
fae714d6bdf45ab0f6b98554d542a0f4e04a0eb9,"[inf] Add config var to enable keeping module on host (#6846)

Using keep_module_on_host config var will let us control if the loaded
checkpoints to model parameters will be moved to the device or stay on
host

---------

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['66d3d3e94dbdfbbf6535cab66256c238983fc7c3'],False,"['config.py', 'engine.py', 'auto_tp.py', 'replace_module.py', 'test_inference.py']"
f97f0885cf942aab1559d0f6a060d4801bff3a61,"Update import for torchvision.transformers (#6958)

Fixes import - found via
[torchfix](https://github.com/pytorch-labs/torchfix).",['018ece5af2d89a11a4a235f81f94496c78b4f990'],False,['alexnet_model.py']
470dd6dcebf712d620b95c9112fafb19dc94ba44,"Precisely track nvme optimizer offload (#6963)

Fix #4998",['de4596bedc61100db9385b5d99efd9025db13a7d'],False,"['engine.py', 'optimizer_utils.py', 'stage3.py', 'test_nvme_checkpointing.py']"
8ad487254c486163177f1947becf8b35287c6d05,Update torch versions to support 2.6 (#6977),['46c6c9e6ce9ede0eeb8a909958a4fecd2fbd942e'],False,['cpu-torch-latest.yml']
593de925cf32d7042e56e19ad9c91d94709915d1,"generalize deepspeed linear and implement it for non cuda systems (#6932)

Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['8ad487254c486163177f1947becf8b35287c6d05'],False,"['fp_quantize.cpp', 'config.py', 'quantization.py', 'quantize.py', 'fp_quantizer.py', 'fp_quantizer.py', 'test_linear.py', 'test_quant_param.py', 'test_fp8_gemm.py', 'test_fp_quant.py']"
065ca8a61d1c071c3ae0354abf25914aff04e789,"Title: Fix setup_env_ranks to Properly Set Environment Variables Instead of Raising Error (#6979)

This pull request addresses an issue in setup_env_ranks where, under
certain conditions, the function raises an error instead of setting the
necessary MPI-related environment variables (LOCAL_RANK, RANK, and
WORLD_SIZE). The intended behavior is to properly map Open MPI variables
(OMPI_COMM_WORLD_*) to the variables expected by DeepSpeed/PyTorch, but
the code currently raises an EnvironmentError if these Open MPI
variables are not found.

With this fix, setup_env_ranks will:

- Correctly detect and map the required Open MPI environment variables.
- Only raise an error if there is genuinely no valid way to obtain rank
information from the environment (e.g., both Open MPI variables and any
fallback mechanism are unavailable).

Fix #6895

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['8bb4d442ad9849ebb8b1340d2106da7c9fce8d5b'],False,"['multinode_runner.py', 'test_multinode_runner.py']"
c963c21f5d93759725f77c4926b38714c2f00650,"Specify torchvision in nv-ds-chat workflow (prevents errors with torch 2.6) (#6982)

Fixes #6984.

The workflow was pulling the updated torch 2.6, which caused CI
failures. This keeps us on torch 2.5 for now, since installing
torchvision as a dependency later on was pulling torch 2.6 with it which
was unintended.

This PR also unsets NCCL_DEBUG to avoid a large print out in the case of
any errors.",['065ca8a61d1c071c3ae0354abf25914aff04e789'],False,['nv-ds-chat.yml']
029e0a33d6d41220cdaab0eec6a06044a24425f9,"Use ds-specific module id to avoid conflicts (#6847)

Fix #6772

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['4fea41f4068f601b4d320ebe4bec7321472804b6'],False,"['parameter_offload.py', 'partitioned_param_coordinator.py', 'stage3.py', 'test_zero.py']"
fd405169232dd83bdc7883df1c7d707d482e1be6,"Update GH org references (#6998)

Signed-off-by: Olatunji Ruwase <olruwase@microsoft.com>
Signed-off-by: Logan Adams <loadams@microsoft.com>
Signed-off-by: Fabien Dupont <fdupont@redhat.com>
Co-authored-by: Fabien Dupont <fabiendupont@fabiendupont.fr>",['f4caed6d4f05e8b30e968fb51f46e885cdee796d'],False,"['deepspeed_chat_bug_report.md', 'inference_bug_report.md', 'nv-a6000.yml', 'nv-ds-chat.yml', 'nv-mii.yml', 'CONTRIBUTING.md', 'README.md', 'real_accelerator.py', 'README.md', 'README.md', 'README.md', 'README.md', 'README.md', 'README.md', 'README.md', 'README.md', 'README.md', 'README.md', 'README-Chinese.md', 'README.md', 'README.md', 'README.md', 'README.md', 'README.md', 'README.md', 'README.md', 'README.md', 'README-Chinese.md', 'README-Japanese.md', 'README.md', 'README.md', 'README.md', 'README.md', 'README.md', 'README.md', 'README.md', 'README.md', 'engine.py', 'engine_factory.py', 'meta_tensor.py', 'replace_module.py', 'sharded_moe.py', 'bert_sparse_self_attention.py', 'README.md', 'coalesced_collectives.py', 'stage3.py', 'Dockerfile', 'navigation.yml', 'deepspeed4science.md', 'inference.md', '2020-05-19-bert-record.md', '2020-05-28-fastest-bert-training.md', '2020-09-08-sparse-attention-news.md', '2020-09-09-ZeRO-Offload.md', '2020-09-09-onebit-adam-news.md', '2020-09-09-pipeline-parallelism.md', '2020-09-09-sparse-attention.md', '2020-10-28-progressive-layer-dropping-news.md', '2021-11-15-autotuning.md', '2021-12-09-deepspeed-moe-nlg.md', '2022-07-26-deepspeed-azure.md', '2022-09-10-zero-inference.md', '2022-10-11-mii.md', '2022-12-12-data-efficiency.md', '2023-03-31-multi-modal.md', '2023-04-24-deepspeed-chat-chinese.md', '2023-04-24-deepspeed-chat-japanese.md', '2023-04-24-deepspeed-chat.md', '2023-06-22-zeropp-chinese.md', '2023-06-22-zeropp-japanese.md', '2023-08-24-ulysses-chinese.md', '2023-08-24-ulysses-japanese.md', '2023-08-24-ulysses.md', '2023-09-12-ZeRO-Inference.md', '2023-09-19-deepspeed4science-chinese.md', '2023-09-19-deepspeed4science-japanese.md', '2023-10-04-deepspeed-visualchat-chinese.md', '2023-10-04-deepspeed-visualchat-japanese.md', '2023-10-04-deepspeed-visualchat.md', '2023-11-06-deepspeed-fastgen-chinese.md', '2023-11-06-deepspeed-fastgen-japanese.md', '2023-11-06-deepspeed-fastgen.md', 'accelerator-abstraction-interface.md', 'accelerator-setup-guide.md', 'advanced-install.md', 'automatic-tensor-parallelism.md', 'autotuning.md', 'azure.md', 'bert-finetuning.md', 'bert-pretraining.md', 'cifar-10.md', 'comms-logging.md', 'curriculum-learning.md', 'data-efficiency.md', 'deepnvme.md', 'domino.md', 'ds-sequence.md', 'flops-profiler.md', 'gan.md', 'inference-tutorial.md', 'large-models-w-deepspeed.md', 'megatron.md', 'mixed_precision_zeropp.md', 'mixture-of-experts-inference.md', 'mixture-of-experts-nlg.md', 'mixture-of-experts.md', 'model-compression.md', 'monitor.md', 'onebit-adam.md', 'onebit-lamb.md', 'sparse-attention.md', 'ulysses-offload.md', 'universal-checkpointing.md', 'zero-offload.md', 'zero-one-adam.md', 'zero.md', 'zeropp.md', 'model-checkpointing.rst', 'contributing.md', 'index.md', 'README.md', 'sparse_attn.py', 'setup.py', 'test_zero.py', 'test_zero_context_ancestry.py', 'test_ulysses.py']"
f04649d278e6d4b59b3e60881ed000b24b6e2bae,"autotp training(fix dco) (#7004)

Same as [this PR](https://github.com/deepspeedai/DeepSpeed/pull/6922).
[affeb88](https://github.com/deepspeedai/DeepSpeed/pull/6922/commits/affeb884576936b3a0efc5ce86435da37db1d87e)
I noticed the CI updated the DCO check recently. Using the suggested
rebase method for sign-off would reintroduce many conflicts, so I opted
for a squash merge with sign-off instead. thanks: )

Signed-off-by: inkcherry <mingzhi.liu@intel.com>",['e7fc5986522cae760d005c18d6907e2068da8f4c'],False,"['__init__.py', 'comm.py', 'torch.py', 'engine.py', '__init__.py', 'auto_tp.py', 'layers.py', 'load_checkpoint.py', 'replace_module.py', 'config.py', 'engine.py', '__init__.py', 'config.py', 'tp_manager.py', 'utils.py', 'groups.py', 'test_autotp_training.py']"
22d7fdc0f444571131d77ab13be858b5118770ef,"Fix ds-chat CI regression (#7015)

Fix #7014 
Avoid naming collision on `partition()`

---------

Signed-off-by: Olatunji Ruwase <olruwase@microsoft.com>",['a83ab17d3dd20987dcfc9e04395db8ddcf93df08'],False,"['layers.py', 'hybrid_engine.py', 'test_autotp_training.py']"
a5b63953d2584e06855192dc68c6849589d60aa5,"[Ulysses tutorial] typos (#7024)

Fix typos",['22d7fdc0f444571131d77ab13be858b5118770ef'],False,['ds-sequence.md']
549e11d2cf9164c3b4604db7ca5ed59a5d4039a1,"fix hostname -I for macOS #6497 (#6990)

BUGFIX for Apple Silicon hostname
https://github.com/microsoft/DeepSpeed/issues/6497

---------

Signed-off-by: Fabien Dupont <fdupont@redhat.com>
Signed-off-by: Olatunji Ruwase <olruwase@microsoft.com>
Signed-off-by: Logan Adams <loadams@microsoft.com>
Signed-off-by: inkcherry <mingzhi.liu@intel.com>
Signed-off-by: Roman Fitzjalen <romaactor@gmail.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Fabien Dupont <fabiendupont@fabiendupont.fr>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Liangliang Ma <1906710196@qq.com>
Co-authored-by: inkcherry <mingzhi.liu@intel.com>",['a5b63953d2584e06855192dc68c6849589d60aa5'],False,['comm.py']
079de6bdff85213fe6af42544426310f8ed3ece6,"Update workflows to cuda 12.4 (#7000)

- Update existing workflows that use cu121 to cu124. Note, this means
that where we download torch latest, we will now be getting torch 2.6
rather than the torch latest 2.5 provided with cuda 12.1.
- Note, nv-nightly is failing in master currently due to unrelated
errors, so this could be ignored in this PR (nv-nightly tested locally,
where it passes with 12.1 and it also passes with 12.4).

---------

Signed-off-by: Fabien Dupont <fdupont@redhat.com>
Signed-off-by: Logan Adams <loadams@microsoft.com>
Signed-off-by: Olatunji Ruwase <olruwase@microsoft.com>
Signed-off-by: inkcherry <mingzhi.liu@intel.com>
Signed-off-by: Omar Elayan <oelayan@habana.ai>
Co-authored-by: Fabien Dupont <fabiendupont@fabiendupont.fr>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Liangliang Ma <1906710196@qq.com>
Co-authored-by: inkcherry <mingzhi.liu@intel.com>
Co-authored-by: Omar Elayan <142979319+oelayan7@users.noreply.github.com>",['549e11d2cf9164c3b4604db7ca5ed59a5d4039a1'],False,"['nv-accelerate-v100.yml', 'nv-ds-chat.yml', 'nv-inference.yml', 'nv-lightning-v100.yml', 'nv-mii.yml', 'nv-nightly.yml', 'nv-torch-latest-v100.yml', 'nv-torch-nightly-v100.yml', 'nv-transformers-v100.yml']"
e637677766e0a2063adc61ddd67b58abef74753e,"Add chinese blog for deepspeed windows, and fix format (#7035)

Fix #7029 
- Add Chinese blog for deepspeed windows
- Fix format in README.md

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['83f5deed419090eb9674c557f7ba2a424b53b750'],False,"['README.md', 'README.md', 'README.md']"
ee3f19bf6679884f51edb8a89e3b75ff2193cd56,"Control trace cache warnings (#7039)

Make trace cache warnings configurable, and disabled by default. 

Fix #6985, #4081, #5033, #5006, #5662

---------

Signed-off-by: Olatunji Ruwase <olruwase@microsoft.com>",['14b3cce4aaedac69120d386953e2b4cae8c2cf2c'],False,"['engine.py', 'config.py', 'parameter_offload.py', 'partitioned_param_coordinator.py', 'stage3.py', 'config-json.md']"
d98204b9f303e0938463334a435f871ce03f99e9,"add autoTP training zero2  tests (#7049)

- add zero2 test
- minor fix with transformer version update & ds master merge.

Signed-off-by: inkcherry <mingzhi.liu@intel.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['c9da4892a5ea8025e14e97e8414a372e132037af'],False,"['replace_module.py', 'engine.py', 'utils.py', 'test_autotp_training.py']"
e2dc3eeb1923073e32739596a4fd051417d4ff92,"Fix, bf16 optimizer remove dup loop (#7054)

bf16 with moe refresh optimizer state from bf16 ckpt will raise
IndexError: list index out of range

Signed-off-by: shaomin <wukon1992@gmail.com>
Co-authored-by: shaomin <wukon1992@gmail.com>
Co-authored-by: Hongwei Chen <33092912+hwchen2017@users.noreply.github.com>",['d98204b9f303e0938463334a435f871ce03f99e9'],False,['bf16_optimizer.py']
461d641f00a04e96f2d3c58af55ff6af4b50fc92,"fix an outdated doc wrt CUDA_VISIBLE_DEVICES (#7058)

@jeffra and I fixed this many years ago, so bringing this doc to a
correct state.

---------

Signed-off-by: Stas Bekman <stas@stason.org>",['fa8967e209ee909de556fc7d363eeae1c8ee3b04'],False,['getting-started.md']
8577bd244903f454e319953a6ae1c3838558ac69,"Handle special case of libuv for Windows (#7064)

More information on libuv in pytorch:
https://pytorch.org/tutorials/intermediate/TCPStore_libuv_backend.html
Issue tracking the prevalence of the error on Windows (unresolved at the
time of this PR): https://github.com/pytorch/pytorch/issues/139990
LibUV github: https://github.com/libuv/libuv

Windows error:
```
  File ""C:\hostedtoolcache\windows\Python\3.12.7\x64\Lib\site-packages\torch\distributed\rendezvous.py"", line 189, in _create_c10d_store
    return TCPStore(
           ^^^^^^^^^
RuntimeError: use_libuv was requested but PyTorch was build without libuv support
```

use_libuv isn't well supported on Windows in pytorch <2.4, so we need to
guard around this case.

---------

Signed-off-by: Logan Adams <loadams@microsoft.com>",['cb20d44978f70a177f7f5a07f8881976ed314428'],False,['torch.py']
38327e07f6cd39b63418f60cedd1651b0be3e7c3,"Bug Fix for offload_states API (#7050)

@fukun07 and I discovered a bug when using the `offload_states` and
`reload_states` APIs of the Zero3 optimizer. When using grouped
parameters (for example, in weight decay or grouped lr scenarios), the
order of the parameters mapping in `reload_states`
([here](https://github.com/deepspeedai/DeepSpeed/blob/14b3cce4aaedac69120d386953e2b4cae8c2cf2c/deepspeed/runtime/zero/stage3.py#L2953))
does not correspond with the initialization of `self.lp_param_buffer`
([here](https://github.com/deepspeedai/DeepSpeed/blob/14b3cce4aaedac69120d386953e2b4cae8c2cf2c/deepspeed/runtime/zero/stage3.py#L731)),
which leads to misaligned parameter loading. This issue was overlooked
by the corresponding unit tests
([here](https://github.com/deepspeedai/DeepSpeed/blob/master/tests/unit/runtime/zero/test_offload_states.py)),
so we fixed the bug in our PR and added the corresponding unit tests.

---------

Signed-off-by: Wei Wu <wuwei211x@gmail.com>
Co-authored-by: Masahiro Tanaka <81312776+tohtana@users.noreply.github.com>",['9f20148a77c1e986204d62437ea7541c9e918b08'],False,"['stage3.py', 'test_offload_states.py']"
9d820e4c053160af0136a9cfc3a3a55d4297a023,"Fix TOCTOU issues, switch to fstat (#7067)

Signed-off-by: Logan Adams <loadams@microsoft.com>",['38327e07f6cd39b63418f60cedd1651b0be3e7c3'],False,['deepspeed_aio_common.cpp']
e1903f0d0a666f2e1b901f50c76f252f87a001c5,"config torch to avoid graph breaks caused by logger (#6999)

Following changes in Pytorch trace rules , my previous PR to avoid graph
breaks caused by logger is no longer relevant. So instead I've added
this functionality to torch dynamo -
https://github.com/pytorch/pytorch/commit/16ea0ddcdbe49f5560bfbbc6ab331c3422e27b2e
This commit allows the user to config torch to ignore logger methods and
avoid associated graph breaks.

To enable ignore logger methods -
os.environ[""DISABLE_LOGS_WHILE_COMPILING""] = ""1""
To ignore logger methods except for a specific method / methods (for
example, info and isEnabledFor) -
os.environ[""DISABLE_LOGS_WHILE_COMPILING""] = ""1""
and os.environ[""LOGGER_METHODS_TO_EXCLUDE_FROM_DISABLE""] = ""info,
isEnabledFor""

Signed-off-by: ShellyNR <shelly.nahir@live.biu.ac.il>
Co-authored-by: snahir <snahir@habana.ai>",['9d820e4c053160af0136a9cfc3a3a55d4297a023'],False,['logging.py']
4b7e2c909fb9c6b161ed9e62c647dea49b486e41,"Fix meta load tensor imcompatible issue (#7073)

The partition tensor doesn't need to move to the current device when
meta load is used.

Signed-off-by: Lai, Yejing <yejing.lai@intel.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['e1903f0d0a666f2e1b901f50c76f252f87a001c5'],False,['layers.py']
729dfafca366961b82daddb6dea6294aecfa74bc,"Revert ""Handle special case of libuv for Windows (#7064)"" (#7076)

This reverts commit 8577bd244903f454e319953a6ae1c3838558ac69.

Fixes: #7072",['1d30b58cba83ea03c1e81d20c8c077467983d769'],False,['torch.py']
c07b635c45aa0611b252108c2c20d4f7fb3f8755,"Improve inference tutorial docs (#7083)

Fixes: #7082

---------

Signed-off-by: Logan Adams <loadams@microsoft.com>",['f0401ade2afc160ad5db43d191797b0d903fbe67'],False,['inference-tutorial.md']
f8d34295d0400fef8f50c3cf53651e48b2fe46ae,"Pin transformers version on tests that use latest. (#7085)

Latest transformers causes failures when cpu-torch-latest test, so we
pin it for now to unblock other PRs.

---------

Signed-off-by: Logan Adams <loadams@microsoft.com>",['c07b635c45aa0611b252108c2c20d4f7fb3f8755'],False,"['cpu-torch-latest.yml', 'hpu-gaudi2.yml', 'nv-a6000.yml', 'nv-torch-latest-v100.yml', 'nv-torch-nightly-v100.yml']"
f2ed2531a73fec10061b93bf5edc8b3a056664ec,"Update parallelism for nv-torch-latest/nightly tests due to more GPUs/runner (#7086)

Signed-off-by: Logan Adams <loadams@microsoft.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['5320d4c9bcf5a120400d31a3dd5c39fca39ae826'],False,"['nv-torch-latest-v100.yml', 'nv-torch-nightly-v100.yml']"
b4177e45f9c9e1c49dc2efb88f85ad40e629d04e,"Use new dlpack api; Formatting fixes (#7101)

Fix CI issues by using new dlpack
[api](https://pytorch.org/docs/stable/_modules/torch/utils/dlpack.html#from_dlpack)
Minor pre-commit fixes.

Signed-off-by: Olatunji Ruwase <olruwase@microsoft.com>",['02bbf501090cde1decf7dbd2058203d89a1dee86'],False,"['autotuner.py', 'engine.py', 'runner.py', 'cupy.py', 'engine.py', 'partition_parameters.py', 'logging.py', 'builder.py']"
a88f56a045a41d8b546238c34fb39d47cb3daed2,"Avoid graph breaks by disabling sourceless calls in instrument_w_nvtx (#7081)

This PR is a continuation of the efforts to improve Deepspeed
performance when using PyTorch compile.

The `instrument_w_nvtx` decorator is used to instrument code with NVIDIA
Tools Extension (NVTX) markers for profiling and visualizing code
execution on GPUs.

Along with executing the function itself, `instrument_w_nvtx` makes
calls to `nvtx.range_push` and `nvtx.range_pop` which can't be traced by
Dynamo.

That's why this decorator causes a graph break.
The impact on performance can be significant due to numerous uses of the
decorator throughout the code.

We propose a simple solution: Don't invoke the sourceless functions when
torch is compiling.

---------

Signed-off-by: Max Kovalenko <mkovalenko@habana.ai>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['b4177e45f9c9e1c49dc2efb88f85ad40e629d04e'],False,['nvtx.py']
776822fe2186b199b4cc402ea8d6c84884c8b42c,"Avoid graph breaks in torch.compile caused by inner classes in the backward hooks (#7062)

This PR is part of the effort to improve Deepspeed performance when
using PyTorch compile.

There is a known [bug](https://github.com/pytorch/pytorch/issues/128942)
in torch.compile which causes a graph break when an inner class is
defined within
a method that is being compiled. The following would then appear in the
log:

`[__graph_breaks] torch._dynamo.exc.Unsupported: missing:
LOAD_BUILD_CLASS`

This is the case with the inner classes `PreBackwardFunctionForModule`
and `PostBackwardFunctionModule`.

While there is an open PyTorch [PR#133805
](https://github.com/pytorch/pytorch/pull/133805) for this, we can solve
the issue by moving the inner classes into the initialization code.

No graph breaks and the corresponding logs are produced anymore.

---------

Signed-off-by: Max Kovalenko <mkovalenko@habana.ai>
Signed-off-by: Olatunji Ruwase <olruwase@microsoft.com>
Signed-off-by: inkcherry <mingzhi.liu@intel.com>
Signed-off-by: shaomin <wukon1992@gmail.com>
Signed-off-by: Stas Bekman <stas@stason.org>
Signed-off-by: siqi <siqi@tecorigin.com>
Signed-off-by: Logan Adams <loadams@microsoft.com>
Signed-off-by: Wei Wu <wuwei211x@gmail.com>
Signed-off-by: ShellyNR <shelly.nahir@live.biu.ac.il>
Signed-off-by: Lai, Yejing <yejing.lai@intel.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: inkcherry <mingzhi.liu@intel.com>
Co-authored-by: wukong1992 <wukong1992@users.noreply.github.com>
Co-authored-by: shaomin <wukon1992@gmail.com>
Co-authored-by: Hongwei Chen <33092912+hwchen2017@users.noreply.github.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: loadams <loadams@users.noreply.github.com>
Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>
Co-authored-by: siqi654321 <siqi202311@163.com>
Co-authored-by: siqi <siqi@tecorigin.com>
Co-authored-by: Wei Wu <45323446+U-rara@users.noreply.github.com>
Co-authored-by: Masahiro Tanaka <81312776+tohtana@users.noreply.github.com>
Co-authored-by: Shelly Nahir <73890534+ShellyNR@users.noreply.github.com>
Co-authored-by: snahir <snahir@habana.ai>
Co-authored-by: Yejing-Lai <yejing.lai@intel.com>",['a88f56a045a41d8b546238c34fb39d47cb3daed2'],False,['parameter_offload.py']
17c6595442527945e39a2e8a9505982d544eb96d,"Avoid graph break due to unsupported frozenset (#7105)

This PR is a continuation of the efforts to improve Deepspeed
performance when using PyTorch compile.

The `fetch_sub_module()` routine makes use of the `frozenset` which is
problematic because:

1. `iter_params` returns an iterable over model parameters
2. `frozenset` wraps this iterable, making it unmodifiable
3. PyTorch’s compilation process cannot infer how `frozenset` interacts
with tensors, leading to a graph break.

If we replace the `frozenset` with a modifiable `set`, then there is no
longer such graph break.

Signed-off-by: Max Kovalenko <mkovalenko@habana.ai>
Co-authored-by: Masahiro Tanaka <81312776+tohtana@users.noreply.github.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['e4c7931a4e3657baea4aed8f76565da1d0e9392b'],False,['partitioned_param_coordinator.py']
71807bceba29736a7e385e958087dc24ebf97b14,"Fix fused_qkv print model ValueError (#7109)

Suppose qkv_linear_weight_shape = [in_features, out_features].
The qkv linear weight shape is [3, in_features, out_features] if using
fued_qkv gemm optimization. It will cause ""ValueError: too many values
to unpack (expected 2)"" issue when printing the model.

Solution: Take the last two weight dimensions shapes as in_features and
out_features.

Signed-off-by: Lai, Yejing <yejing.lai@intel.com>
Co-authored-by: Hongwei Chen <33092912+hwchen2017@users.noreply.github.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['17c6595442527945e39a2e8a9505982d544eb96d'],False,['layers.py']
8ec1af5f5ca40e503f51fec17168aa305b3c2e3d,"fix keep_module_on_host (#7112)

Reapply https://github.com/deepspeedai/DeepSpeed/pull/6846.
FYI @oelayan7

---------

Signed-off-by: inkcherry <mingzhi.liu@intel.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['c1acd49cdf99278b2d600c60ca58653146d5f53b'],False,"['auto_tp.py', 'layers.py']"
436c126211b0d2bf0af02073f1a9fd7bb9646d2d,"Add sequential pytest mark to TestNVMeCheckpointing to resolve pytest forked hangs (#7131)

Signed-off-by: Logan Adams <loadams@microsoft.com>",['8ec1af5f5ca40e503f51fec17168aa305b3c2e3d'],False,['test_nvme_checkpointing.py']
b418cf6c1b1a9961717c577186ca3f7ff01c89a0,"Training multiple models (#7018)

Support training multiple models, such as in
[HF](https://huggingface.co/docs/accelerate/en/usage_guides/deepspeed_multiple_model)

Here is some update on supporting multiple DS engines with single
loss.backward(). The main message is that I think we can support this.
First, some context. Backward pass in ZeRO is complicated because the
optimizations/features require special handling of gradients, such as:

1. Gradient partitioning
2. Overlapping backward and reduction
3. Upcasting for fp32 grad accumulation

So, we created engine.backward(loss) as a wrapper function to provide us
fine-grained control over backward as below

```python
def backward(loss):
 backward_prologue() # setup logic for special gradient handling
 loss.backward()
 backward_epilogue() # cleanup/teardown logic
```

As demonstrated by @muellerzr, this approach breaks down when loss
originates from multiple DS engines. Our proposed solution is to use
backward hooks on the module to launch backward_prologue() and
backward_epilogue() . Specifically,

1. backward pre hook on engine.module to launch backward_prologue()
before any module gradient is created.
2. backward post hook on engine.module to launch backward_epilogue()
after all module gradients are created.

We plan for this solution to preserve BC, i.e., engine.backward() will
remain correct for single engine scenarios.
The current status is that (1) is completed, while (2) is in progress.
To unblock e2e testing for multi-engine scenarios, since there are
probably other issues, we have a temporarily added
engine._backward_prologue() . You can try this out via the following
artifacts.

1. Simple multi-engine test code:
https://gist.github.com/tjruwase/f1adccf087b8fa269ffce2ab91c4f1c6#file-multi_engine-py
2. DS branch:
https://github.com/microsoft/DeepSpeed/tree/olruwase/zero_multi_models

---------

Signed-off-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>",['436c126211b0d2bf0af02073f1a9fd7bb9646d2d'],False,"['bf16_optimizer.py', 'constants.py', 'engine.py', 'stage_1_and_2.py', 'training.rst', 'test_data_efficiency.py', 'test_multiple_models.py']"
7d9dbf2830d0d38733e0aab118a0c9a1e024ae90,"Update CONTRIBUTING.md to reflect changes from CLA to DCO (#7135)

Copy changes from https://github.com/deepspeedai/DeepSpeed-MII/pull/558.
Fixes issue where docs still referenced CLA.

---------

Signed-off-by: Logan Adams <loadams@microsoft.com>",['b418cf6c1b1a9961717c577186ca3f7ff01c89a0'],False,['CONTRIBUTING.md']
9288bc4f824145524680f4ad0bdbb860d33fbb50,"Avoid missing attr error (#7133)

Fix #7132

Signed-off-by: Olatunji Ruwase <olruwase@microsoft.com>",['7d9dbf2830d0d38733e0aab118a0c9a1e024ae90'],False,['engine.py']
d095b181859dac9867305164c588c9ade098e251,"Unpin transformers version for most workflows (#7139)

Unpin transformers version for all workflows except
`nv-torch-latest-v100` as this still has a tolerance issue with some
quantization tests.

Signed-off-by: Logan Adams <loadams@microsoft.com>",['39027c300885effb414d22763624f1e68feb1c16'],False,"['cpu-torch-latest.yml', 'hpu-gaudi2.yml', 'nv-a6000.yml', 'nv-torch-nightly-v100.yml']"
591d4d4d547498991731aa0e7fa33284822a67eb,"Conditionally quote env vars (#7071)

Resolves #6997 

This PR conditionally quotes environment variable values—only wrapping
those containing special characters (like parentheses) that could
trigger bash errors. Safe values remain unquoted.

---------

Signed-off-by: Saurabh <saurabhkoshatwar1996@gmail.com>
Signed-off-by: Saurabh Koshatwar <saurabhkoshatwar1996@gmail.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['d095b181859dac9867305164c588c9ade098e251'],False,"['multinode_runner.py', 'test_user_args.py']"
2e7f8e580e7088f05008e5211ebf724b480614ad,"fix leak of z3 buffer

Signed-off-by: Masahiro Tanaka <mtanaka@microsoft.com>",['3388f8331b7bacff979624e6c21fa8c97fa1335b'],False,"['stage3.py', 'test_fp16.py', 'test_multiple_models.py']"
d40cf4662c8dd29a51ca60fbaaaad73bed3e9251,"Avoid graph break by removing redundant requires_grad attr change (#7158)

This PR is a continuation of the efforts to improve DeepSpeed
performance when using PyTorch compile.

Dynamo breaks the graph because `flat_tensor.requires_grad = False`:

* Is a side-effecting operation on tensor metadata
* Occurs in a context where Dynamo expects static tensor properties for
tracing

`flat_tensor.requires_grad` is redundant and can be safely removed
because:
* `_allgather_params()` function is already decorated with
`@torch.no_grad()` which ensures the desired property
* `flat_tensor` is created using the `torch.empty()` which sets the
`requires_grad=False` by default.

---------

Signed-off-by: Max Kovalenko <mkovalenko@habana.ai>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Hongwei Chen <33092912+hwchen2017@users.noreply.github.com>",['1ca83a6bb9f3fffdb98c94093ab48605294241ae'],False,['partition_parameters.py']
9ae010e629576be05399c4b627ff1cd02a7b4e2e,"Add destroy to tests to free memory (#7160)

ZeRO3 requires explicit cleaning in tests when reusing the environment.
This PR adds `destroy` calls to the tests to free memory and avoid
potential errors due to memory leaks.

Signed-off-by: Masahiro Tanaka <mtanaka@microsoft.com>",['d40cf4662c8dd29a51ca60fbaaaad73bed3e9251'],False,['test_zero.py']
2b245a999e8bf7c5b7e7785e8493553b9f6d69ea,"[NFC] Typo fix in SP layer. (#7152)

Signed-off-by: c8ef <c8ef@outlook.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['9ae010e629576be05399c4b627ff1cd02a7b4e2e'],False,['layer.py']
2dbb5d9416177422f0cbbd0890d43cd43b2c353f,"fix `seq_parallel_communication_data_type` constant. (#7175)

A user couldn't override `seq_parallel_communication_data_type` because
of a typo in a name, this PR fixes it.",['e7e20e4ace455699318d82a43bd8c899be17eb5b'],False,['constants.py']
ac295aa06cb9ca5ee7976dcfb55eb014a82cf1f3,"Fix typos in GDS blog (#7177)

Signed-off-by: Logan Adams <loadams@microsoft.com>",['2dbb5d9416177422f0cbbd0890d43cd43b2c353f'],False,['README.md']
20f988eade5217ab0045ba1681030f3d255d67e3,"Variable batch size and LR scheduler (#7104)

# Background and rationale

In many use cases, particularly LLMs, one is faced with inputs
(sentences) of variable lengths. A common practice is to pack batches by
token count (not a fixed batch size), ie by putting together sentences
whose given metric (eg sequence lengths) will add up to an user-provided
value. As an example, in [Attention is all you
need](https://arxiv.org/abs/1706.03762), section 5.1:

> Sentence pairs were batched together by approximate sequence length.
Each training
batch contained a set of sentence pairs containing approximately 25000
source tokens and 25000
target tokens.

Dynamic batch sizes has been requested in [DeepSpeed issue
1051](https://github.com/microsoft/DeepSpeed/issues/1051), [DeepSpeed
issue 3455 ](https://github.com/microsoft/DeepSpeed/issues/3455),
[Pytorch Lightning issue
16914](https://github.com/Lightning-AI/pytorch-lightning/issues/16914),
[huggingface issue
2647](https://github.com/huggingface/accelerate/issues/2647) and is
available already in many libraries e.g. [NVIDIA
Triton](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_configuration.md#dynamic-batcher)
and [Meta FairSeq](https://github.com/facebookresearch/fairseq)
(implementation
[here](https://github.com/facebookresearch/fairseq/blob/34973a94d09ecc12092a5ecc8afece5e536b7692/fairseq/data/fairseq_dataset.py#L104)
).

The immediate use case for this is when one needs to maximize GPU
utilization. Moreover, this is particularly relevant for curriculum
learning where a `BxTxE` (Batch x Time x Embedding) -shaped input should
ideally have high `B` and low `T` at the early curriculum steps (many
short sentences packed together as a batch), and low `B` and high `T` at
the late steps (few long sentences in the batch). A dynamic size `T` is
already supported by Deepspeed, e.g. in the documentation for pipeline
parallelism's
[reset_activation_shape()](https://deepspeed.readthedocs.io/en/stable/pipeline.html#deepspeed.runtime.pipe.engine.PipelineEngine.reset_activation_shape):
> For curriculum learning that changes the seqlen of each sample, we
need to call this whenever the seqlen is going to change.

However, dynamic `B` is not supported. A dynamic `B` would require an
adequate increase/decrease of learning rate. This technique has been
applied previously, and the two most common LR scaling algorithms have
been described as:
1. Linear Scaling Rule: ""When the minibatch size is multiplied by k,
multiply the learning rate by k"", as in [Accurate, Large Minibatch SGD:
Training ImageNet in 1 Hour, Goyal et
al.](https://arxiv.org/abs/1706.02677)
2. Square Root scaling: ""when multiplying the batch size by k, multiply
the learning rate by √k, to keep the variance in the gradient
expectation constant"" by [One weird trick for parallelizing
convolutional neural networks, A. Krizhevsky et
al.](https://arxiv.org/abs/1404.5997)

In practice, the user picks the total token count per batch as the
metric that drives batching, instead of batching by sentence count.
During runtime, the variable batch size is computed and the LR is
adjusted respectively, based on the LR and batch size provided by the
config.

# Illustration of dynamic batch size, sequence length and LR

Imagine we picked a limit of `30` tokens per batch, and have set a
reference `lr=1e-3` for a `train_batch_size=2` (in the deepspeed
config). The batching algorithm for curriculum may pack the data into
batches of short sentences (left) at the early stages, and batches of
long sentences (right) as later stages, e.g.:


![dynamic_batch_size_and_lr](https://github.com/microsoft/DeepSpeed/assets/150697676/324bda09-8f0b-430c-bb33-cc1bd01c3fe7)

Above, we collected samples until we filled up the batch with at most 30
tokens. The batch sizes (number of samples) became then `10` and `4` on
the left and right examples, respectively. Using the linear scaling
rule, the LR for those batches become `5e-3` and `2e-3`.

# Pipeline parallelism

Pipeline parallelism requires the same batch size and same sequence
length across all micro-batches in a batch, as the activation sizes must
be fixed between gradient accumulation steps. Between batches, these may
change, and long as `engine.reset_activation_shape()` is called so that
the new shapes are communicated on the first gradient accumulation step
in the batch. Enforcing similar `BxTxE` between batches may lead to
smaller micro-batches. As an example, below we can see an illustration
of a 2-node 2-gradient-accumulation-step (ie 4 micro-batches) batching
for the same dataset, when preparing data for the regular DDP (left) and
for the pipeline parallelism use cases (right):


![dynamic_batch_size_and_lr_microbatching](https://github.com/microsoft/DeepSpeed/assets/150697676/3fed5e1c-f2f5-4efe-a9c5-5b5e20719d45)

We can see that the pipeline use case (right) has the same `BxTxE` shape
across all the 4 micro-batches in the same batch, and in order to
respect that, it packs less samples in the batch, when compared to the
standard use case (left hand size)

# Attention Head

For an input of size `BxTxE` the attention has a shape of `TxT` for a
mask of fixed size across samples of same size, or `BxTxT` for a
different mask per sample (when samples have different sizes, as in the
dataset above). This 3D attention matrix can be illustrated for the DDP
microbatch 1 (picture above top-left, 4 sentences) as:
 

![dynamic_batch_size_and_lr_attn_matrix](https://github.com/microsoft/DeepSpeed/assets/150697676/707d2f17-66da-4034-8a12-a87df2044bfb)

Note the memory savings: the attention head has a size of `BxTxT`, i.e.
a linear memory dependency on the batch size `B` and quadratic memory
dependency on the largest sequence length `T` in the (micro-) batch.
Thus, supporting a dynamic size `T` allows for an increase of `B`.

# PR overview

This PRs implements dynamic batching and LR scaling. The dataloader and
LR scheduler necessary can be retrieved by calling
`get_dataloader_and_lr_scheduler_for_variable_batch_size`. A small
explanation of that function follows:
- The logic behind the algorithms for LR scaling is in `scale_lr`;
- The partitioning of samples into batches is done by `batch_by_seqlen`.
- For pipeline parallelism, it is required that all micro-batches in a
pipeline pass to have the same activation shapes. This is enabled by
setting to `True` the following parameters:
- `required_microbatches_of_same_sizes` that will force the `B`
dimension to be the same across all gradient accumulation steps of all
dataloaders on a batch;
- `required_microbatches_of_same_lengths` that will force the `T`
dimension to be the same across all gradient accumulation steps. Works
by calling the user-provided `sample_padding_fn(sentence, len)` that
pads a given sentence to the argument length;
- `batch_by_seqlen` returns `microbatch_sample_ids` (the list of sample
ids per micro-batch), `batch_sizes` (the size of effective batch sizes,
and `batch_max_seqlens` (longest sequence across all microbatches in a
batch)
- `dataloader_for_variable_batch_size` relies on `microbatch_sample_ids`
and will iterate/collate/pad samples for every batch and return a
dataloader that iterates the final (variable-size) batches;
- `lr_scheduler_for_variable_batch_size` relies on `batch_sizes` to
compute the learning rate for each effective batch, taking into account
the batch size and LR in the config file, and scaling the LR based on
the size of each effective batch, and the scaling rule mentioned above
(Linear, Square root, etc).
- Special note to the `lr_scheduler` returned that will either accept
either:
1. an user-provided `Optimizer` that will scale the learning rates (in
param groups) at every batch, or
2. an user-defined `LRScheduler`, that in this case will first get the
learning rate from the scheduler and then scale it accordingly.

# Example

An example for the use case with and without pipelining is provided in
file
[`DeepSpeedExamples/training/data_efficiency/variable_batch_size_and_lr/variable_batch_size_and_lr_example.py`](https://github.com/deepspeedai/DeepSpeedExamples/tree/master/training/data_efficiency/variable_batch_size_and_lr).
The example shows an attention head with attention of variable-sized
`BxTxT` per batch, followed by a fixed size feed forward network. These
are the main blocks on a Large Language Model. The feed-forward (or
linear layer) that follows the attention head requires a constant input
size, equivalent to the largest sentence in the whole dataset, so the
output of the attention must be padded (see `feedforward: needs to
convert BxTxE to BxMxE by padding extra tokens` in the code).


# Config

The example file also comments the relevant deepspeed config with
comments:

```python
config = {
  ""train_batch_size"": 16,
  # `train_micro_batch_size_per_gpu` tells how many sequence packs of `max_tokens` each will be collated together.
  #  I.e. the number of tokens per micro batch (ie per gpu iteration) is `train_micro_batch_size_per_gpu`*`max_tokens`.
  ""train_micro_batch_size_per_gpu"": 2,
  ""data_efficiency"": {
    ""enabled"": True,
    # seed to be applied to all data efficiency modules, including dynamic batching
    ""seed"": 42,
    ""data_sampling"": {
      ""num_workers"": 0, # dataloader num_workers argument
      ""pin_memory"": False,  # dataloader pin_memory argument
      ""dynamic_batching"": {
        # enables or disables dynamic batching
        ""enabled"": True,
        # how many tokens we need to fill a pack of sequences (that will be collated together as a sample)
        ""max_tokens"": 100,
        # Input and output write to read from or write the length of every sequence.
        # Sequence lengths will be loaded from: {metrics_path}/seqlen/seqlen_sample_to_metric.bin and *.idx
        # If files dont exist, they'll be computed and saved on the first run, and loaded on subsequent runs.
        ""metrics_path"": ""./curriculum_output/"",
        # As batch size increases/decreses, which method to use to scale LR accordingly?
        # Options: linear, sqrt (square root), or None to disable
        ""lr_scaling_method"": ""linear"",
        # how to pick sentences to be packed into samples:
        # - dataloader: by same order as they come in with the dataloader
        # - seqlen: by sequence length (shortest to longest)
        # - random: random order using the seed in config['data_efficiency']['seed'
        ""sentence_picking_order"": ""dataloader"",  # ""random"" / ""seqlen"" / ""dataloader""
        # minimum number of sequences required to reach `max_tokens`. If sentence pack is smaller, it's discarded.
        ""min_batch_size"": 1,
        # maximum number of sequences required to reach `max_tokens`. If sentence pack is larger, it's discarded.
        ""max_batch_size"": 10,
        # enable the output of microbatching information about sentence packing
        ""verbose"": True,
      },
    },
  },
}
```

# Future work

A follow-up PR will enable dynamic batching when calling
`deepspeed.initialize`. I.e. instead of this:

```python
engine, _, _, _ = deepspeed.initialize(config=config, model=model)
dataloader, lr_scheduler, _ = get_dataloader_and_lr_scheduler_for_variable_batch_size_deepspeed(...)
engine.lr_scheduler = lr_scheduler
```

we'd ideally have this:

```python
engine, _, dataloader, lr_scheduler = deepspeed.initialize(config=config, model=model)
```

where `initialize` will call internally
`get_dataloader_and_lr_scheduler_for_variable_batch_size_deepspeed`.

---------

Signed-off-by: Bruno Magalhaes <bruno.magalhaes@synthesia.io>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['ac295aa06cb9ca5ee7976dcfb55eb014a82cf1f3'],False,"['runner.py', 'config.py', 'config.py', 'constants.py', 'data_analyzer.py', 'variable_batch_size_and_lr.py', 'engine.py']"
b8cc1eb078095347b46ef83c441203803bb52418,"async tp allreduce  (#7115)

Signed-off-by: inkcherry <mingzhi.liu@intel.com>
Signed-off-by: Logan Adams <loadams@microsoft.com>
Signed-off-by: Olatunji Ruwase <olruwase@microsoft.com>
Signed-off-by: Shaik Raza Sikander <srsikander@habana.ai>
Signed-off-by: Max Kovalenko <mkovalenko@habana.ai>
Signed-off-by: shaomin <wukon1992@gmail.com>
Signed-off-by: Stas Bekman <stas@stason.org>
Signed-off-by: siqi <siqi@tecorigin.com>
Signed-off-by: Wei Wu <wuwei211x@gmail.com>
Signed-off-by: ShellyNR <shelly.nahir@live.biu.ac.il>
Signed-off-by: Lai, Yejing <yejing.lai@intel.com>
Signed-off-by: Hongwei <hongweichen@microsoft.com>
Signed-off-by: Liang Cheng <astarxp777@gmail.com>
Signed-off-by: A-transformer <astarxp777@gmail.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: A-transformer <cl5743590921@gmail.com>
Co-authored-by: Raza Sikander <srsikander@habana.ai>
Co-authored-by: Max Kovalenko <mkovalenko@habana.ai>
Co-authored-by: wukong1992 <wukong1992@users.noreply.github.com>
Co-authored-by: shaomin <wukon1992@gmail.com>
Co-authored-by: Hongwei Chen <33092912+hwchen2017@users.noreply.github.com>
Co-authored-by: loadams <loadams@users.noreply.github.com>
Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>
Co-authored-by: siqi654321 <siqi202311@163.com>
Co-authored-by: siqi <siqi@tecorigin.com>
Co-authored-by: Wei Wu <45323446+U-rara@users.noreply.github.com>
Co-authored-by: Masahiro Tanaka <81312776+tohtana@users.noreply.github.com>
Co-authored-by: Shelly Nahir <73890534+ShellyNR@users.noreply.github.com>
Co-authored-by: snahir <snahir@habana.ai>
Co-authored-by: Yejing-Lai <yejing.lai@intel.com>
Co-authored-by: A-transformer <astarxp777@gmail.com>
Co-authored-by: Ma, Guokai <guokai.ma@gmail.com>",['f355b9eadff51ec954e3d1921651bf21217b6061'],False,"['__init__.py', 'layers.py', 'engine.py', 'config.py', 'test_autotp_training.py']"
1f706621f15b5df83f3bcff1e21dd0bbcfc1614b,"Fix issue #5242 grad_norm and loss is nan (#7171)

This PR addresses a regression introduced in commit
[61daaa1](https://github.com/deepspeedai/DeepSpeed/commit/61daaa1ea211e3a114250a7cf7117cd0c8fecd5e)
that affects gradient clipping when handling infinite values.

The modified NaN/Inf handling logic in total_norm calculation leads to
unexpected behavior:

Original logic
([v0.10.3](https://github.com/deepspeedai/DeepSpeed/blob/v0.10.3/deepspeed/runtime/zero/stage_1_and_2.py#L1233)):
Converted both NaN and Inf to -1 before entering unscale_and_clip_grads
Post-commit behavior: When total_norm is Inf, inf_or_nan.logical_not() *
total_norm produces NaN instead of 0, causing gradient clipping to fail

Here is a minimal reproducible example comparing gradient clipping
behavior across implementations.
```python
import torch
import numpy as np
import copy

def test(total_norm):
    test_old_deepspeed(total_norm)
    test_deepspeed(total_norm)
    test_torch(total_norm)
    test_deepspeed_fix(total_norm)

def test_old_deepspeed(total_norm_tensor):
    total_norm = copy.deepcopy(total_norm_tensor)
    # https://github.com/deepspeedai/DeepSpeed/blob/v0.10.3/deepspeed/runtime/zero/stage_1_and_2.py#L1233
    if total_norm == float('inf') or total_norm == -float('inf') or total_norm != total_norm:
        total_norm = torch.tensor(float(-1))
        
    # https://github.com/deepspeedai/DeepSpeed/blob/v0.10.3/deepspeed/runtime/zero/stage_1_and_2.py#L1848
    clip_grad = float(1.0)
    loss_scale = float(1.0)
    combined_scale = loss_scale
    clip = ((total_norm / loss_scale) + 1e-6) / clip_grad
    if clip > 1:
        combined_scale = clip * loss_scale
    print(f""old_deepspeed: {1. / combined_scale}"")

def test_deepspeed(total_norm_tensor):
    total_norm = copy.deepcopy(total_norm_tensor)
    # https://github.com/deepspeedai/DeepSpeed/blob/v0.16.4/deepspeed/runtime/zero/stage_1_and_2.py#L1710
    norm_is_inf = total_norm.isinf()
    norm_is_nan = total_norm.isnan()
    inf_or_nan = norm_is_nan.logical_or(norm_is_inf)

    err = torch.tensor(-1.0, dtype=torch.float)
    total_norm = inf_or_nan * err + inf_or_nan.logical_not() * total_norm

    # https://github.com/deepspeedai/DeepSpeed/blob/v0.16.4/deepspeed/runtime/zero/stage_1_and_2.py#L1970
    clip_grad = float(1.0)
    loss_scale = float(1.0)
    clip = ((total_norm / loss_scale) + 1e-6) / clip_grad
    clip = torch.clamp(clip, min=1.0)
    combined_scale = clip * loss_scale
    print(f""test_deepspeed: {1. / combined_scale}"")
    
def test_torch(total_norm_tensor):
    # https://github.com/pytorch/pytorch/blob/v2.6.0/torch/nn/utils/clip_grad.py#L155
    total_norm = copy.deepcopy(total_norm_tensor)
    max_norm = float(1.0)
    clip_coef = max_norm / (total_norm + 1e-6)
    clip_coef_clamped = torch.clamp(clip_coef, max=1.0)
    print(f""torch: {clip_coef_clamped}"")

def test_deepspeed_fix(total_norm_tensor):
    total_norm = copy.deepcopy(total_norm_tensor)
    if total_norm.isinf() or total_norm.isnan():
        total_norm = torch.tensor(-1.0, dtype=torch.float)

    # https://github.com/deepspeedai/DeepSpeed/blob/v0.16.4/deepspeed/runtime/zero/stage_1_and_2.py#L1970
    clip_grad = float(1.0)
    loss_scale = float(1.0)
    clip = ((total_norm / loss_scale) + 1e-6) / clip_grad
    clip = torch.clamp(clip, min=1.0)
    combined_scale = clip * loss_scale
    print(f""test_deepspeed_fix: {1. / combined_scale}"")
    
if __name__ == '__main__':
    print(""*****NAN*****"")
    test(torch.tensor(float('nan')))
    print(""*****INF*****"")
    test(torch.tensor(float('inf')))
    print(""*****positive*****"")
    test(torch.tensor(float(2.0)))

```
Result:

![20250325165135](https://github.com/user-attachments/assets/bd32209d-14f6-4c21-8b57-f8bd94786fe2)

---------

Signed-off-by: yueyang.hyy <yueyang.hyy@alibaba-inc.com>
Co-authored-by: Hongwei Chen <33092912+hwchen2017@users.noreply.github.com>",['b8cc1eb078095347b46ef83c441203803bb52418'],False,['stage_1_and_2.py']
3c1817f38fc46aba3a2d5b0b10e56c33aca11a22,"Reland perf fix for nan inf check (#7184)

replace previous usage with logical ops for nan/inf detect with
torch.where

---------

Signed-off-by: Nadav Elyahu <nelyahu@habana.ai>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Hongwei Chen <33092912+hwchen2017@users.noreply.github.com>",['79ff16272274e9f71dc631716cf20224190b5d11'],False,"['utils.py', 'stage3.py', 'stage_1_and_2.py']"
027ee21af9a24b9b4de6a4358dec2bc6d0967e0d,"Update to fix pydantic warning (#7193)

Warning:

```
site-packages/deepspeed/runtime/config_utils.py:64: PydanticDeprecatedSince211: Accessing this attribute on the instance is deprecated, and will be removed in Pydantic V3. Instead, you should access this attribute from the model class. Deprecated in Pydantic V2.11 to be removed in V3.0.
    kwargs = pydantic_config.model_fields[dep_field].json_schema_extra
```

Signed-off-by: Logan Adams <loadams@microsoft.com>",['3c1817f38fc46aba3a2d5b0b10e56c33aca11a22'],False,['config_utils.py']
56005d2b256eb81a88cba0a1984375f9663a3110,HPU accelerator memory mapping is broken because of torch fill uninit memory (#7209),['29fa95a819be85f18d485c0d4fb10a9ef3c26407'],False,['hpu_accelerator.py']
185330cdff318247b134503d1690cffb39a450dc,"Support complicated use cases with TiedLayerSpec (#7208)

I want to reuse a composed module in the pipeline. For example, the
following `MyModule` has a member `linear`, which is also a module.

```python
class MyModule(torch.nn.Module):
    def __init__(self, n_in: int, n_out: int):
        super().__init__()
        self.linear = torch.nn.Linear(n_in, n_out)
        self.layer_norm = torch.nn.LayerNorm(n_out)

    def forward(self, data: torch.Tensor) -> torch.Tensor:
        hidden = self.linear(data)
        hidden = self.layer_norm(hidden)
        return hidden
```

`MyModule.linear.weight` should be synchronized among related ranks. As
a result, I add `linear.weight` to `TiedLayerSpec.tied_weight_attr`.
BTW, I generate the whole `tied_weight_attr` by the following
instruction.

```python
tied_weight_attr = [name for name, p in layer.named_parameters() if p.numel() > 1]
```

However, the builtin `getattr` used by `PipelineModule` fails to find a
nested attribute like `linear.weight`.
Hence, this PR first extends the builtin `getattr` to a recursive
version `PipelineModule._recursive_getattr`, accessing each attribute
segment one by one.

Meanwhile, the order of tied weights matters in synchronization. This PR
suggests to sort tie_keys in `PipelineModule._index_tied_modules` to
avoid hanging.

Signed-off-by: Mingjie Li <limingjie@chinamobile.com>
Co-authored-by: Mingjie Li <limingjie@chinamobile.com>
Co-authored-by: Masahiro Tanaka <81312776+tohtana@users.noreply.github.com>",['56005d2b256eb81a88cba0a1984375f9663a3110'],False,['module.py']
227a60c0c412ddf4619401b5d8d9d1674aee17b5,"DeepCompile for enhanced compiler integration (#7154)

This PR introduces *DeepCompile*, a new feature that efficiently
integrates compiler optimizations with other DeepSpeed features.
DeepCompile utilizes torch's dynamo to capture the computation graph and
modifies it to incorporate DeepSpeed’s optimizations seamlessly.

Currently, DeepCompile supports ZeRO-1 and ZeRO-3, with enhancements
such as proactive prefetching and selective unsharding to improve
performance.
(More details will be added later.)

---------

Signed-off-by: Masahiro Tanaka <mtanaka@microsoft.com>
Signed-off-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: zafarsadiq <zafarsadiq120@gmail.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>",['a21e5b9db68adf25e9fc797d0e67fdb5879f6069'],False,"['nv-pre-compile-ops.yml', 'nv-torch-latest-v100.yml', 'README.md', 'README.md', 'opt_loop.png', 'perf_offload.png', 'perf_summary.png', 'perf_zero1.png', 'perf_zero3.png', 'workflow.png', 'build_win.bat', 'deepcompile.cpp', 'init.cpp', 'util.cpp', 'z1.cpp', 'z1.h', 'z3.cpp', 'z3.h', 'deepcompile.h', 'comm.py', 'torch.py', '__init__.py', 'backend.py', 'config.py', 'fx.py', 'graph_param.py', 'inductor.py', 'init_z1.py', 'init_z3.py', 'list_schedule.py', 'partitioner.py', '__init__.py', 'offload_activation.py', 'offload_adam_states.py', 'offload_parameters.py', 'prefetch.py', 'selective_gather.py', 'zero1_compile.py', 'zero3_compile.py', 'patch_compiled_func.py', 'patch_fake_tensor.py', '__init__.py', 'comm_profile.py', 'graph_profile.py', 'util.py', '__init__.py', 'config.py', 'engine.py', 'parameter_offload.py', 'partition_parameters.py', 'stage3.py', 'dc.py', 'requirements-deepcompile.txt', 'setup.py', 'test_compile_zero.py', 'util.py']"
8f93f8b9b003ffec4a7c531a7997b0cb945e91bb,"Fix release links (#7219)

Fix DS release links

---------

Signed-off-by: Masahiro Tanaka <mtanaka@microsoft.com>
Signed-off-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Masahiro Tanaka <mtanaka@microsoft.com>
Co-authored-by: Masahiro Tanaka <81312776+tohtana@users.noreply.github.com>
Co-authored-by: zafarsadiq <zafarsadiq120@gmail.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['a01a2688b98aa90ac1dcf23012efbe13ba69ebcd'],False,"['README.md', 'README.md', 'index.md']"
1d022e05c3089a04614ebf822e970110fe9321b0,"Fix pass for z3 and profiler (#7222)

This PR adds a missing line for scheduling in Z3 pass and fixes
attribute names in the profiler.

Signed-off-by: Masahiro Tanaka <mtanaka@microsoft.com>",['8f93f8b9b003ffec4a7c531a7997b0cb945e91bb'],False,"['zero3_compile.py', 'graph_profile.py']"
0f224d79c63acdb2339660fc977ae1ede8d02dc3,"Fix build on AMD GPUs (related to DeepCompile) (#7224)

We should use `torch.utils.cpp_extension.ROCM_HOME` for ROCm pytorch.

```log
  Traceback (most recent call last):
    File ""<string>"", line 2, in <module>
    File ""<pip-setuptools-caller>"", line 34, in <module>
    File ""DeepSpeed/setup.py"", line 195, in <module>
      builder.hipify_extension()
    File ""DeepSpeed/op_builder/builder.py"", line 750, in hipify_extension
      header_include_dirs=self.include_paths(),
                          ^^^^^^^^^^^^^^^^^^^^
    File ""DeepSpeed/op_builder/dc.py"", line 32, in include_paths
      return ['csrc/includes', os.path.join(torch.utils.cpp_extension.CUDA_HOME, ""include"")]
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File ""<frozen posixpath>"", line 76, in join
  TypeError: expected str, bytes or os.PathLike object, not NoneType
```

Signed-off-by: Hollow Man <hollowman@opensuse.org>",['1d022e05c3089a04614ebf822e970110fe9321b0'],False,['dc.py']
c66fdaf3c9b8ab2d9d7f988b59f522dfa33e73dd,"Make sure it's not None before offloading contiguous_grad_buffer (#7227)

Resolves #7223

When DeepCompile is enabled in ZeRO-3, contiguous_grad_buffer is
released, so we should check and make sure it's not None before we
continue.


https://github.com/deepspeedai/DeepSpeed/blob/227a60c0c412ddf4619401b5d8d9d1674aee17b5/deepspeed/compile/init_z3.py#L22-L25

Signed-off-by: Hollow Man <hollowman@opensuse.org>
Co-authored-by: Masahiro Tanaka <81312776+tohtana@users.noreply.github.com>",['625ea528d112de997f34d276d709b721a94ca873'],False,['stage3.py']
9c9d32c2ca74497433284f5eb6aaa2a4a8d95959,"[NFC] Fix comment related to SP group (#7234)

Signed-off-by: c8ef <c8ef@outlook.com>",['962a8f0ad7693c6ebda901121f8b323b545f8cd6'],False,['groups.py']
00b5678bbf10c12b97a5f80d4b89247dcd837a95,"Update torch cpu test version

Signed-off-by: Logan Adams <loadams@microsoft.com>",['d79bd930d659b0d2f6d8a5bb27be4d5c04a87ae1'],False,['cpu-torch-latest.yml']
8d2865e0149bf54bf2a211d5e8529751e7420afc,"Revert ""Update torch cpu test version""

This reverts commit 00b5678bbf10c12b97a5f80d4b89247dcd837a95.",['00b5678bbf10c12b97a5f80d4b89247dcd837a95'],False,['cpu-torch-latest.yml']
9926879b5986d6903f61f2683be68ab8c48c9766,"Update CPU torch version to 2.7 (#7241)

Signed-off-by: Logan Adams <loadams@microsoft.com>",['8d2865e0149bf54bf2a211d5e8529751e7420afc'],False,['cpu-torch-latest.yml']
ee492c30a716474578d6d42115f2698ab131ab79,"Fix compile error for nv_bloat162 (#7248)

some systems seem not to have the __nv_bfloat162 definition so a
placeholder was introduced. newer CUDA libs have that definition, which
breaks the compile process. this patch adds the official cuda_bf16.h
guard while keeping the old code and a safety assert in case the
definition should change in the future. see #7190 for reference

---------

Signed-off-by: LosCrossos <165311345+loscrossos@users.noreply.github.com>
Signed-off-by: LosCrossos <165311345+mytait@users.noreply.github.com>
Co-authored-by: LosCrossos <165311345+mytait@users.noreply.github.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['fff77bd29306442af2c6f9ae22a1ae2788ba8cd6'],False,"['gelu.cu', 'transform.cu']"
069ec31c5968126a72135f7372481f0ba08baeb6,"Fix fp8 gemm (#7265)

This PR addresses this issue
https://github.com/deepspeedai/DeepSpeed/issues/7236.
I might have reverted some of the recent changes introduced in this
[PR](https://github.com/deepspeedai/DeepSpeed/pull/6932), which was
necessary to remove a misaligned address issue on the CUDA kernel. I
will get back to this and try to make the necessary changes for the
other pass.

cc: @mrwyattii @jeffra

---------

Co-authored-by: Reza Yazdani <reza.yazdani@snowflake.com>
Co-authored-by: Reza Yazdani <rezay@microsoft.com>
Co-authored-by: Jeff Rasley <jeffra45@gmail.com>
Co-authored-by: Michael Wyatt <michael.wyatt@snowflake.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['e1ba9e614f0faa52d0e8f1e240c2233282bd25d5'],False,"['fp_quantize.cpp', 'fp8_gemm_triton.py', 'quantize.py', 'fp_quantizer.py']"
930ab46e63b86878b6ce2c86d58b618cb21a7d64,"Fix issues XPU tests hit with extra-index-url (#7291)

cc: @Liangliang-Ma

---------

Signed-off-by: Logan Adams <loadams@microsoft.com>",['5a4e7a08ecc8c226d33e3a5fc9a3420ce731ec50'],False,"['xpu-max1100.yml', 'check-extraindexurl.py']"
d46947db4a21827faec26ca39b65e25719d68535,"Temporarily skip AIO tests due to an issue with runners (#7288)

Signed-off-by: Logan Adams <loadams@microsoft.com>
Signed-off-by: Olatunji Ruwase <tunji.ruwase@snowflake.com>
Co-authored-by: Olatunji Ruwase <tunji.ruwase@snowflake.com>",['930ab46e63b86878b6ce2c86d58b618cb21a7d64'],False,"['action.yml', 'test_intX_quantization.py', 'test_compile_zero.py']"
f45950258b4eda0105a0467c1a2d0435ba120255,"rollback #6726 (#7258)

This PR rollback #6726 which caused
https://github.com/deepspeedai/DeepSpeed/issues/7116 .

---------

Signed-off-by: Guokai Ma <guokai.ma@gmail.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Olatunji Ruwase <tunji.ruwase@snowflake.com>",['d46947db4a21827faec26ca39b65e25719d68535'],False,"['parameter_offload.py', 'partitioned_param_coordinator.py', 'test_zero_multiple_run.py']"
80bc7b76da6c7f6ab6bb790ea0b9038ece617403,"Add qwen3 meta loading for AutoTP (#7293)

This PR fixes https://github.com/deepspeedai/DeepSpeed/issues/7275 to
enable Qwen3 meta loading for AutoTP

Signed-off-by: Ma, Guokai <guokai.ma@intel.com>",['88a1b5c0578f9001dde9dbbd6a62d0bd34c64b51'],False,['auto_tp.py']
d0ef6501b8371547cf9f12ed81c073e45f308445,"Avoid graph break by removing another redundant requires grad false (#7263)

This PR is an follow-up to [PR
#7158](https://github.com/deepspeedai/DeepSpeed/pull/7158) handling the
same issue in another place.
See [PR #7158](https://github.com/deepspeedai/DeepSpeed/pull/7158) for
details.

---------

Signed-off-by: Max Kovalenko <mkovalenko@habana.ai>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Hongwei Chen <33092912+hwchen2017@users.noreply.github.com>",['80bc7b76da6c7f6ab6bb790ea0b9038ece617403'],False,['partition_parameters.py']
b048cc2b46a559b8f81c4315008cec34e650bbe0,"Modernize system executable detection across components (#7290)

# PR Summary
This small PR resolves deprecation warnings caused by the use of
`distutils.spawn.find_executable`:
```python
DeprecationWarning: Use shutil.which instead of find_executable
```
Please note that `find_executable` is deprecated from Python 3.10 and
removed in 3.12. `shutil.which` available since Python 3.3.

Signed-off-by: Emmanuel Ferdman <emmanuelferdman@gmail.com>
Co-authored-by: Olatunji Ruwase <tunji.ruwase@snowflake.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['d0ef6501b8371547cf9f12ed81c073e45f308445'],False,"['numa.py', 'async_io.py', 'async_io.py', 'async_io.py', 'async_io.py']"
0e741714f5a708b75a4db0a8140f9016f3aa5fd3,"Enable ZeRO set/get APIs for NVMe offload (#7046)

- Extend APIs for
[debugging](https://deepspeed.readthedocs.io/en/latest/zero3.html#debugging)
and
[modifying](https://deepspeed.readthedocs.io/en/latest/zero3.html#modifying-partitioned-states)
ZeRO partitioned states to NVMe offload.
- Add vectorized update API. This is performance-critical for NVMe
offloading scenarios.

---------

Signed-off-by: Olatunji Ruwase <olruwase@microsoft.com>
Signed-off-by: Masahiro Tanaka <mtanaka@microsoft.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Logan Adams <loadams@microsoft.com>
Co-authored-by: Masahiro Tanaka <mtanaka@microsoft.com>
Co-authored-by: Masahiro Tanaka <81312776+tohtana@users.noreply.github.com>
Co-authored-by: Guanhua Wang <alexwgh333@gmail.com>",['b048cc2b46a559b8f81c4315008cec34e650bbe0'],False,"['nv-torch-latest-v100.yml', '__init__.py', 'optimizer_utils.py', 'partitioned_optimizer_swapper.py', 'pipelined_optimizer_swapper.py', 'utils.py', 'offload_states.py', 'stage3.py', '__init__.py', 'tensor_fragment.py', 'zero3.rst', 'test_offload_states.py', 'test_zero_tensor_fragment.py']"
41fceadeeb41c1a95e2b3aeef4d04077a5902b20,"Add qwen3moe meta loading for AutoTP (#7297)

Enable Qwen3-Moe meta loading for AutoTP, for issue
https://github.com/deepspeedai/DeepSpeed/issues/7275

Signed-off-by: ranzhejiang <zhejiang.ran@intel.com>",['0e741714f5a708b75a4db0a8140f9016f3aa5fd3'],False,['auto_tp.py']
0e3209a16ba595f7a9f5249dffdeb3cf9e420ce4,"Fix extra_repr_str when weight is None / in zero-3 (#7254)

extra_repr_str will be undefined if self.weight is None with current
code.

In addition, the shape is stored in ds_shape if it's in ZeRO-3, so we
also need to do this check (Although currently AutoTP hasn't supported
ZeRO-3).

```logs
  File ""deepspeed/__init__.py"", line 394, in tp_model_init
    model = TpTrainingManager(model=model, tp_size=tp_size, dtype=dtype).module
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""deepspeed/runtime/tensor_parallel/tp_manager.py"", line 35, in __init__
    self._apply_policies(parser_dict)
  File ""deepspeed/runtime/tensor_parallel/tp_manager.py"", line 47, in _apply_policies
    self._apply_injection_policy(self.config, client_module)
  File ""deepspeed/runtime/tensor_parallel/tp_manager.py"", line 53, in _apply_injection_policy
    replace_transformer_layer(client_module, self.module, None, self.config, self.model_config)
  File ""deepspeed/module_inject/replace_module.py"", line 400, in replace_transformer_layer
    replaced_module = replace_module(model=model,
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""deepspeed/module_inject/replace_module.py"", line 653, in replace_module
    replaced_module, _ = _replace_module(model, policy, state_dict=sd)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""deepspeed/module_inject/replace_module.py"", line 713, in _replace_module
    _, layer_id = _replace_module(child,
                  ^^^^^^^^^^^^^^^^^^^^^^
  File ""deepspeed/module_inject/replace_module.py"", line 713, in _replace_module
    _, layer_id = _replace_module(child,
                  ^^^^^^^^^^^^^^^^^^^^^^
  File ""deepspeed/module_inject/replace_module.py"", line 689, in _replace_module
    replaced_module = policies[child.__class__][0](child,
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""deepspeed/module_inject/replace_module.py"", line 333, in replace_fn
    new_module = replace_wo_policy(child, _policy, prefix=prefix, state_dict=state_dict)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""deepspeed/module_inject/replace_module.py"", line 316, in replace_wo_policy
    return _autotp._replace_module(module)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""deepspeed/module_inject/auto_tp.py"", line 481, in _replace_module
    self._replace_module(child, name, class_name)
  File ""deepspeed/module_inject/auto_tp.py"", line 466, in _replace_module
    setattr(r_module, name, self.linear_policies[child.__class__](child, prev_name + '.' + name,
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""deepspeed/module_inject/auto_tp.py"", line 361, in _replace
    if 'Yuan' in str(self.module):
                 ^^^^^^^^^^^^^^^^
  File ""torch/nn/modules/module.py"", line 2940, in __repr__
    mod_str = repr(module)
              ^^^^^^^^^^^^
  File ""torch/nn/modules/module.py"", line 2940, in __repr__
    mod_str = repr(module)
              ^^^^^^^^^^^^
  File ""torch/nn/modules/module.py"", line 2934, in __repr__
    extra_repr = self.extra_repr()
                 ^^^^^^^^^^^^^^^^^
  File ""deepspeed/module_inject/layers.py"", line 267, in extra_repr
    out_features, in_features = self.weight.shape[-2:] if self.weight is not None else (None, None)
    ^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: not enough values to unpack (expected 2, got 1)
```

Signed-off-by: Hollow Man <hollowman@opensuse.org>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['e290bf580d90324abccd2e17b1f4f58c3d54e910'],False,['layers.py']
b666844ffc1aea30dc4f66112f01855b54a736ad,"Fix AutoTP gathering replaced layer params when bias is not None (#7257)

Some params are one-dimensional, this PR adds support for these params.

Resolve #7249

```log
param.shape torch.Size([768, 1536])
param.shape torch.Size([768])
...
```

```log
with deepspeed.module_inject.layers.GatherReplacedLayerParams([param], model, enabled=True):
     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File ""deepspeed/module_inject/layers.py"", line 359, in __enter__
self.params[0].gather_params(self.params)
File ""torch/utils/_contextlib.py"", line 116, in decorate_context
return func(*args, **kwargs)
       ^^^^^^^^^^^^^^^^^^^^^
File ""deepspeed/module_inject/layers.py"", line 473, in gather_params
param.shape[1],
~~~~~~~~~~~^^^
IndexError: tuple index out of range
```

---------

Signed-off-by: Hollow Man <hollowman@opensuse.org>
Signed-off-by: inkcherry <mingzhi.liu@intel.com>
Co-authored-by: Hongwei Chen <33092912+hwchen2017@users.noreply.github.com>
Co-authored-by: inkcherry <mingzhi.liu@intel.com>",['d4032ec7d17d7260094644f254fc4756adfade07'],False,"['layers.py', 'test_autotp_training.py']"
17c8be07060045632190bd1f66e482192be0c1dd,"Fix the GPU memory usage of ZeRO-Offload (only update stage_1_and_2.py) (#7309)

Signed-off-by: Armin Zhu <mingzhengzhu1998@gmail.com>

Fix the memory usage of ZeRO-Offload with stage 1 and 2. Before the fix,
the memory usage is about 3x that of params_FP16. This is caused by the
H2D data copy is using different data type. Now the GPU memory usage is
about 1x params_FP16. And the H2D memory copy needs a 16bit pinned
memory buffer.",['b666844ffc1aea30dc4f66112f01855b54a736ad'],False,['stage_1_and_2.py']
b9af5d8d6181c68b8b3f9e754442bd12ab3a914e,"Fix: Update grad norm calculation for CPU offload (#7302)

## Description
This PR fixes an issue where gradient clipping modifications are not
reflected in the global gradient norm calculation when CPU offloading is
enabled. The issue occurs because the `averaged_gradients` are not being
updated with the clipped gradients when CPU offloading is active.

## Problem
When using CPU offloading with gradient clipping:
1. The gradients are successfully clipped using `safe_set_local_grad`
2. However, the `_global_grad_norm` calculation still uses the original
unclipped gradients.
3. This leads to incorrect gradient norm reporting and potential issues
with gradient clipping effectiveness

## Solution
The fix ensures that the `averaged_gradients` are properly updated with
the clipped gradients when CPU offloading is enabled, similar to how it
works when CPU offloading is disabled.

## Testing
The fix has been tested with:
- CPU offloading enabled and disabled
- Different gradient clipping values
- A simple model with linear layers
- Both FP16 and BF16

## Related Issues
Fixes #7292

---------

Signed-off-by: Naveenraj Kamalakannan <therealnaveenkamal@gmail.com>",['17c8be07060045632190bd1f66e482192be0c1dd'],False,"['stage3.py', 'test_zero_grad_clip.py']"
b4cc079eee835d8dbcbd5d9b894380830e5c5abf,"CI: prefer bf16 over fp16 (#7304)

these days fp16 is barely ever used, so we should be testing bf16
instead of fp16 where possible.

had to fix a bunch of tests to adapt to this change. a few bugs as well
on the way.

---------

Signed-off-by: Stas Bekman <stas.bekman@snowflake.com>
Co-authored-by: Olatunji Ruwase <tunji.ruwase@snowflake.com>
Co-authored-by: Stas Bekman <stas.bekman@snowflake.com>",['b9af5d8d6181c68b8b3f9e754442bd12ab3a914e'],False,"['nv-torch-latest-v100.yml', 'action.yml', 'partition_parameters.py', 'common.py', 'test_lr_scheduler.py', 'test_other_optimizer.py', 'test_zero_optimizer.py', 'common.py', 'test_moe.py', 'test_onebit.py', 'test_fp16.py', 'test_ds_config_dict.py', 'test_ds_initialize.py', 'test_multi_output_model.py', 'test_mup_optimizers.py', 'test_pld.py', 'test_ignore_unused_parameters.py', 'test_unwrap_model.py', 'test_zero.py', 'test_zero_context.py', 'test_zero_context_return.py', 'test_zero_offloadpp.py', 'test_zeropp.py']"
e5afb8876065ae72fa84aa40ebebeddcdd7c06a0,"`tests/conftest.py`: automatically add local deepspeed repo when running tests (#7317)

This is a follow up to https://github.com/deepspeedai/DeepSpeed/pull/923

my original code was a copy from transformers, which has a different fs
layout and I missed that. So this PR is fixing it to actually do the
right thing.

Now you can have multiple clones of deepspeed and the tests will use the
local repo automatically and not the pre-installed deepspeed.",['b4cc079eee835d8dbcbd5d9b894380830e5c5abf'],False,['conftest.py']
b66c81077c05b0f76c49d94b1381669248293714,"anchor transformers version (#7316)

some features require minimal transformers versions so let's start
anchoring.

and fixing tests that break with recent transformers.

I need this fixed to be able to merge
https://github.com/deepspeedai/DeepSpeed/pull/7268 which requires
`transformers>=4.51.3`

---------

Signed-off-by: Stas Bekman <stas.bekman@snowflake.com>
Co-authored-by: Stas Bekman <stas.bekman@snowflake.com>",['ec6b254dce2c51789d2565707ac0c1e3eb847b3c'],False,"['nv-flash-attn.yml', 'nv-mii.yml', 'requirements-dev.txt', 'test_ctx.py']"
0baf79ead0c0d63c10cb2b981655526f3da6f2bc,"fix asymmetric in dequantize (#7283)

框架中反量化算子（dequantize）在非对称量化产生结果时只将float类型转成half类型，漏掉了对float类型的转换，导致在输出是float类型时，会产生精度误差。
<img width=""809"" alt=""企业微信截图_1747294273387""
src=""https://github.com/user-attachments/assets/3be19f06-89fe-404c-bc32-efcacc31bb1d""
/>

---------

Co-authored-by: 潘俊涵 <sp.junhan.pan@enflame-tech.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Olatunji Ruwase <tunji.ruwase@snowflake.com>
Co-authored-by: Zhipeng Wang <zhipeng.rainbowserie@gmail.com>",['b66c81077c05b0f76c49d94b1381669248293714'],False,['quantization_utils.h']
4d00b38ada8db8e4eca33a5c2f9139cecc5d3fd1,"Ulysses SP for HF Integration (#7268)

This is the Deepspeed counterpart of
https://github.com/snowflakedb/ArcticTraining/pull/45 - as the new
feature(s) require changes on both sides.


For PR reviewers: 

Readiness status:
- [x] Code
- [x] Tests
- [ ] Docs - working on it


Features:

- [x] add support for delaying grad addition via
`param.ds_grad_is_ready` flag (used when performing tiled compute in an
autograd function)
- [x] add light sp-only mpu version (Jeff Rasley)
- [x] improved debug
- [x] added `all_gather_object` to `dist`
- [x] `UlyssesSPAttentionHF` (port of UlyssesAttention from
Megatron-Deepspeed plus modern MHA-variations)
- [x] `UlyssesSPDataLoaderAdapter` - DL adapter to shard the normal DL
batches to be used by `UlyssesSPAttentionHF`
- [x] `SequenceTiledCompute` - generic autograd function to perform
compute after tiling on the sequence dimension
- [x] `TiledMLP` - a specific autograd function to perform tiled MLP
(it's much easier to understand before trying to grok
`SequenceTiledCompute`)
- [x] added a differentiable `_DimZeroAllToAll` (Samyam Rajbhandari)
- [x] torch-dist-check now allows `torch.distributed.nn` (which is
needed since deepspeed's dist is not up to date with
`torch.distributed.nn`)

---------

Signed-off-by: Stas Bekman <stas.bekman@snowflake.com>
Signed-off-by: Stas Bekman <stas@stason.org>
Co-authored-by: Stas Bekman <stas.bekman@snowflake.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>
Co-authored-by: Olatunji Ruwase <tunji.ruwase@snowflake.com>",['0baf79ead0c0d63c10cb2b981655526f3da6f2bc'],False,"['nv-ds-chat.yml', 'comm.py', 'torch.py', 'config.py', 'engine.py', '__init__.py', 'parallel_state_sp.py', 'ulysses_sp.py', 'utils.py', 'parameter_offload.py', 'partition_parameters.py', 'partitioned_param_coordinator.py', 'stage3.py', 'stage_1_and_2.py', 'layer.py', 'debug.py', 'groups.py', 'check-torchdist.py', 'test_tiled_compute.py', 'test_ulysses_sp_hf.py', 'util.py']"
8b03a35646930919fd9ae243afd2c4636008a83b,"Fix ci hang in torch2.7& improve ut (#7321)

fix ci hang.
improve the ut.

---------

Signed-off-by: inkcherry <mingzhi.liu@intel.com>
Co-authored-by: Olatunji Ruwase <tunji.ruwase@snowflake.com>",['4d00b38ada8db8e4eca33a5c2f9139cecc5d3fd1'],False,['test_autotp_training.py']
720787e79b5529dd3dd6ccbeccb4a6b710a0b876,"Bump to v0.17.0 (#7324)

Co-authored-by: Logan Adams <loadams@microsoft.com>",['8b03a35646930919fd9ae243afd2c4636008a83b'],False,"['cpu-torch-latest.yml', 'version.txt']"
b8d4b84260e4ba6632b9faa3aa5cbbaa5ca1614a,"Improve Ulysses Plus Docs (#7335)

Improve or fix some minor indentation, typo, and list numbering issues
of the Ulysses Plus tutorial.

---------

Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>",['097f0637d5b002f8d3e1a647f8622ef0359438a4'],False,['ulysses-plus-sequence-pallellism.md']
d0f7091aa49aeef4edfe8ea2ae7d504045964c4c,"Update config_utils.py (#7333)

Fixes this warning:

```
 /fsx/qgallouedec/miniconda3/envs/trl/lib/python3.12/site-packages/deepspeed/runtime/config_utils.py:100: PydanticDeprecatedSince211: Accessing the 'model_fields' attribute on the instance is deprecated. Instead, you should access this attribute from the model class. Deprecated in Pydantic V2.11 to be removed in V3.0.
    fields = self.model_fields
```

Co-authored-by: Olatunji Ruwase <tjruwase@gmail.com>",['b8d4b84260e4ba6632b9faa3aa5cbbaa5ca1614a'],False,['config_utils.py']
2ad2011cc99f6ef021f0dfcfc31e6830607960fa,"Fix pytest  version to 8.3.5 in hpu-gaudi actions (#7337)

This is needed to avoid the issue of ci failure in #7330 PR.

Signed-off-by: Shaik Raza Sikander <srsikander@habana.ai>
Co-authored-by: Olatunji Ruwase <tjruwase@gmail.com>",['d0f7091aa49aeef4edfe8ea2ae7d504045964c4c'],False,"['hpu-gaudi2-nightly.yml', 'hpu-gaudi2.yml']"
7d0c3f782e29383d44a8c979ae09ad5a6a421900,"Fix issue with symint input (#7243)

This PR fixes an issue with symint input in backend. (See #7229)

---------

Signed-off-by: Masahiro Tanaka <mtanaka@microsoft.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['2ad2011cc99f6ef021f0dfcfc31e6830607960fa'],False,"['__init__.py', 'backend.py', 'init_z3.py', 'util.py', 'engine.py']"
cb3ad0c176c292a9763cc1978f1f144432964d92,"fp16 optimizer timers fix - TypeError: 'NoneType' object is not callable  (#7330)

This fix is required to prevent the below error:

=================================== FAILURES
===================================
__________________ TestFp8ComposabilityAcrossZero.test[fp16]
___________________
multiprocessing.pool.RemoteTraceback:
""""""
Traceback (most recent call last):
File ""/opt/conda/envs/py_3.10/lib/python3.10/multiprocessing/pool.py"",
line 125, in worker
    result = (True, func(*args, **kwds))
File ""/opt/conda/envs/py_3.10/lib/python3.10/multiprocessing/pool.py"",
line 51, in starmapstar
    return list(itertools.starmap(args[0], args[1]))
File ""/root/PR/test/DeepSpeed/tests/unit/common.py"", line 322, in
_dist_run
    raise e
File ""/root/PR/test/DeepSpeed/tests/unit/common.py"", line 314, in
_dist_run
    self.run(**self._fixture_kwargs)
  File ""/root/PR/test/DeepSpeed/tests/unit/common.py"", line 470, in run
    self._current_test(**fixture_kwargs)
File
""/root/PR/test/DeepSpeed/tests/unit/runtime/half_precision/test_fp8.py"",
line 88, in test
    loss = run_zero(stage, model_dtype)
File
""/root/PR/test/DeepSpeed/tests/unit/runtime/half_precision/test_fp8.py"",
line 74, in run_zero
    model.step()
File ""/root/PR/test/DeepSpeed/deepspeed/runtime/engine.py"", line 2387,
in step
    self._take_model_step(lr_kwargs)
File ""/root/PR/test/DeepSpeed/deepspeed/runtime/engine.py"", line 2290,
in _take_model_step
    self.optimizer.step()
File
""/root/PR/test/DeepSpeed/deepspeed/runtime/fp16/fused_optimizer.py"",
line 255, in step
    self.timers(OVERFLOW_CHECK_TIMER).start()
TypeError: 'NoneType' object is not callable
""""""

Co-authored-by: Olatunji Ruwase <tjruwase@gmail.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Olatunji Ruwase <tunji.ruwase@snowflake.com>",['7d0c3f782e29383d44a8c979ae09ad5a6a421900'],False,['engine.py']
24a1d8f9365ba778407ab32e729fc91c2d0627dd,"DeepNVMe update (#7215)

- FastPersist
- ZeRO-Inference+SGLang

---------

Signed-off-by: Olatunji Ruwase <olruwase@microsoft.com>
Signed-off-by: Olatunji Ruwase <tunji.ruwase@snowflake.com>
Co-authored-by: jerryyangli <jerryyangli@gmail.com>
Co-authored-by: Yang Li <yangli2@microsoft.com>
Co-authored-by: Guanhua Wang <alexwgh333@gmail.com>
Co-authored-by: Connor Holmes <connorholmes@microsoft.com>
Co-authored-by: Bing Xie <67908712+xiexbing@users.noreply.github.com>
Co-authored-by: cassieesvelt <73311224+cassieesvelt@users.noreply.github.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>
Co-authored-by: Carlos Mocholí <carlossmocholi@gmail.com>
Co-authored-by: swli <47371259+lucasleesw@users.noreply.github.com>
Co-authored-by: Cheng Li <pistasable@gmail.com>
Co-authored-by: Molly Smith <112220543+molly-smith@users.noreply.github.com>
Co-authored-by: Ubuntu <jomayeri@microsoft.com>
Co-authored-by: Olatunji Ruwase <tunji.ruwase@snowflake.com>
Co-authored-by: Zhipeng Wang <zhipeng.rainbowserie@gmail.com>",['cb3ad0c176c292a9763cc1978f1f144432964d92'],False,"['README.md', 'dnvme_file_access.png', 'dnvme_scaling.png', 'fastpersist_phi3_mini.png', 'fastpersist_tensor.png', 'hf_zinf_llama_70b.png', 'sg_zinf_llama_70b.png', 'README.md', 'README.md', 'README.md', 'figure1.png', 'figure2.png', 'figure3.png', 'table1.png', 'deepspeed_aio_utils.cpp', 'deepspeed_aio_utils.h', 'deepspeed_aio_op_desc.cpp', 'deepspeed_aio_op_desc.h', 'deepspeed_cpu_op.cpp', 'deepspeed_cpu_op.h', 'deepspeed_py_aio_handle.h', 'deepspeed_py_io_handle.cpp', 'deepspeed_py_io_handle.h', 'py_ds_aio.cpp', 'aio_bench_perf_sweep.py', 'dgx2_v100_optimal_read.sh', 'dgx2_v100_optimal_write.sh', 'dgx2_v100_suboptimal_read.sh', 'dgx2_v100_suboptimal_write.sh', 'ds_aio_args.py', 'ds_aio_basic.py', 'ds_aio_constants.py', 'ds_aio_handle.py', 'io_engine.py', 'run_read_sweep.sh', 'run_write_sweep.sh', 'single_process_config.json', 'test_ds_aio.py', 'test_ds_aio_utils.py', 'torch_fastio_engine.py', 'torch_io.py', 'dgx2_mount_nvme.sh', 'dgx2_umount_nvme.sh', 'deepspeed_gds_op.cpp', 'deepspeed_gds_op.h', 'deepspeed_py_gds_handle.cpp', 'deepspeed_py_gds_handle.h', 'py_ds_gds.cpp', 'flatten_unflatten.cpp', 'py_ds_utils.cpp', 'tensor_cast.cpp', 'tensor_cast.h', 'constants.py', '__init__.py', 'base_file_writer.py', 'base_io_buffer.py', 'constants.py', 'double_io_buffer.py', 'fast_file_writer.py', 'mock_file_writer.py', 'py_file_writer.py', 'single_io_buffer.py', 'utils.py', 'launch.py', 'multinode_runner.py', 'runner.py', 'ds_aio_args.py', 'ds_aio_basic.py', 'ds_aio_constants.py', 'ds_aio_handle.py', 'io_engine.py', 'perf_generate_param.py', 'perf_run_sweep.py', 'test_ds_aio.py', 'test_ds_aio_utils.py', 'torch_fastio_engine.py', 'torch_io.py', 'README.md', '__init__.py', 'checkpoint_engine.py', 'decoupled_checkpoint_engine.py', 'fast_checkpoint_engine.py', 'nebula_checkpoint_engine.py', 'torch_checkpoint_engine.py', 'utils.py', 'config.py', 'engine.py', 'fused_optimizer.py', 'loss_scaler.py', '__init__.py', 'config.py', 'constants.py', 'data_parallel_writer_factory.py', 'utils.py', 'writer_factory.py', 'module.py', 'topology.py', 'partitioned_param_swapper.py', 'utils.py', '__init__.py', 'parameter_offload.py', 'partition_parameters.py', 'groups.py', 'utils.py', 'test_aio.py', 'test_gds.py', 'test_byte_cast.py']"
770967f5f036cdeeb49caae3b2f0c9d0d94d467c,"fixed: Modified the topkgating function and modified the test_moe file for testing (#7163)

Since the previous PR encountered the DCO problem and could not be
solved for some reason, I resubmitted a completely identical PR but
without the problem.

---------

Signed-off-by: xiongjyu <xiongjyu@gmail.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Olatunji Ruwase <tjruwase@gmail.com>
Co-authored-by: Masahiro Tanaka <81312776+tohtana@users.noreply.github.com>",['24a1d8f9365ba778407ab32e729fc91c2d0627dd'],False,"['sharded_moe.py', 'test_moe.py']"
05818e90d916734d45fb4a23a85d17574161d0e2,"Fix LoRA arxiv reference (#7340)

## PR Summary
This small PR fixes the LoRA arxiv reference in
`mixed_precision_zeropp.md`. Relevant docs page:
https://www.deepspeed.ai/tutorials/mixed_precision_zeropp/

Signed-off-by: Emmanuel Ferdman <emmanuelferdman@gmail.com>",['770967f5f036cdeeb49caae3b2f0c9d0d94d467c'],False,['mixed_precision_zeropp.md']
e440506bee5f523691693a7fad6251202ec3dbcb,"Improve overflow handling in ZeRO (#6976)

Fix #5241: Improve overflow handling 
- [x] ZeRO 1
- [x] ZeRO 2
- [ ] ZeRO 3
- [ ] BF16Optimizer

Enable pydantic configuration for mixed precision
- [x] bf16
- [x] fp16

---------

Signed-off-by: Olatunji Ruwase <olruwase@microsoft.com>
Signed-off-by: Fabien Dupont <fdupont@redhat.com>
Signed-off-by: Logan Adams <loadams@microsoft.com>
Signed-off-by: inkcherry <mingzhi.liu@intel.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Xinyu Lian <lian7@illinois.edu>
Co-authored-by: loadams <loadams@users.noreply.github.com>
Co-authored-by: Omar Elayan <142979319+oelayan7@users.noreply.github.com>
Co-authored-by: Fabio Geraci <118277438+fabiosanger@users.noreply.github.com>
Co-authored-by: Sam Foreman <saforem2@gmail.com>
Co-authored-by: Fabien Dupont <fabiendupont@fabiendupont.fr>
Co-authored-by: Liangliang Ma <1906710196@qq.com>
Co-authored-by: inkcherry <mingzhi.liu@intel.com>
Co-authored-by: Logan Adams <loadams@microsoft.com>
Co-authored-by: Olatunji Ruwase <tunji.ruwase@snowflake.com>",['bb293aea5d61198327f8d8956925ab754b747f44'],False,"['bf16_optimizer.py', 'config.py', 'constants.py', 'engine.py', 'loss_scaler.py', 'hybrid_engine.py', 'engine.py', 'precision_config.py', 'partition_parameters.py', 'stage_1_and_2.py', 'tensor_fragment.py', 'test_dynamic_loss_scale.py', 'test_zero_optim_overflow.py', 'test_ds_config_dict.py', 'test_multiple_models.py', 'test_zero.py']"
4d0c159630efd7b36ae587c1c979af98cb301524,"Fix docs that are rendering Incorrectly (#7344)

Fixes #6747 

### Changes

- Added missing imports required for the documentation to render
correctly.
- Changed `autoclass_content` from `auto` to `both`
The value `auto` is **not valid** according to the [Sphinx
documentation](https://www.sphinx-doc.org/en/master/usage/extensions/autodoc.html#confval-autoclass_content).


### Preview

Sample fixed page:
https://deepspeedfelixgondwefork.readthedocs.io/en/latest/model-checkpointing.html

Current broken page:
https://deepspeed.readthedocs.io/en/latest/model-checkpointing.html

---------

Signed-off-by: felixgondwe <zungwala@gmail.com>
Signed-off-by: Shaik Raza Sikander <srsikander@habana.ai>
Signed-off-by: Masahiro Tanaka <mtanaka@microsoft.com>
Signed-off-by: Olatunji Ruwase <olruwase@microsoft.com>
Signed-off-by: Olatunji Ruwase <tunji.ruwase@snowflake.com>
Signed-off-by: xiongjyu <xiongjyu@gmail.com>
Signed-off-by: Emmanuel Ferdman <emmanuelferdman@gmail.com>
Co-authored-by: Quentin Gallouédec <45557362+qgallouedec@users.noreply.github.com>
Co-authored-by: Olatunji Ruwase <tjruwase@gmail.com>
Co-authored-by: Raza Sikander <srsikander@habana.ai>
Co-authored-by: Masahiro Tanaka <81312776+tohtana@users.noreply.github.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Ramya Ramineni <62723901+rraminen@users.noreply.github.com>
Co-authored-by: Olatunji Ruwase <tunji.ruwase@snowflake.com>
Co-authored-by: jerryyangli <jerryyangli@gmail.com>
Co-authored-by: Yang Li <yangli2@microsoft.com>
Co-authored-by: Guanhua Wang <alexwgh333@gmail.com>
Co-authored-by: Connor Holmes <connorholmes@microsoft.com>
Co-authored-by: Bing Xie <67908712+xiexbing@users.noreply.github.com>
Co-authored-by: cassieesvelt <73311224+cassieesvelt@users.noreply.github.com>
Co-authored-by: Jeff Rasley <jerasley@microsoft.com>
Co-authored-by: Michael Wyatt <michaelwyatt@microsoft.com>
Co-authored-by: Carlos Mocholí <carlossmocholi@gmail.com>
Co-authored-by: swli <47371259+lucasleesw@users.noreply.github.com>
Co-authored-by: Cheng Li <pistasable@gmail.com>
Co-authored-by: Molly Smith <112220543+molly-smith@users.noreply.github.com>
Co-authored-by: Ubuntu <jomayeri@microsoft.com>
Co-authored-by: Zhipeng Wang <zhipeng.rainbowserie@gmail.com>
Co-authored-by: xiongjyu <xiongjyu@gmail.com>
Co-authored-by: Emmanuel Ferdman <emmanuelferdman@gmail.com>",['e440506bee5f523691693a7fad6251202ec3dbcb'],False,['conf.py']
2ce55057999bdf1c9bf26cb5f000a8a9fb97e2d9,"Move pytest pinning from individual tests to requirements-dev.txt until fixed. (#7327)

pytest 8.4.0 seems to break a number of our tests, rather than pinning
in each individually, we should just put this in the requirements file
until we resolve the issue.

---------

Co-authored-by: Olatunji Ruwase <tjruwase@gmail.com>",['4d0c159630efd7b36ae587c1c979af98cb301524'],False,"['cpu-torch-latest.yml', 'hpu-gaudi2-nightly.yml', 'hpu-gaudi2.yml', 'requirements-dev.txt']"
d7e60fd0f6f214f618b7af0396358f1e50161e8f,"s/UlyssesPlus/Arctic Long Sequence Training (ALST)/ (#7348)

The project has been renamed at the last moment, so this PR is adapting
to that change.

There are no code changes in this PR, just docs.

---------

Signed-off-by: Stas Bekman <stas@stason.org>",['4686d5ef0b450a42f3807231f4e9b2544c14ae6d'],False,"['ulysses_sp.py', 'navigation.yml', 'ds-sequence.md', 'ulysses-alst-sequence-pallellism.md', 'test_tiled_compute.py', 'test_ulysses_sp_hf.py']"
10b106619a0da36e0fdd7b3c3a2cf8bd6eefa002,"Don't break set_start_method (#7349)

Fix #7347

---------

Signed-off-by: Tunji Ruwase <tunji@ip-172-31-0-204.us-west-2.compute.internal>
Signed-off-by: Olatunji Ruwase <tjruwase@gmail.com>
Co-authored-by: Tunji Ruwase <tunji@ip-172-31-0-204.us-west-2.compute.internal>",['d7e60fd0f6f214f618b7af0396358f1e50161e8f'],False,"['cpu-torch-latest.yml', 'decoupled_checkpoint_engine.py', 'test_ds_initialize.py']"
766312154954bab3003193417da1a2f79d14cc34,"Fix error of <glog/logging.h> (#7351)

Fix #7350",['10b106619a0da36e0fdd7b3c3a2cf8bd6eefa002'],False,['builder.py']
600d280f211a1479cde6ff59790876b10c5f7003,"Improve padding util for compile (#7355)

This PR improves `pad_tensors` in `deepspeed/compile/util.py`, which
pads tensors so that all ranks have tensors with the same shape.
Previously, this function only adjusts tensor shapes, but tensor strides
could differ across ranks, leading to recompilation on only some ranks.
As DeepCompile inserts communication operators in the graph, the
communication collective easily gets stuck.

To address this issue, this PR replaces the use of
`torch.nn.functional.pad` with a new approach that ensures consistent
strides and avoids communication issues during distributed operations.

Signed-off-by: Masahiro Tanaka <mtanaka@microsoft.com>",['766312154954bab3003193417da1a2f79d14cc34'],False,['util.py']
9ac94414000978054dd67b298d91b603ae794ce8,"Fix 404s (#7363)

Signed-off-by: Olatunji Ruwase <tjruwase@gmail.com>",['600d280f211a1479cde6ff59790876b10c5f7003'],False,['README.md']
f394e7803611f31157c5f449ba0c7f01859e936a,"Fix tutorial title (#7365)

Missed this renamed in last PR
https://github.com/deepspeedai/DeepSpeed/pull/7348",['9ac94414000978054dd67b298d91b603ae794ce8'],False,['navigation.yml']
6f1a1c04c15dc598186e7c290860279f3a325df9,"Restore real inputs for recompilation (#7356)

This PR keeps some of real inputs given to the custom backend for
DeepCompile.

DeepCompile expects that the custom backend at TorchFX graph level is
always called when recompilation happens. In some cases, however, only
the Aten-level backend is called. As the Aten-level backend uses real
inputs saved by TorchFX-level backend, we need to keep the real inputs
for recompilation.

Currently we discard the real inputs after the Aten-level backend uses
it as the real inputs are often too large to keep in GPU memory. This
causes an error in cases where recompilation only calls Aten-level
backends because we don't have a chance to record new real inputs in
TorchFX-level backend.

This PR always keeps only tensor metadata and non-tensor data on CPU and
materialize the tensors when needed (i.e. when recompilation happens and
only Aten-level backends are called without real inputs). As we use
dummy data to materialize tensors, this solution might still not work
but improves the coverage.
The new module `InputStorage` keeps tensor metadata and non-tensor data
for this purpose and materialize tensors.

---------

Signed-off-by: Masahiro Tanaka <mtanaka@microsoft.com>",['f394e7803611f31157c5f449ba0c7f01859e936a'],False,"['backend.py', 'config.py', 'init_z1.py', 'init_z3.py', 'input_storage.py']"
22cf1a44013cd026a91a4a048053934d1eb80ef4,"Fix(scheduler): WarmupLR inherits optimizer lr when not specified (#7360)

This PR fixes issue #7303.

### 1. Description of the Bug

Currently, when using the `WarmupLR` scheduler, if `warmup_max_lr` is
not explicitly set in the scheduler's parameters, it incorrectly falls
back to its internal default value (`0.001`), ignoring the learning rate
set in the optimizer's parameters. This can lead to unexpected training
behavior and diverges from user expectations.

### 2. Description of the Fix

This fix modifies the `__init__` method of the `WarmupLR` scheduler in
`deepspeed/runtime/lr_schedules.py`.

- The default value for the `warmup_max_lr` argument in the function
signature is changed from `0.001` to `None`.
- Logic is added to check if `warmup_max_lr` is `None` upon
initialization. If it is, the scheduler now correctly inherits the
learning rate from the optimizer's parameter groups.

This change ensures that the optimizer's learning rate is respected as
the default `warmup_max_lr`, aligning the scheduler's behavior with the
user's configuration intent.

### 3. Verification

The fix has been verified using a minimal reproduction script that
clearly demonstrates the behavioral change.

**Before Fix:**
Without `warmup_max_lr` in the scheduler config, the learning rate
incorrectly defaults to `0.001`.
<img width=""1711"" alt=""Screenshot 2025-06-16 at 18 34 31""
src=""https://github.com/user-attachments/assets/fe68f39e-2bbc-4f94-b322-546d9ce43bb0""
/>


**After Workaround (Demonstrating the Mechanism):**
By explicitly adding `warmup_max_lr` to the scheduler config, the
learning rate behaves as expected. My code change makes this the default
behavior.
<img width=""1195"" alt=""Screenshot 2025-06-16 at 20 17 11""
src=""https://github.com/user-attachments/assets/cc170246-fdac-4a56-8b9c-f204ebb47895""
/>

Signed-off-by: Vensenmu <vensenmu@gmail.com>
Co-authored-by: Olatunji Ruwase <tjruwase@gmail.com>",['6f1a1c04c15dc598186e7c290860279f3a325df9'],False,['lr_schedules.py']
ed5f737554bf30f04c209e6236852f92386318b6,"Enable torch.autocast with ZeRO (#6993)

DeepSpeed supports mixed precision training, but the behavior is
different from `torch.autocast`. DeepSpeed maintains parameters and
gradients both in FP32 and a lower precision (FP16/BF16) (NVIDIA Apex
AMP style) and computes all modules in the lower precision while
`torch.autocast` maintains parameters in FP32 but computes only certain
operators in the lower precision.
This leads to differences in:
- performance: `torch.autocast` needs downcast in forward/backward
- memory usage: DeepSpeed needs more memory to keep copies of parameters
and gradients in lower precision
- accuracy: `torch.autocast` has a list of modules that can safely be
computed in lower precision. Some precision-sensitive operators (e.g.
softmax) are computed in FP32.

To align DeepSpeed's behavior with `torch.autocast` when necessary, this
PR adds the integration with `torch.autocast` with ZeRO. Here is an
examples of the configuration.

```json
""torch_autocast"": {
  ""enabled"": true,
  ""dtype"": ""bfloat16"",
  ""lower_precision_safe_modules"": [""torch.nn.Linear"", ""torch.nn.Conv2d""]
}
```

Each configuration works as follows:
- `enabled`: Enable the integration with `torch.autocast` if this is set
to `True`. You don't need to call `torch.autocast` in your code. The
grad scaler is also applied in the DeepSpeed optimizer.
- `dtype`: lower precision dtype passed to `torch.autocast`. Gradients
for allreduce (reduce-scatter) and parameters for allgather (only for
ZeRO3) of `lower_precision_safe_modules` are also downcasted to this
dtype.
- `lower_precision_safe_modules`: Downcast for allreduce
(reduce-scatter) and allgather (ZeRO3) are applied only to modules
specified in this list. (The precision for PyTorch operators in
forward/backward follows `torch.autocast`'s policy, not this list.) You
can set names of classes with their packages. If you don't set this
item, DeepSpeed uses the default list: `[torch.nn.Linear,
torch.nn.Conv1d, torch.nn.Conv2d, torch.nn.Conv3d]`.

Note that we only maintain FP32 parameters with this feature enabled.
For consistency, you cannot enable `fp16` or `bf16` in DeepSpeed config.

---------

Signed-off-by: Masahiro Tanaka <mtanaka@microsoft.com>
Signed-off-by: Fabien Dupont <fdupont@redhat.com>
Signed-off-by: Olatunji Ruwase <olruwase@microsoft.com>
Signed-off-by: Logan Adams <loadams@microsoft.com>
Signed-off-by: inkcherry <mingzhi.liu@intel.com>
Signed-off-by: Omar Elayan <oelayan@habana.ai>
Signed-off-by: Roman Fitzjalen <romaactor@gmail.com>
Signed-off-by: Hongwei <hongweichen@microsoft.com>
Signed-off-by: shaomin <wukon1992@gmail.com>
Signed-off-by: Stas Bekman <stas@stason.org>
Signed-off-by: siqi <siqi@tecorigin.com>
Signed-off-by: Wei Wu <wuwei211x@gmail.com>
Signed-off-by: ShellyNR <shelly.nahir@live.biu.ac.il>
Signed-off-by: Lai, Yejing <yejing.lai@intel.com>
Co-authored-by: Olatunji Ruwase <olruwase@microsoft.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Fabien Dupont <fabiendupont@fabiendupont.fr>
Co-authored-by: Liangliang Ma <1906710196@qq.com>
Co-authored-by: inkcherry <mingzhi.liu@intel.com>
Co-authored-by: Omar Elayan <142979319+oelayan7@users.noreply.github.com>
Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>
Co-authored-by: Roman Fitzjalen <romaactor@gmail.com>
Co-authored-by: Ramya Ramineni <62723901+rraminen@users.noreply.github.com>
Co-authored-by: Guanhua Wang <alexwgh333@gmail.com>
Co-authored-by: root <root@ftqtmec25000000.taxzvufipdhelhupulxcbvr15f.ux.internal.cloudapp.net>
Co-authored-by: Hongwei Chen <33092912+hwchen2017@users.noreply.github.com>
Co-authored-by: Joe Mayer <114769929+jomayeri@users.noreply.github.com>
Co-authored-by: wukong1992 <wukong1992@users.noreply.github.com>
Co-authored-by: shaomin <wukon1992@gmail.com>
Co-authored-by: loadams <loadams@users.noreply.github.com>
Co-authored-by: siqi654321 <siqi202311@163.com>
Co-authored-by: siqi <siqi@tecorigin.com>
Co-authored-by: Wei Wu <45323446+U-rara@users.noreply.github.com>
Co-authored-by: Shelly Nahir <73890534+ShellyNR@users.noreply.github.com>
Co-authored-by: snahir <snahir@habana.ai>
Co-authored-by: Yejing-Lai <yejing.lai@intel.com>
Co-authored-by: Siddharth Singh <siddharth9820@gmail.com>
Co-authored-by: Olatunji Ruwase <tjruwase@gmail.com>",['d3b9cb8c4e785b44e2fe6516868c3ca4efcaa206'],False,"['base_optimizer.py', 'config.py', 'constants.py', 'engine.py', 'torch_autocast.py', 'offload_states.py', 'partition_parameters.py', 'stage3.py', 'stage_1_and_2.py', 'common.py', 'test_moe.py', 'util.py', 'test_zero_autocast.py']"
25da6fc1ab30e21c7e7346c3e384fffc1ef030dd,"Flops profiler support for F.interpolate (#7353)

Fix #4504 

Credit @xmfbit

---------

Signed-off-by: Olatunji Ruwase <tunji.ruwase@snowflake.com>
Co-authored-by: Hongwei Chen <33092912+hwchen2017@users.noreply.github.com>
Co-authored-by: Olatunji Ruwase <tjruwase@gmail.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['86097872c672961de659369c3a064e6911d61e10'],False,['profiler.py']
d33baf009bfc0d1a55b541c6131f3b47a5184330,"Relax tolerances for FP8 unit test only for ROCm + FP16  (#7373)

Relaxing the tolerance values to enable the below unit test, with FP16
data type on ROCm


`unit/runtime/half_precision/test_fp8.py::TestFp8ComposabilityAcrossZero::test[fp16]
`

```
        # Relax tolerance only for ROCm + FP16
        if is_rocm_pytorch() and model_dtype == torch.float16:
            rtol, atol = 3e-07, 3e-05
```

cc: @jithunnair-amd",['25da6fc1ab30e21c7e7346c3e384fffc1ef030dd'],False,['test_fp8.py']
d5f69151041c31cef6819040e9dc62e43bc0bb32,"Fix release of IPG buffer (#7376)

#6993 broke many paths in ZeRO1/2 optimizer. This PR fixes most of the
issues the PR caused. Currently we still have one error with tests in
`unit/runtime/zero`.

```
====================================== short test summary info ======================================
FAILED test_zero.py::TestParamPartitioningSkipInit::test[dtype1] - RuntimeError: mat1 and mat2 must have the same dtype, but got Half and BFloat16
========= 1 failed, 204 passed, 66 skipped, 15 deselected, 5 warnings in 2305.03s (0:38:25) =========
```

---------

Signed-off-by: Masahiro Tanaka <mtanaka@microsoft.com>",['9606f8f0100700fab58ead3a98bece26085878bd'],False,['stage_1_and_2.py']
8dd215162b30aec07816e8fc50913c8776d5abc9,"fix wandb.log() call by removing `sync` kwarg (#7383)

Remove `sync` kwarg from `wandb.log()` invocation, which was [removed in
wandb==0.20.0](https://github.com/wandb/wandb/releases/tag/v0.20.0).

Fixes #7381",['d5f69151041c31cef6819040e9dc62e43bc0bb32'],False,['wandb.py']
e049bbfa1c34073cb76b7efb56239012f1319fea,"Fix dtype mismatch in `TestParamPartitioningSkipInit` (#7377)

`TestParamPartitioningSkipInit` throws the following error.
```
====================================== short test summary info ======================================
FAILED test_zero.py::TestParamPartitioningSkipInit::test[dtype1] - RuntimeError: mat1 and mat2 must have the same dtype, but got Half and BFloat16
========= 1 failed, 204 passed, 66 skipped, 15 deselected, 5 warnings in 2305.03s (0:38:25) =========
```

The test always sets the model's dtype to `torch.bfloat16` and ignores
the test parameter `dtype` when bfloat16 is supported. This causes a
dtype mismatch when `dtype=torch.float16` is given as the test parameter
because the data loader respects the test parameter dtype.

---------

Signed-off-by: Masahiro Tanaka <mtanaka@microsoft.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['8dd215162b30aec07816e8fc50913c8776d5abc9'],False,['test_zero.py']
61829b55ea59a5f7d6386f4f061ca13e7aeccae3,"fix(inference): Add missing dtype attribute to ParameterBase setter (#7378)

### Description

This PR fixes an `AttributeError: 'UnembedParameter' object has no
attribute 'dtype'` that occurs in the Inference V2 engine. The issue is
triggered when using a high-level interface like
[DeepSpeed-MII](https://github.com/deepspeedai/DeepSpeed-MII) to run
inference on models with tied input/output embeddings, such as Llama 2.

**Resolves: #7260**

### Root Cause Analysis

The root cause is that while the `ParameterBase` metaclass correctly
creates property setters for parameter tensors, the setter function
(`param_setter`) only assigns the tensor value itself. It does not
propagate the tensor's `dtype` to the container instance.

Downstream functions, such as `flatten_inference_model`, expect every
parameter container to have a `.dtype` attribute. When they encounter a
custom container like `UnembedParameter` that lacks this attribute, an
`AttributeError` is raised.

### The Fix

The solution is to modify the `param_setter` function within
`make_param_setter` located in
`deepspeed/inference/v2/model_implementations/parameter_base.py`.

I have added the line `self.dtype = value.dtype` immediately after the
parameter tensor is assigned. This simple change ensures that any object
inheriting from `ParameterBase` will now correctly expose the `dtype` of
the tensor it wraps, resolving the error.

### Verification

This fix has been thoroughly verified in a containerized GPU environment
(RunPod with PyTorch 2.1). The verification process involved:
1. Cloning both the `deepspeed` and `DeepSpeed-MII` repositories from
source.
2. Installing the modified `deepspeed` library from this branch.
3. Installing the `DeepSpeed-MII` library (with a packaging fix) to
trigger the bug.
4. Running an end-to-end inference script with `mii.pipeline` and a
standard language model.

The logs confirm that with this fix, the program successfully executes
past the original point of failure. The `AttributeError` is completely
resolved, and the DeepSpeed engine proceeds correctly to the model
loading phase.

*(Note: A full end-to-end run in the test environment was ultimately
blocked by a separate, pre-existing build issue in DeepSpeed's op
builder (`ModuleNotFoundError: dskernels`), which is unrelated to this
logic fix. The successful progression past the original error point
serves as definitive proof of this fix's effectiveness.)*

### Related Context

This bug is primarily triggered via the
[**DeepSpeed-MII**](https://github.com/deepspeedai/DeepSpeed-MII)
project. A companion PR,
**[deepspeedai/DeepSpeed-MII#567](https://github.com/deepspeedai/DeepSpeed-MII/pull/567)**,
has been submitted to fix a packaging issue in that repository that was
a prerequisite for this verification.

output：

<img width=""1014"" alt=""Screenshot 2025-06-22 at 14 16 15""
src=""https://github.com/user-attachments/assets/1a658f98-a98b-4584-ae11-59e9edfd0b7e""
/>

<img width=""1012"" alt=""Screenshot 2025-06-22 at 14 16 26""
src=""https://github.com/user-attachments/assets/3959d0e5-d6dc-4ed4-adbc-6919e00da172""
/>

<img width=""1728"" alt=""Screenshot 2025-06-22 at 14 17 40""
src=""https://github.com/user-attachments/assets/537fd354-b840-4af2-98ab-d243c6902412""
/>

Signed-off-by: Vensenmu <vensenmu@gmail.com>
Co-authored-by: Masahiro Tanaka <81312776+tohtana@users.noreply.github.com>",['2a450b3a339a1f61bac982d307fe2415a4ba23fb'],False,['parameter_base.py']
ebb64239a6768fa35f6ca25d0c112e4a59434486,fix broken url (#7390),['ec73d91b4d3b82bf95b01920138ce76110549a2e'],False,['ulysses_sp.py']
6c469425dc601f722038f68d2f57fdec0665843c,"Fix unbound local error for `return_val` (#7395)

......
File ""torch/_dynamo/backends/common.py"", line 72, in
_wrapped_bw_compiler
  return disable(disable(bw_compiler_fn)(*args, **kwargs))
File ""torch/_dynamo/eval_frame.py"", line 838, in _fn
  return fn(*args, **kwargs)
File ""deepspeed/compile/inductor.py"", line 27, in wrapped_compiler
  mod_graph = dc_compiler(gm, fake_inputs)
File ""deepspeed/compile/backend.py"", line 330, in make_bw_graph
  run_opt_passes(
File ""deepspeed/compile/backend.py"", line 206, in run_opt_passes
  mem_prof.run(*create_inputs_fn())
File ""deepspeed/compile/profilers/graph_profile.py"", line 261, in run
  return return_val
UnboundLocalError: local variable 'return_val' referenced before
assignment

Signed-off-by: Hollow Man <hollowman@opensuse.org>
Co-authored-by: Masahiro Tanaka <81312776+tohtana@users.noreply.github.com>",['59bb08bf908ae3516b806744b36672b533e4405f'],False,['graph_profile.py']
be8124c88edc9f8d3ba5deb2314b7fb990fedf49,"Fix ZeRO stage 1 and add stage 2 support with DeepCompile (#7366)

This PR fixes the behavior of DeepCompile's ZeRO stage 1 and adds stage
2 support.

DeepCompile's ZeRO1 currently performs allreduce at every iteration even
when it is not a gradient accumulation boundary. This significantly
slows down the performance when gradient accumulation is enabled. This
PR fixes this issue by performing allreduce only at the gradient
accumulation boundary.

As the current behavior is similar to ZeRO2, this PR also adds
DeepCompile's ZeRO2 support. We can now set zero stage to 2 with
DeepCompile.

The loss values, performance, and memory usages were verified using this
[verification tool](https://github.com/tohtana/ds_verify_loss)
([results](https://github.com/tohtana/ds_verify_loss/blob/main/results/results_20250617_035117/report.md)).

---------

Signed-off-by: Masahiro Tanaka <mtanaka@microsoft.com>
Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['6c469425dc601f722038f68d2f57fdec0665843c'],False,"['init.cpp', 'z1.cpp', 'z1.h', 'z2.cpp', 'z2.h', 'z3.cpp', 'deepcompile.h', 'init_z1.py', 'zero1_compile.py', 'engine.py', 'dc.py']"
6594c266c2e313241c34f8c870dd295570c6b0a5,"Improve coverage of DeepCompile (#7386)

This PR improves the coverage of DeepCompile.

- Use real parameters when recompilation happens
- Handling overflow error in profiling

This PR should be merged after #7366.

ZeRO1 and ZeRO3 both worked with OpenRLHF. See [Wiki
page](https://github.com/tohtana/DeepCompile_docs/wiki/Debug-with-OpenRLHF-(%237243))
for more details.

---------

Signed-off-by: Masahiro Tanaka <mtanaka@microsoft.com>
Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>",['be8124c88edc9f8d3ba5deb2314b7fb990fedf49'],False,"['backend.py', 'graph_param.py', 'graph_profile.py']"
4c687bfdac2b290489761e1175a598186895b922,"Added device detection to communication logging (#7398)

In `comms_logging.py`, when calling log_all and the `show_straggler`
option is enabled, an all_reduce is performed across all nodes to
calculate the minimum latency to find stragglers. However, the tensors
on which this is performed are not sent to the configured devices. This
commit adds this capability using deepspeed's abstract accelerator api.

Resolves #7397

Signed-off-by: Alex Kiefer <alexkiefer51@gmail.com>
Co-authored-by: Masahiro Tanaka <81312776+tohtana@users.noreply.github.com>",['6594c266c2e313241c34f8c870dd295570c6b0a5'],False,['comms_logging.py']
a755a9e52c973ced97a2a5c621500cb6dc99cb12,"fix: Add `csrc/compile` to include paths for DeepCompile builder (#7401)

Since currently `z1.h`, `z2.h` and `z3.h` are located under
`csrc/compile`, without this patch, torch hipify will fail to identify
these hipified headers on AMD platform:

```log
In file included from torch/include/ATen/cuda/CUDAEvent.h:3,
                  from deepspeed/ops/csrc/includes/deepcompile.h:16,
                  from deepspeed/ops/csrc/compile/z1.h:6,
                  from deepspeed/ops/csrc/compile/z1_hip.cpp:7:
 torch/include/ATen/cuda/ATenCUDAGeneral.h:3:10: fatal error: cuda.h: No such file or directory
     3 | #include <cuda.h>
       |          ^~~~~~~~
 compilation terminated.
```

Signed-off-by: Hollow Man <hollowman@opensuse.org>",['4c687bfdac2b290489761e1175a598186895b922'],False,['dc.py']
384e6d34142f69283b84a714197b172a790f20a2,"fix: DeepCompile for torch 2.8 (#7402)

In torch v2.8.0, all symm mem code are moved into a dedicated folder
https://github.com/pytorch/pytorch/commit/ffc6cbfaf78ca219092ce64dcf113377ae698300

So this PR tries to address this change by checking if we have it
located under `torch/csrc/distributed/c10d/symm_mem/SymmetricMemory.hpp`
(new location). If not, we fall back to the original place for backward
compatibilities.

This PR also clean up some includes in `z1/2/3.cpp` that has already
been included in `deepcompile.h`

Signed-off-by: Hollow Man <hollowman@opensuse.org>",['a755a9e52c973ced97a2a5c621500cb6dc99cb12'],False,"['z1.cpp', 'z2.cpp', 'z3.cpp', 'deepcompile.h']"
e6324af96b520663120b703b5bf889c07e046790,"fix(comm): Expose GradBucket in deepspeed.comm API (#7400)

This PR fixes an omission in the `deepspeed.comm` API where `GradBucket`
was not exposed, despite the package aiming for full compatibility with
`torch.distributed`.

##The Problem

As reported in issue #7393, when a user replaces `torch.distributed`
with `deepspeed.comm`, they expect all public APIs to be available.
However, attempting to access `deepspeed.comm.GradBucket` (for example,
when using it as a type hint for DDP communication hooks) results in an
`AttributeError`.

##The Solution

This change resolves the issue by importing `GradBucket` directly from
`torch.distributed` into the `deepspeed/comm/comm.py` file, making it
part of the public `deepspeed.comm` namespace.

A `# noqa: F401` comment has been added to the import line. This is
necessary to bypass the `flake8` linter's ""imported but unused"" check,
as the specific purpose of this import is to expose the symbol to the
end-user, not for it to be used within the `comm.py` file itself.

##How This Was Tested

The fix was verified with a local test script that confirms
`deepspeed.comm.GradBucket` can now be accessed correctly and is
identical to `torch.distributed.GradBucket`. The pre-commit hooks now
pass successfully.

##Related run test Screenshout 

<img width=""1250"" alt=""Screenshot 2025-06-30 at 22 41 10""
src=""https://github.com/user-attachments/assets/cadf18e1-9d1a-4164-a5ff-0b3e6804ac48""
/>

##Related Issue
Fixes #7393

Signed-off-by: Vensenmu <vensenmu@gmail.com>
Co-authored-by: Masahiro Tanaka <81312776+tohtana@users.noreply.github.com>",['384e6d34142f69283b84a714197b172a790f20a2'],False,['comm.py']
da60a878acadef477c8c3bb03c0e949ea6d4a08f,"fix: fix FileNotFoundError for build_win.bat (#7399)

fix FileNotFoundError for build_win.bat

`FileNotFoundError: [WinError 2] 系统找不到指定的文件。ERROR Backend subprocess
exited when trying to invoke get_requires_for_build_wheel`

Signed-off-by: gjj2828 <gjj2828@sina.com>
Co-authored-by: gjj2828 <gjj2828@gmail.com>",['e6324af96b520663120b703b5bf889c07e046790'],False,['setup.py']
15f054d97fc4261780821a916fd8e2da79c3a6f7,"fix: engine initializes optimizer attributes at the beginning (#7410)

As in `destroy`, `self.optimizer` is called, but the error out calling
to `destroy` can happen in `__init__`, even before optimizer and
scheduler is configured. So we need to move `self.optimizer` to the top
to avoid triggering another exception.

e.g.:
```logs
  File ""deepspeed/runtime/engine.py"", line 453, in _configure_tensor_parallel_states
    assert self.zero_optimization_stage(
AssertionError: Currently, the compatibility between 'autotp' and 'zero_stage = 3' has not been validated
Exception ignored in: <function DeepSpeedEngine.__del__ at 0x1516c0610820>
Traceback (most recent call last):
  File ""deepspeed/runtime/engine.py"", line 509, in __del__
    self.destroy()
  File ""deepspeed/runtime/engine.py"", line 512, in destroy
    if self.optimizer is not None and hasattr(self.optimizer, 'destroy'):
  File ""deepspeed/runtime/engine.py"", line 621, in __getattr__
    raise AttributeError(f""'{type(self).__name__}' object has no attribute '{name}'"")
AttributeError: 'DeepSpeedEngine' object has no attribute 'optimizer'
```

Signed-off-by: Hollow Man <hollowman@opensuse.org>",['da60a878acadef477c8c3bb03c0e949ea6d4a08f'],False,['engine.py']
2790220d31254c9c85da1dd6f48fe848ce94f1ac,"[TiledMLP]: fix for bs>1 (#7412)

It looks like my TiledMLP was working correctly only for batch_size=1

fixing to work with any bs 

thanks to @winglian for detecting the problem and sending me an easy
repro

---------

Signed-off-by: Stas Bekman <stas@stason.org>",['15f054d97fc4261780821a916fd8e2da79c3a6f7'],False,"['ulysses_sp.py', 'test_tiled_compute.py']"
8ace4da7c626145d0a0bd6c37c7d828ea7324d56,"Enable torch version dependent compilation of record_module and iter_params (#7362)

Dynamo breaks graphs because currently compile is disabled for a number
of functions such as `iter_params` and `record_module`.

The above functions compile successfully for at least PyTorch version
2.7.0.

We enable the compilation based on the user PyTorch version using a new
`compiler.enable(min_version=None)` decorator.
This should avoid the corresponding graph breaks and improve the
performance.

---------

Signed-off-by: Max Kovalenko <mkovalenko@habana.ai>
Co-authored-by: Masahiro Tanaka <81312776+tohtana@users.noreply.github.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['d6fe70ef88f7ef4dbbd4d1f205a9a85c376f4064'],False,"['compiler.py', 'partitioned_param_coordinator.py']"
27228462b0fbe2f38b2a3a3ff93388da93138b6f,"[BUGFIX] Reset `bucket.elements` after reduction in ZeRO Stage 3 (#7418)

closes #7415 

# Description
Resets `bucket.elements` after reduction in ZeRO Stage 3.
Without this, the bucket grows indefinitely, reducing only one param at
a time.
Added `bucket.elements = 0` after `params_in_bucket.clear()`.

Co-authored-by: a <a>",['8ace4da7c626145d0a0bd6c37c7d828ea7324d56'],False,['stage3.py']
affee605e47c9befd21c4c1445e11fd29d295201,"trying to fix nv-accelerate-v100.yml CI job (#7424)

trying a day old accelerate from the day before
https://github.com/huggingface/accelerate/commit/1ac8643df70273fcf2e2f0605a6bd8e64a4e49d7

---------

Signed-off-by: Stas Bekman <stas@stason.org>",['f485e1369ed7ccd89f3605b503803a8d006b8712'],False,['nv-accelerate-v100.yml']
a687d327eaf438c32a45d1fad3d677d1698ae02b,"fix: Propagate `strip_tensor_paddings` (#7426)

Trying to use the `DeepSpeed/deepspeed/checkpoints/ds_to_universal.py`,
I encountered:


```python
Traceback (most recent call last):
  File ""/opt/aurora/24.347.0/frameworks/aurora_nre_models_frameworks-2025.0.0/lib/python3.10/concurrent/futures/process.py"", line 246, in _process_worker
    r = call_item.fn(*call_item.args, **call_item.kwargs)
  File ""/lus/flare/projects/AuroraGPT/CPT-AuroraGPT-v0/foremans/projects/argonne-lcf/Megatron-DeepSpeed/deps/DeepSpeed/deepspeed/checkpoint/ds_to_universal.py"", line 114, in extract_zero_shards
    sd = ds_checkpoint.get_zero_checkpoint_state(pp_index=pp_index, tp_index=tp_index, dp_index=dp_index)
  File ""/lus/flare/projects/AuroraGPT/CPT-AuroraGPT-v0/foremans/projects/argonne-lcf/Megatron-DeepSpeed/venvs/aurora/aurora_nre_models_frameworks-2025.0.0/lib/python3.10/site-packages/deepspeed/checkpoint/deepspeed_checkpoint.py"", line 124, in get_zero_checkpoint_state
    return self.zero_checkpoint.get_state_for_rank(pp_index=pp_index,
  File ""/lus/flare/projects/AuroraGPT/CPT-AuroraGPT-v0/foremans/projects/argonne-lcf/Megatron-DeepSpeed/venvs/aurora/aurora_nre_models_frameworks-2025.0.0/lib/python3.10/site-packages/deepspeed/checkpoint/zero_checkpoint.py"", line 62, in get_state_for_rank
    self._strip_tensor_paddings(sd)
  File ""/lus/flare/projects/AuroraGPT/CPT-AuroraGPT-v0/foremans/projects/argonne-lcf/Megatron-DeepSpeed/venvs/aurora/aurora_nre_models_frameworks-2025.0.0/lib/python3.10/site-packages/deepspeed/checkpoint/zero_checkpoint.py"", line 110, in _strip_tensor_paddings
    group_state[state_name] = torch.narrow(state_value, 0, 0, raw_length).clone()
RuntimeError: narrow(): length must be non-negative.
```

(see full traceback[^traceback] below)


The issue is, there's no way to propagate the `strip_tensor_paddings`
argument from the
[`DeepSpeedCheckpoint.get_zero_checkpoint_state(...)`](https://github.com/deepspeedai/DeepSpeed/blob/affee605e47c9befd21c4c1445e11fd29d295201/deepspeed/checkpoint/deepspeed_checkpoint.py#L123)
method through to the [`ZeroCheckpoint.get_state_for_rank(...)`
method](https://github.com/deepspeedai/DeepSpeed/blob/affee605e47c9befd21c4c1445e11fd29d295201/deepspeed/checkpoint/zero_checkpoint.py#L53)
(which accepts it as an argument) since it doesn't accept it.

This PR adds this additional `strip_tensor_paddings` argument (default
`True`) in the `DeepSpeedCheckpoint.get_zero_checkpoint_state` method,
and passes it through to the
`self.zero_checkpoint.get_state_for_rank(...,
strip_tensor_paddings=strip_tensor_paddings)`, as shown below:

```diff
-    def get_zero_checkpoint_state(self, pp_index, tp_index, dp_index) -> dict:
+    def get_zero_checkpoint_state(self, pp_index, tp_index, dp_index, strip_tensor_paddings: bool = True) -> dict:
        return self.zero_checkpoint.get_state_for_rank(pp_index=pp_index,
                                                       tp_index=tp_index,
                                                       dp_index=dp_index,
-                                                       keys_to_ignore=[PARAM_SHAPES])
+                                                       keys_to_ignore=[PARAM_SHAPES],
+                                                       strip_tensor_paddings=strip_tensor_paddings)
```

[^traceback]: Full traceback:

	<details closed><summary>[Full Traceback]:</summary>
	
	```bash
#[🐍 aurora_nre_models_frameworks-2025.0.0](👻
aurora_nre_models_frameworks-2025.0.0)
	#[/f/A/C/f/p/a/Megatron-DeepSpeed][🌱 saforem2/fix-formatting][✓]
	#[07/12/25 @ 16:07:12][x4209c2s4b0n0]
;
ckpt_dir=checkpoints/ws768_ds_stage1_nl32_hs4096_mb1_seq4096_gb3072_sp1_pp1_tp1_bf16_optadamw_lr_lwf_flash
; gs=$(cat ""${ckpt_dir}/latest_checkpointed_iteration.txt"") && echo
""global step: ${gs}"" && python3
deps/DeepSpeed/deepspeed/checkpoint/ds_to_universal.py
--input_folder""${ckpt_dir}/global_step${gs}"" --output_folder
""${ckpt_dir}/global_step${gs}_universal"" --keep_temp_folder
	global step: 158945
[W712 16:07:17.966425018 OperatorEntry.cpp:155] Warning: Warning only
once for all operators, other operators may also be overridden.
Overriding a previously registered kernel for the same operator and the
same dispatch key
operator: aten::_cummax_helper(Tensor self, Tensor(a!) values,
Tensor(b!) indices, int dim) -> ()
registered at /build/pytorch/build/aten/src/ATen/RegisterSchema.cpp:6
	  dispatch key: XPU
previous kernel: registered at
/build/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30476
new kernel: registered at
/build/intel-pytorch-extension/build/Release/csrc/gpu/csrc/aten/generated/ATen/RegisterXPU.cpp:2971
(function operator())

/opt/aurora/24.347.0/frameworks/aurora_nre_models_frameworks-2025.0.0/lib/python3.10/site-packages/intel_extension_for_pytorch/nn/utils/_weight_prepack.py:6:
UserWarning: pkg_resources is deprecated as an API. See
https://setuptools.pypa.io/en/latest/pkg_resources.html. The
pkg_resources package is slated for removal as early as 2025-11-30.
Refrain from using this package or pin to Setuptools<81.
	  import pkg_resources
	AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'
	AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'
	AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'
	AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'
	AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'
[2025-07-12 16:07:27,740] [INFO]
[real_accelerator.py:254:get_accelerator] Setting ds_accelerator to xpu
(auto detect)
[2025-07-12 16:07:29,078] [INFO] [logging.py:107:log_dist] [Rank -1]
[TorchCheckpointEngine] Initialized with serialization = False
args =
Namespace(input_folder='checkpoints/ws768_ds_stage1_nl32_hs4096_mb1_seq4096_gb3072_sp1_pp1_tp1_bf16_optadamw_lr_lwf_flash/global_step158945',
output_folder='checkpoints/ws768_ds_stage1_nl32_hs4096_mb1_seq4096_gb3072_sp1_pp1_tp1_bf16_optadamw_lr_lwf_flash/global_step158945_universal',
num_extract_workers=4, num_merge_workers=2, keep_temp_folder=True,
strict=True, inject_missing_state=False)
	Convert DeepSpeed Checkpoint to Universal Checkpoint
Converting DeepSpeed checkpoint in
checkpoints/ws768_ds_stage1_nl32_hs4096_mb1_seq4096_gb3072_sp1_pp1_tp1_bf16_optadamw_lr_lwf_flash/global_step158945
to Universal checkpoint in
checkpoints/ws768_ds_stage1_nl32_hs4096_mb1_seq4096_gb3072_sp1_pp1_tp1_bf16_optadamw_lr_lwf_flash/global_step158945_universal

/lus/flare/projects/AuroraGPT/CPT-AuroraGPT-v0/foremans/projects/argonne-lcf/Megatron-DeepSpeed/megatron/core/tensor_parallel/layers.py:290:
FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated.
Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
	  def forward(

/lus/flare/projects/AuroraGPT/CPT-AuroraGPT-v0/foremans/projects/argonne-lcf/Megatron-DeepSpeed/megatron/core/tensor_parallel/layers.py:334:
FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated.
Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
	  def backward(ctx, grad_output):
[2025-07-12 16:07:39,134079][I][ezpz/__init__:264:ezpz] Setting logging
level to 'INFO' on 'RANK == 0'
[2025-07-12 16:07:39,136376][I][ezpz/__init__:265:ezpz] Setting logging
level to 'CRITICAL' on all others 'RANK != 0'
	*** 1. Extracting ZeRO fragments

100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋|
767/768 [01:29<00:00, 8.53it/s]
	concurrent.futures.process._RemoteTraceback:
	""""""
	Traceback (most recent call last):
File
""/opt/aurora/24.347.0/frameworks/aurora_nre_models_frameworks-2025.0.0/lib/python3.10/concurrent/futures/process.py"",
line 246, in _process_worker
	    r = call_item.fn(*call_item.args, **call_item.kwargs)
File
""/lus/flare/projects/AuroraGPT/CPT-AuroraGPT-v0/foremans/projects/argonne-lcf/Megatron-DeepSpeed/deps/DeepSpeed/deepspeed/checkpoint/ds_to_universal.py"",
line 114, in extract_zero_shards
sd = ds_checkpoint.get_zero_checkpoint_state(pp_index=pp_index,
tp_index=tp_index, dp_index=dp_index)
File
""/lus/flare/projects/AuroraGPT/CPT-AuroraGPT-v0/foremans/projects/argonne-lcf/Megatron-DeepSpeed/venvs/aurora/aurora_nre_models_frameworks-2025.0.0/lib/python3.10/site-packages/deepspeed/checkpoint/deepspeed_checkpoint.py"",
line 124, in get_zero_checkpoint_state
	    return self.zero_checkpoint.get_state_for_rank(pp_index=pp_index,
File
""/lus/flare/projects/AuroraGPT/CPT-AuroraGPT-v0/foremans/projects/argonne-lcf/Megatron-DeepSpeed/venvs/aurora/aurora_nre_models_frameworks-2025.0.0/lib/python3.10/site-packages/deepspeed/checkpoint/zero_checkpoint.py"",
line 62, in get_state_for_rank
	    self._strip_tensor_paddings(sd)
File
""/lus/flare/projects/AuroraGPT/CPT-AuroraGPT-v0/foremans/projects/argonne-lcf/Megatron-DeepSpeed/venvs/aurora/aurora_nre_models_frameworks-2025.0.0/lib/python3.10/site-packages/deepspeed/checkpoint/zero_checkpoint.py"",
line 110, in _strip_tensor_paddings
group_state[state_name] = torch.narrow(state_value, 0, 0,
raw_length).clone()
	RuntimeError: narrow(): length must be non-negative.
	""""""
	
	The above exception was the direct cause of the following exception:
	
	Traceback (most recent call last):
File
""/lus/flare/projects/AuroraGPT/CPT-AuroraGPT-v0/foremans/projects/argonne-lcf/Megatron-DeepSpeed/deps/DeepSpeed/deepspeed/checkpoint/ds_to_universal.py"",
line 549, in <module>
	    main(args)
File
""/lus/flare/projects/AuroraGPT/CPT-AuroraGPT-v0/foremans/projects/argonne-lcf/Megatron-DeepSpeed/deps/DeepSpeed/deepspeed/checkpoint/ds_to_universal.py"",
line 499, in main
	    _extract_zero_shard_files(args, ds_checkpoint, temp_dir)
File
""/lus/flare/projects/AuroraGPT/CPT-AuroraGPT-v0/foremans/projects/argonne-lcf/Megatron-DeepSpeed/deps/DeepSpeed/deepspeed/checkpoint/ds_to_universal.py"",
line 370, in _extract_zero_shard_files
_do_parallel_work(do_work, _3d_range_list, args.num_extract_workers)
File
""/lus/flare/projects/AuroraGPT/CPT-AuroraGPT-v0/foremans/projects/argonne-lcf/Megatron-DeepSpeed/deps/DeepSpeed/deepspeed/checkpoint/ds_to_universal.py"",
line 354, in _do_parallel_work
	    results.append(f.result())
File
""/opt/aurora/24.347.0/frameworks/aurora_nre_models_frameworks-2025.0.0/lib/python3.10/concurrent/futures/_base.py"",
line 451, in result
	    return self.__get_result()
File
""/opt/aurora/24.347.0/frameworks/aurora_nre_models_frameworks-2025.0.0/lib/python3.10/concurrent/futures/_base.py"",
line 403, in __get_result
	    raise self._exception
	RuntimeError: narrow(): length must be non-negative.
[1] 144664 exit 1 python3
deps/DeepSpeed/deepspeed/checkpoint/ds_to_universal.py --input_folder
	took: 0h:02m:08s
	```
	
	</details>

Signed-off-by: Sam Foreman <saforem2@gmail.com>",['affee605e47c9befd21c4c1445e11fd29d295201'],False,['deepspeed_checkpoint.py']
88ba24a3a6d22c88cb686fb632987fd02b5900b6,"Use past_key_value when provided (#7428)

The KV cache can be passed via `layer_past` or `past_key_value`
arguments. Previously, `past_key_value` was ignored, causing workload
incompatibilities.

This PR fixes the issue while preserving the original logic.

---------

Signed-off-by: Max Kovalenko <mkovalenko@habana.ai>",['a687d327eaf438c32a45d1fad3d677d1698ae02b'],False,['ds_transformer.py']
ee286e53c8c394680a94ad6e469f037515ef6098,"set `device_id` in torch's `init_process_group` (#7266)

This PR overcomes this issue when using any `torch.distributed` calls w/
deepspeed:
```
[W404 00:15:21.693690333 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 
to perform barrier as devices used by this process are currently unknown. This can
 potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in
 barrier() to force use of a particular device, or call init_process_group() with a device_id.
```
by setting `device_id` to the correct device corresponding to
`LOCAL_RANK` env var.

-------------------

Update: discovered `torch.dist` deadlocks with `torch=>2.7.0` when using
`device_id` arg - switching to draft for now as we can't commit this
until we know how to work around this.

---------

Signed-off-by: Stas Bekman <stas@stason.org>
Signed-off-by: Stas Bekman <stas.bekman@snowflake.com>
Co-authored-by: Olatunji Ruwase <tunji.ruwase@snowflake.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Stas Bekman <stas.bekman@snowflake.com>",['88ba24a3a6d22c88cb686fb632987fd02b5900b6'],False,"['cuda_accelerator.py', 'torch.py']"
c2bb53f20fa32d6cbf472c08a42959a287dd9049,"TiledMLP + SequenceTiledCompute: improve the bs>1 use-case (#7422)

Improved TiledMLP and SequenceTiledCompute for bs>1

This PR:
- extends the testing utils to add `CaptureStd*`, `CaptureLogger`
context managers
- extends the test to run both bs=1 and bs=2
- use an uneven seqlen to test varlen shards
- flattens bs+seqlen dim, to avoid problems with grad tensor strides
when bs>1 - mlp doesn't care for the bs dimension so using a pretend
`bs*seqlen` seqlen instead and restoring the shape at the end for the
grad.

---------

Signed-off-by: Stas Bekman <stas@stason.org>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['d33b5623f9d4fcd38ecd7cdcf774b167df584c7b'],False,"['ulysses_sp.py', 'test_tiled_compute.py', 'util.py']"
1d10d482917f1a8b21fb1d20c534f0aafb7cd39d,"[ALST] fix typo in the url (#7444)

fixing the misspelled url

---------

Signed-off-by: Stas Bekman <stas@stason.org>",['3bf53451e59e120d95e457a4c0b7ecdef99351c6'],False,"['ulysses_sp.py', 'navigation.yml', 'ds-sequence.md']"
70caefe3c1833fce5ef881024587fd274c265ddb,"[ALST] fix typo in the url part2 (#7446)

oops, forgot to rename the file itself :( continuation of
https://github.com/deepspeedai/DeepSpeed/pull/7444

---------

Signed-off-by: Stas Bekman <stas@stason.org>",['1d10d482917f1a8b21fb1d20c534f0aafb7cd39d'],False,['ulysses-alst-sequence-parallelism.md']
43f00ba31cabc76dfe1d4faf652a8fbf4587cd78,Remove additional unused tests (human-eval) (#7445),['70caefe3c1833fce5ef881024587fd274c265ddb'],False,"['nv-human-eval.yml', 'test_human_eval.py']"
092625c7eb24a76779167fb74c29dc347653248f,"Fix: Adapt Llama injection policy for newer transformers versions (#7443)

This PR fixes an `AttributeError` that occurs during
`deepspeed.init_inference` when using kernel injection
(`replace_with_kernel_inject=True`) with Llama models from recent
versions of `transformers`.

**The Bug:**

In newer `transformers` versions (e.g., `4.53.3`), configurations like
`num_heads` and `rope_theta` were moved from direct attributes of the
`LlamaAttention` module into a nested `config` object.

The current DeepSpeed injection policy tries to access these attributes
from their old, direct location, causing the initialization to fail with
an `AttributeError: 'LlamaAttention' object has no attribute
'num_heads'`.

**The Solution:**

This change updates the Llama injection logic to be more robust:
1. It first tries to read attributes like `num_heads` from the new
`config` object location.
2. If that fails, it falls back to the legacy direct attribute path.

---------

Signed-off-by: huanyuqu <yc37960@um.edu.mo>",['43f00ba31cabc76dfe1d4faf652a8fbf4587cd78'],False,"['llama.py', 'test_inference.py']"
56fed13a1a7d40b402a7495ff47854302348aeab,"Fix: UnboundLocalError for variable 'dim' about issue (#7449)

## Fix `UnboundLocalError` in `ZeroLinear.backward()` when training only
bias parameters, as mentioned in #7435

This PR addresses an issue in the `ZeroLinear.backward()` method, where
the local variable `dim` could be referenced before assignment. This
happens specifically when:

- Only the bias parameters are set to `requires_grad=True`, and
- The training setup uses **ZeRO Stage 3**, **AMP**, and **gradient
checkpointing**.

###  Problem

When only the bias requires gradients, the condition for setting `dim =
grad_output.dim()` is skipped, but the value of `dim` is still used
later in the computation, leading to:


###  Fix

Move the assignment `dim = grad_output.dim()` to occur unconditionally,
so that `dim` is always defined before being used in any branch of the
gradient computation logic.

###  Impact

This makes the backward pass more robust across different training
setups.

Signed-off-by: weeknan <zhounan0431@163.com>
Co-authored-by: Olatunji Ruwase <tjruwase@gmail.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['b8668fb96ce34cf7940b2ffd6c1e4076e4882ef1'],False,['linear.py']
3292e07a92486c06123913ebc6d094a7c4cbcc35,"adding TiledFusedLogitsLoss (#7437)

This PR adds `TiledFusedLogitsLoss` for an efficient fused logits+loss
computation - this version pre-calculates grads in `forward`, avoiding
recomputation in the backward (similar to the Liger-Kernel
implementation).

---------

Signed-off-by: Stas Bekman <stas@stason.org>
Co-authored-by: Aurick Qiao <aurick.qiao@snowflake.com>",['56fed13a1a7d40b402a7495ff47854302348aeab'],False,"['ulysses_sp.py', 'test_tiled_compute.py']"
c4b1a8cb8f0ab93c1c81660c668b35d720bddcee,"`TiledFusedLogitsLoss` bug fix (#7459)

bug fix - mixed up tuple and list.",['3292e07a92486c06123913ebc6d094a7c4cbcc35'],False,['ulysses_sp.py']
0e51e0939605669f744bd639ca1b26b0bfbd80da,"Add getter APIs for TP/PP/DP ranks in DeepSpeedEngine (#7427)

Thanks again for giving opportunity for improving this Community!
This PR is from Issue #7423.

1) Motivation

To improve compatibility with low-level profiling tools (e.g., NVIDIA
CUPTI or DCGM), it can be useful to expose parallelism-specific rank
(tensor/pipeline/data) at the engine level.

2) Changes

I Added three getter methods to DeepSpeedEngine:
  - get_tensor_parallel_rank()
  - get_pipeline_parallel_rank()
  - get_data_parallel_rank()


Thank you for reviewing this contribution!

---------

Signed-off-by: WoosungMyung <dntjd517@naver.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['e1560d8499372bdc67feab56bf175201baf6e6f9'],False,"['engine.py', 'engine.py']"
1a8ad24f0d88b0a0f5f43e5b5d490e66a78ab220,"fix issues raised by Coverity scans (#7431)

This commit combines fixes for 37 potential code issues found in
Coverity scans.
the issues include but are not limited to potential access to
uninitialized variables, dead and redundant code.
We understand that reviewing such a commit can be difficult and will be
happy to help with any questions or changes required.

---------

Signed-off-by: Nir Sonnenschein <nsonnenschein@habana.ai>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Olatunji Ruwase <tunji.ruwase@snowflake.com>",['0e51e0939605669f744bd639ca1b26b0bfbd80da'],False,"['real_accelerator.py', 'autotuner.py', 'constants.py', 'ccl.py', 'offload_activation.py', 'helper.py', 'qkv.py', 'sequence_descriptor.py', 'megatron_gpt.py', 'replace_module.py', 'sharded_moe.py', 'matmul.py', 'diffusers_transformer_block.py', 'transformer.py', 'data_analyzer.py', 'engine.py', 'engine.py', 'sparse_tensor.py', 'stage3.py', 'stage_1_and_2.py', 'fpdt_layer.py', 'groups.py', 'builder.py', 'comm.py']"
0aff6b2c20f8f68149cea848eb2c6928f2c5710f,"Fix all-gather duplicate params and wrong dtype (#7462)

The following assertion error arises when torch autocast is enabled.

[rank3]: File
""/opt/deepspeed/deepspeed/runtime/zero/partitioned_param_coordinator.py"",
line 337, in fetch_sub_module
[rank3]:
self.__inflight_param_registry.pop(param).wait(handle_dependency=not
fast_fetch)
[rank3]: File
""/opt/deepspeed/deepspeed/runtime/zero/partition_parameters.py"", line
787, in wait
[rank3]:     handle.wait(handle_dependency)
[rank3]: File ""/opt/deepspeed/deepspeed/utils/nvtx.py"", line 20, in
wrapped_fn
[rank3]:     ret_val = func(*args, **kwargs)
[rank3]: File
""/opt/deepspeed/deepspeed/runtime/zero/partition_parameters.py"", line
750, in wait
[rank3]: assert param.ds_status == ZeroParamStatus.INFLIGHT, f""expected
param {param.ds_summary()} to be inflight""
[rank3]: AssertionError: expected param {'id': 685, 'status':
'AVAILABLE', 'numel': 131334144, 'ds_numel': 131334144, 'shape': (32064,
4096), 'ds_shape': (32064, 4096), 'requires_grad': True, 'grad_shape':
None, 'persist': False, 'active_sub_modules': set(), 'ds_tensor.shape':
torch.Size([16416768])} to be inflight

This is due to multiple all-gather ops in the same coalesced all-gather
sharing the same list of params (of mixed dtypes).

Make each all-gather exchange only params of a certain dtype. Also pass
the allgather dtype that matches the params.

Signed-off-by: Junjie Mao <banxing.mjj@alibaba-inc.com>
Co-authored-by: Junjie Mao <banxing.mjj@alibaba-inc.com>
Co-authored-by: Olatunji Ruwase <tjruwase@gmail.com>",['1a8ad24f0d88b0a0f5f43e5b5d490e66a78ab220'],False,['partition_parameters.py']
f897b67394827e2bc18a354603470d45b7e687ae,"fix #7188  (#7371)

I found that when using DeepSpeed Zero2 for my training task, the loss
becomes 0 at the third step with a grad_norm of 1.414. This issue
doesn't occur when using Zero3. I found the same issue #7188. After
conducting a series of experiments, I identified the cause: there's a
synchronization problem when using double ipg_buffer swapping. The issue
was resolved after making modifications.

before 

![image](https://github.com/user-attachments/assets/981d0829-e15f-4899-ae2c-4eca16ef138d)

after

![image](https://github.com/user-attachments/assets/8b6b8403-d5df-4aa8-b573-195b9ee1fdfb)

Signed-off-by: vinceliu <lpnpcs@gmail.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Olatunji Ruwase <tunji.ruwase@snowflake.com>
Co-authored-by: Hongwei Chen <33092912+hwchen2017@users.noreply.github.com>",['0aff6b2c20f8f68149cea848eb2c6928f2c5710f'],False,['stage_1_and_2.py']
8c83e42ba1abf466662f1b111caa517e9a0da4eb,"Fix cpu CI (#7481)

Fix torch version

---------

Signed-off-by: Olatunji Ruwase <tunji.ruwase@snowflake.com>",['cda3f9628cfe1dd90c7af10ddc5b8d3b16aa459c'],False,['cpu-torch-latest.yml']
8e029923328861fbf599a07e2aa5c335173e2299,"fix `deepspeed --venv_script` (#7469)

currently passing `deepspeed ... --venv_script foo.sh` ends up with a
pdsh cmd like:

```
pdsh -S -f 1024 -w 10.4.11.15,10.4.10.1 source foo.sh export NCCL_NET_PLUGIN=blah; ...
```
you can see, `;` is missing before exports start, so the first export is
ignored.

It should be:


```
pdsh -S -f 1024 -w 10.4.11.15,10.4.10.1 source foo.sh; export NCCL_NET_PLUGIN=blah; ...
```

This PR is fixing it.",['8c83e42ba1abf466662f1b111caa517e9a0da4eb'],False,['multinode_runner.py']
a12de38db645ee0ab6298e03823eace5cbc9bd01,"Modal CI (#7289)

This is an initial effort to migrate CI unto Modal infra. This PR
creates two new workflows that run on Modal
1. modal-torch-latest: a subset of nv-torch-latest-v100 that includes
`tests/unit/runtime/zero/test_zero.py`.
2. modal-accelerate: a full copy of nv-accelerate-v100. 

Follow up PRs will selectively migrate relevant workflows onto Modal.

---------

Signed-off-by: Olatunji Ruwase <tunji.ruwase@snowflake.com>
Signed-off-by: Olatunji Ruwase <tjruwase@gmail.com>
Signed-off-by: Tunji Ruwase <tunji.ruwase@snowflake.com>
Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>
Co-authored-by: Logan Adams <loadams@microsoft.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Olatunji Ruwase <tjruwase@gmail.com>
Co-authored-by: Stas Bekman <stas.bekman@snowflake.com>",['8e029923328861fbf599a07e2aa5c335173e2299'],False,"['modal-accelerate.yml', 'modal-torch-latest.yml', '__init__.py', 'accelerate.py', 'torch_latest.py', 'test_zero.py']"
d75196a09871f607c836f346ac72aecbc93cf520,"[UlyssesSPDataLoaderAdapter] fix iterator reset (#7472)

Fixes https://github.com/snowflakedb/ArcticTraining/issues/254 - to
support multi-epoch training with `UlyssesSPDataLoaderAdapter`.

Thanks to @yanrui27 for the fix

Signed-off-by: Stas Bekman <stas@stason.org>
Co-authored-by: Rui Yan <49115835+yanrui27@users.noreply.github.com>",['a12de38db645ee0ab6298e03823eace5cbc9bd01'],False,['ulysses_sp.py']
8aadf6cbe42f9249c51f691e441a37ed5bfb7aa5,"Fix pre-compile on cpu-only machines (#7168)

+ Fix pre-compile on cpu-only machines

---------

Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Olatunji Ruwase <tunji.ruwase@snowflake.com>",['a54c394392ceeb2909f80ab09aa88ac13e4b8c40'],False,['cuda_accelerator.py']
64ac13f72e2e00ec305bae6615cb402bf155d2e4,"Enable forked PRs (#7486)

Enable forked PRs

---------

Signed-off-by: Olatunji Ruwase <tunji.ruwase@snowflake.com>",['8aadf6cbe42f9249c51f691e441a37ed5bfb7aa5'],False,"['modal-accelerate.yml', 'modal-torch-latest.yml']"
33cd94500e4cf7a495e915eda69d28d75737fa0d,"fix xpu device_id AttributeError issue (#7488)

# Reproduce
w/ PyTorch 2.8
```
$ git clone https://github.com/huggingface/trl.git
$ cd ./trl
$ accelerate launch     --config_file examples/accelerate_configs/deepspeed_zero3.yaml     examples/scripts/sft_gpt_oss.py     --torch_dtype bfloat16     --model_name_or_path openai/gpt-oss-20b     --packing true packing_strategy wrapped     --run_name 20b-full-eager     --attn_implementation sdpa     --dataset_num_proc 6     --dataset_name HuggingFaceH4/Multilingual-Thinking     --gradient_checkpointing     --max_length 4096     --per_device_train_batch_size 1     --num_train_epochs 1     --logging_steps 1     --warmup_ratio 0.03     --lr_scheduler_type cosine_with_min_lr     --lr_scheduler_kwargs '{""min_lr_rate"": 0.1}'     --output_dir gpt-oss-20b-multilingual-reasoner     --report_to trackio     --seed 42
```

# Issue

> File ""/workspace/accelerate/src/accelerate/state.py"", line 216, in
__init__
> dist.init_distributed(dist_backend=self.backend,
auto_mpi_discovery=False, **kwargs)
> File ""/usr/local/lib/python3.12/dist-packages/deepspeed/comm/comm.py"",
line 854, in init_distributed
> cdb = TorchBackend(dist_backend, timeout, init_method, rank,
world_size)
> ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
> File
""/usr/local/lib/python3.12/dist-packages/deepspeed/comm/torch.py"", line
120, in __init__
> self.init_process_group(backend, timeout, init_method, rank,
world_size)
> File
""/usr/local/lib/python3.12/dist-packages/deepspeed/comm/torch.py"", line
164, in init_process_group
>     torch.distributed.init_process_group(backend, **kwargs)
> File
""/usr/local/lib/python3.12/dist-packages/torch/distributed/c10d_logger.py"",
line 81, in wrapper
>     return func(*args, **kwargs)
>            ^^^^^^^^^^^^^^^^^^^^^
> File
""/usr/local/lib/python3.12/dist-packages/torch/distributed/c10d_logger.py"",
line 95, in wrapper
>     func_return = func(*args, **kwargs)
>                   ^^^^^^^^^^^^^^^^^^^^^
> File
""/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py"",
line 1685, in init_process_group
>     if device_id is not None and device_id.type != ""cpu"":
> AttributeError: 'device' object has no attribute 'type'

# Root Cause
`torch.xpu.device` in PyTorch is a context manager in PyTorch rather
than a device class, it doesn't have attribute `type`

# Fix
switch to use `torch.device`

Signed-off-by: Yao, Matrix <matrix.yao@intel.com>
Co-authored-by: Olatunji Ruwase <tunji.ruwase@snowflake.com>",['64ac13f72e2e00ec305bae6615cb402bf155d2e4'],False,['xpu_accelerator.py']
1d7b90adc48d57c2283e8825f5c668a3730ff899,"Add Zenflow code for Stage 1 & 2 (#7391)

This PR adds ZenFlow, a importance-aware offloaded training framework
for DeepSpeed ZeRO. ZenFlow enables multi-step overlap between
computation and communication during offloaded training, improving GPU
utilization and reducing stalls.

Highlights:
- New ZenFlow optimizers (ZenFlowCPUAdam, ZenFlowSelectiveAdamW)
- ZenFlowZeroOptimizer for ZeRO Stage 1/2 integration
- Configurable via ZenFlowConfig, integrated with DeepSpeedZeroConfig
- Unit tests and documentation included

Note: This PR focuses on Stage 1 and 2 integration. Stage 3 support will
be introduced in a follow-up PR.

---------

Signed-off-by: Tingfeng Lan <erc8gx@virginia.edu>
Signed-off-by: Yusen Wu <xrn4ub@virginia.edu>
Signed-off-by: Olatunji Ruwase <tunji.ruwase@snowflake.com>
Co-authored-by: Yusen Wu <xrn4ub@virginia.edu>
Co-authored-by: Masahiro Tanaka <81312776+tohtana@users.noreply.github.com>
Co-authored-by: Olatunji Ruwase <tunji.ruwase@snowflake.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Olatunji Ruwase <tjruwase@gmail.com>
Co-authored-by: Guokai Ma <guokai.ma@gmail.com>",['33cd94500e4cf7a495e915eda69d28d75737fa0d'],False,"['__init__.py', 'zenflow_cpu_adam.py', 'zenflow_torch_adam.py', 'engine.py', '__init__.py', 'engine.py', 'zenflow_config.py', 'zenflow_stage_1_and_2.py', 'zenflow_utils.py', 'config.py', 'stage_1_and_2.py', 'zenflow.md', 'test_zf_torch_adam.py', 'test_zf.py', 'test_zf_config.py']"
1c03d1b1bb9228739ac879213098b10fa29fc00e,"Fix invalid f-strings (#7457)

Fix invalid f-strings detected by ruff.

---------

Signed-off-by: cyy <cyyever@outlook.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Olatunji Ruwase <tunji.ruwase@snowflake.com>
Co-authored-by: Michael Wyatt <michael.wyatt@snowflake.com>",['1d7b90adc48d57c2283e8825f5c668a3730ff899'],False,"['hpu_accelerator.py', 'real_accelerator.py', 'ds_aio_args.py', 'ds_aio_basic.py', 'ds_aio_handle.py', 'io_engine.py', 'test_ds_aio.py', 'torch_fastio_engine.py', 'torch_io.py', 'autotuner.py', 'deepspeed_checkpoint.py', 'ds_to_universal.py', 'reshape_meg_2d.py', 'fx.py', 'offload_activation.py', 'offload_parameters.py', 'config.py', 'env_report.py', 'launch.py', 'launcher_helper.py', 'quantization.py', 'ds_transformer.py', 'fusedqkv_utils.py', 'replace_module.py', 'sharded_moe.py', 'ds_aio_args.py', 'ds_aio_basic.py', 'ds_aio_handle.py', 'io_engine.py', 'parse_nvme_stats.py', 'test_ds_aio.py', 'torch_fastio_engine.py', 'torch_io.py', 'quantize.py', 'sparsity_config.py', 'attention.py', 'checkpointing.py', 'bf16_optimizer.py', 'decoupled_checkpoint_engine.py', 'nebula_checkpoint_engine.py', 'config_utils.py', 'curriculum_scheduler.py', 'eigenvalue.py', 'engine.py', 'loss_scaler.py', 'lr_schedules.py', 'data_parallel_writer_factory.py', 'engine.py', 'ulysses_sp.py', 'optimizer_utils.py', 'partitioned_param_swapper.py', 'pipelined_optimizer_swapper.py', 'utils.py', 'utils.py', 'contiguous_memory_allocator.py', 'mics.py', 'partition_parameters.py', 'stage3.py', 'stage_1_and_2.py', 'tensor_fragment.py', 'timer.py', 'zero_to_fp32.py', 'fp_quantizer.py', 'sparse_attn.py', 'DS4Sci_EvoformerAttention_bench.py', 'stage3_test.py', 'test_autotp_training.py', 'test_moe.py', 'test_onebit.py', 'test_data_efficiency.py', 'test_multi_output_model.py', 'test_offload_states.py', 'test_zero_leaf_module.py']"
12b4dc19a731d61536278c5cc1501196edcd21c6,"Fix DeepCompile for PyTorch v2.8 (#7496)

This PR updates the kernel generation function arguments in Inductor to
ensure DeepCompile is compatible with PyTorch v2.8.
It also fixes the logging output of DeepCompile.",['1c03d1b1bb9228739ac879213098b10fa29fc00e'],False,"['backend.py', 'inductor.py']"
8cf5fc57874da1fe7324755190b777493e5c6bb4,"Reduce performance impact of compiler.enable decorator (#7498)

For some accelerators (such as HPU) running in a non-compile scenarios,
the `compiler.enable` decorator can cause significant performance drops
up to 8-12%.

We can easily avoid the performance hit in non-compile scenarios, by
detecting the ongoing compilation and returning immediately.

Signed-off-by: Max Kovalenko <mkovalenko@habana.ai>
Co-authored-by: Olatunji Ruwase <tunji.ruwase@snowflake.com>",['12b4dc19a731d61536278c5cc1501196edcd21c6'],False,['compiler.py']
bc8c0db3b46b939cd3e96b0673d5964a676e4c52,"Support DeepSpeed offload and reload states with ZeRO1 and ZeRO2 (#7421)

Please refer to https://github.com/deepspeedai/DeepSpeed/issues/7251

---------

Signed-off-by: lym <letusgo126@126.com>
Signed-off-by: Max Kovalenko <mkovalenko@habana.ai>
Signed-off-by: Alex Kiefer <alexkiefer51@gmail.com>
Signed-off-by: Stas Bekman <stas@stason.org>
Signed-off-by: Sam Foreman <saforem2@gmail.com>
Signed-off-by: Stas Bekman <stas.bekman@snowflake.com>
Signed-off-by: huanyuqu <yc37960@um.edu.mo>
Signed-off-by: weeknan <zhounan0431@163.com>
Signed-off-by: WoosungMyung <dntjd517@naver.com>
Signed-off-by: Nir Sonnenschein <nsonnenschein@habana.ai>
Signed-off-by: Junjie Mao <banxing.mjj@alibaba-inc.com>
Signed-off-by: vinceliu <lpnpcs@gmail.com>
Signed-off-by: Tingfeng Lan <erc8gx@virginia.edu>
Signed-off-by: Olatunji Ruwase <tunji.ruwase@snowflake.com>
Signed-off-by: Olatunji Ruwase <tjruwase@gmail.com>
Signed-off-by: Tunji Ruwase <tunji.ruwase@snowflake.com>
Signed-off-by: Yao, Matrix <matrix.yao@intel.com>
Signed-off-by: Yusen Wu <xrn4ub@virginia.edu>
Signed-off-by: cyy <cyyever@outlook.com>
Co-authored-by: Max Kovalenko <mkovalenko@habana.ai>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Masahiro Tanaka <81312776+tohtana@users.noreply.github.com>
Co-authored-by: Alexander Kiefer <56556451+alexk101@users.noreply.github.com>
Co-authored-by: Olatunji Ruwase <tunji.ruwase@snowflake.com>
Co-authored-by: Hongwei Chen <33092912+hwchen2017@users.noreply.github.com>
Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>
Co-authored-by: Sam Foreman <saforem2@gmail.com>
Co-authored-by: Stas Bekman <stas.bekman@snowflake.com>
Co-authored-by: huanyuqu <55744355+huanyuqu@users.noreply.github.com>
Co-authored-by: weeknan <57584045+weeknan@users.noreply.github.com>
Co-authored-by: Olatunji Ruwase <tjruwase@gmail.com>
Co-authored-by: Aurick Qiao <aurick.qiao@snowflake.com>
Co-authored-by: Logan Adams <loadams@microsoft.com>
Co-authored-by: Zhipeng Wang <zhipeng.rainbowserie@gmail.com>
Co-authored-by: WoosungMyung <115716986+WoosungMyung@users.noreply.github.com>
Co-authored-by: Nir Sonnenschein <nsonnenschein@habana.ai>
Co-authored-by: Junjie Mao <junjie.mao@hotmail.com>
Co-authored-by: Junjie Mao <banxing.mjj@alibaba-inc.com>
Co-authored-by: lpnpcs <lpnpcs@vip.qq.com>
Co-authored-by: Ma, Guokai <guokai.ma@gmail.com>
Co-authored-by: Tingfeng Lan <tafflann@outlook.com>
Co-authored-by: Rui Yan <49115835+yanrui27@users.noreply.github.com>
Co-authored-by: Feng Yunlong <20281571+AlongWY@users.noreply.github.com>
Co-authored-by: Yao Matrix <matrix.yao@intel.com>
Co-authored-by: Tingfeng Lan <erc8gx@virginia.edu>
Co-authored-by: Yusen Wu <xrn4ub@virginia.edu>
Co-authored-by: Yuanyuan Chen <cyyever@outlook.com>
Co-authored-by: Michael Wyatt <michael.wyatt@snowflake.com>",['f45159e4159e88f11e09fccdb95d884e3b0b57cc'],False,"['engine.py', 'offload_states.py', 'stage_1_and_2.py', 'test_offload_states.py']"
38d1a9eb64c9e01e32eccc50b25ba18925287441,"Fix assert when 'pp_int' object has no attribute 'custom_print_str' (#7507)

Fix assert `'pp_int' object has no attribute 'custom_print_str'` when
tracking deepspeed module with some track debug tools like
[objwatch](https://github.com/aeeeeeep/objwatch)

```python3
    import objwatch
    objwatch.watch(targets=[deepspeed], framework=""torch.distributed"", indexes=[0,], with_locals=True)
```

Signed-off-by: aeeeeeep <aeeeeeep@proton.me>",['d9cb78683e38ee0858ad133b572dfa3d8952ac69'],False,['config_utils.py']
66ad2780489744e932baa8f8592f3aa35fa145cb,"Enabling Muon Optimizer in DeepSpeed (#7509)

Authorship: @pengdurice and @PKUWZP 

Related Issue: #7438

# Introduction

[Muon](https://arxiv.org/abs/2502.16982), a new optimizer that has
attracted the community’s attention recently shows promising results in
training large language models. Adding the Muon Optimizer to DeepSpeed,
a popular OSS framework for large scale training and inference is
critically important for DeepSpeed users and developers. There has been
a [PR](https://github.com/deepspeedai/DeepSpeed/pull/7454) attempting
the adoption. (Huge Thanks to @qimcis), which is a good starting point.
It still requires more substantial effort to make it fully compatible
and work within DeepSpeed. We are publishing this PR to fully enable
Muon Optimizer capabilities for DeepSpeed.

# Issues and solutions
## Issues
1. With stage 1, 2 or 3, the optimizer states will be partitioned within
the same data parallel group. This means that each process is already
handling only parts of the model parameters and there is no need to use
the DP solution as in the
[code](https://github.com/KellerJordan/Muon/blob/master/muon.py#L195).
2. The parameters (and the gradients) will be flattened to 1D vector
before being used in the optimizer, thus nullifying the major hypothesis
of the muon optimizer: it works by orthogonalizing the updates for each
matrix (dim >=2)

## Solutions
To solve the issues, we propose this new PR in which: 
1. We simplify the Muon code by
[removing](https://github.com/deepspeedai/DeepSpeed/compare/master...pengdurice:DeepSpeed:peng-add-muon-v1#diff-c9052994e41caee9ca88363749c10af08655f8019f08dc971c018663d25a3712R22)
the partitioning and muon updates logics.

2. We
[move](https://github.com/deepspeedai/DeepSpeed/compare/master...pengdurice:DeepSpeed:peng-add-muon-v1#diff-99dcf26ea2876ff5bbf05b5165c4133eaa0d0f36b170685643c2f7e2eb566addR1867)
the muon update to the
[get_flat_partition](https://github.com/deepspeedai/DeepSpeed/compare/master...pengdurice:DeepSpeed:peng-add-muon-v1#diff-99dcf26ea2876ff5bbf05b5165c4133eaa0d0f36b170685643c2f7e2eb566addR1848)
function of stage 1 and 2 DeepSpeedZeroOptimizer in which per parameter
gradients are collected before being flattened and used by the optimizer
to update the model parameters. Since each parameter is still in its
original shape, we can easily apply the muon updates.
3. We also save the momentum buffer into the optimizer’ state so that we
have a smooth convergence after applying the saved checkpoints.
4. We added comprehensive unit tests to validate Muon Optimizer's
correctness and functionality.

# Future directions and roadmap
In the future, several follow up works are of interests:
- [ ] Create a CPU offload version.
- [ ] Apply Muon to Stage 3
- [ ] Use the highly optimized version of Adam for the Adam part of
MuonWithAuxAdam optimizer.
- [ ] More efficient implementations e.g. a) add specialized kernels for
Newton-Schulz iteration and muon updates; b) parallelize updates for the
parameters (currently, each parameter is updated separately and
sequentially)

---------

Co-authored-by: Peng Du <pedu@linkedin.com>
Co-authored-by: pengdurice <pengduhit@gmail.com>
Co-authored-by: Zhipeng Wang <zhipengbayern@gmail.com>
Co-authored-by: Olatunji Ruwase <tunji.ruwase@snowflake.com>",['e4662faffd6803939c97d3d818e4318655259797'],False,"['config.py', 'engine.py', '__init__.py', 'muon_optimizer.py', 'original_muon.py', 'stage_1_and_2.py', 'utils.py', 'test_muon.py']"
889f0ead27435cd7755a73a9ba27e0913ab3c548,"Enable non-ZeRO mode (#7515)

Enabled via `stage=0` which corresponds to DDP. 
Remove hardwired path to b16_optimizer.
Enable`torch.autocast` for DDP training
Enable native mixed precision DDP for bfloat16
Update torch.autocast and native mixed precision UTs

<img width=""976"" height=""184"" alt=""image""
src=""https://github.com/user-attachments/assets/92904cdc-e312-46a4-943f-011eb5ab146a""
/>

---------

Signed-off-by: Olatunji Ruwase <tunji.ruwase@snowflake.com>
Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>",['66ad2780489744e932baa8f8592f3aa35fa145cb'],False,"['torch_latest.py', 'constants.py', 'engine.py', 'fused_optimizer.py', 'training.rst', 'test_ds_initialize.py', 'test_zero_autocast.py']"
eabb687ac100c7aad0ce5cadd2d8d16f52f7c1d8,"ZeRO3: Improve mismatch detection (#7525)

ZeRO3 tracks DDP (SPMD) behavior by matching values different training
states across ranks. Some of these states are represented as lists, and
mismatches sometimes manifests as hangs during error detection. This PR
improves error detection by first validating the list lengths across
ranks before validating the list contents.

Motivated by
https://github.com/deepspeedai/DeepSpeed/issues/7461#issuecomment-3235146207

---------

Signed-off-by: Olatunji Ruwase <tunji.ruwase@snowflake.com>
Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>",['9bf215d21331ec92e8f761b3f1a62364c3c8fc82'],False,"['engine.py', 'config.py', 'stage3.py', 'utils.py']"
902e78c989383dac09d47325f39b5c83d5e7f889,"fix typo s/1014 /1024 (#7528)

fix typo s/1014 /1024  
         s/was_interruptted /was_interrupted

detail info 
        modified:   deepspeed/autotuning/scheduler.py
        modified:   deepspeed/autotuning/utils.py

Co-authored-by: Olatunji Ruwase <tunji.ruwase@snowflake.com>",['eabb687ac100c7aad0ce5cadd2d8d16f52f7c1d8'],False,"['scheduler.py', 'utils.py']"
066d912052b5eaf6094a2e57d20a163ba6517db8,"[logging] less startup noise (#7526)

This PR removes some and enables removing other startup noise -
especially when it's replicated rank-times and doesn't carry any
informative payload.

1. add `--log_level` flag which sets the launcher's logger to a desired
setting - defaulting to `logging.INFO` for now for BC, but will change
to `logging.WARNING` in v1
2. add `--quiet/-q` flag which sets the launcher's logger to
`logging.ERROR` which essentially disables startup info messages
3. change the logging defaults elsewhere to `logging.WARNING` (main
impact is the accelerator.py), once deepspeed started the frameworks
control its loglevel for each rank, so the tricky part is this pre-start
stage info logs. this part is breaking BC as there is no machinery to
set the logger level for `real_accelerator.py`)
4. builder is changed to non-verbose (BC breaking)

---------

Signed-off-by: Stas Bekman <stas@stason.org>
Co-authored-by: Olatunji Ruwase <tunji.ruwase@snowflake.com>",['411e20a3f799fac97e4f953cba0eb089d8a7b58c'],False,"['launch.py', 'runner.py', '__init__.py', 'logging.py', 'builder.py']"
9e4957eb30fcd0f53214727732e878cabab366c9,"[doc] fixing moe tutorial (#7538)

MoE tutorial fixes:
1. cifar example has been moved - fix the url
2. fixing text and improving markup

---------

Signed-off-by: Stas Bekman <stas@stason.org>",['066d912052b5eaf6094a2e57d20a163ba6517db8'],False,['mixture-of-experts.md']
c07b3abf9ac2637f5106f7d3991bb785e6ff38ef,"fixed DeepSpeedCPULion with ZeRO-Offload bug (#7531)

fixed DeepSpeedCPULion with ZeRO-Offload bug
[issues/7524](https://github.com/deepspeedai/DeepSpeed/issues/7524)

Signed-off-by: Qi Bin <qibin0506@users.noreply.github.com>
Co-authored-by: Olatunji Ruwase <tunji.ruwase@snowflake.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>",['4d83f3fe13ba9bbd8a0e55b4b6ebde256d60e37d'],False,['engine.py']
1e183a6a9d1b47aace14051e9b2aa0b2524550a7,"Fix scaling and allgather with `torch.autocast` (#7534)

This PR includes these two fixes:
- Use GradScaler only for FP16 (not for BF16)
- Fix dtype conversion for ZeRO3 allgather
- The reduce hook should be called only once, even when a parameter is
shared across multiple layers (tied parameters).
- Currently, the hook is triggered at each tied layer because we
temporarily set `.data` with a different dtype.
- The fix ensures that the parameter consistently retains the same
dtype.

---------

Signed-off-by: Masahiro Tanaka <mtanaka@anyscale.com>
Signed-off-by: Olatunji Ruwase <tunji.ruwase@snowflake.com>
Signed-off-by: Stas Bekman <stas@stason.org>
Signed-off-by: jakehemmerle <jakehemmerle@protonmail.com>
Signed-off-by: Qi Bin <qibin0506@users.noreply.github.com>
Co-authored-by: Olatunji Ruwase <tunji.ruwase@snowflake.com>
Co-authored-by: Stas Bekman <stas00@users.noreply.github.com>
Co-authored-by: digger yu <digger-yu@outlook.com>
Co-authored-by: Jake Hemmerle <jakehemmerle@gmail.com>
Co-authored-by: Logan Adams <114770087+loadams@users.noreply.github.com>
Co-authored-by: Qi Bin <qibin0506@users.noreply.github.com>",['c07b3abf9ac2637f5106f7d3991bb785e6ff38ef'],False,"['base_optimizer.py', 'torch_autocast.py', 'partition_parameters.py', 'stage3.py', 'stage_1_and_2.py']"
